{"import_tasks": {"1": {"id": "76670aa8-32cf-4418-bc2a-a487768df0a6", "name": "papers", "category": "cs.AI", "semantic_query": "", "text_search": "", "check_interval": 120, "is_active": true, "created_at": "2025-11-25T07:46:06.941626", "last_run": "2025-11-25T07:57:05.694604", "papers_imported": 25, "errors": 0}}, "papers": {"1": {"id": "2511.19433", "arxiv_id": "2511.19433", "title": "Mixture of Horizons in Action Chunking", "authors": ["Dong Jing", "Gang Wang", "Jiaqi Liu", "Weiliang Tang", "Zelong Sun", "Yunchao Yao", "Zhenyu Wei", "Yunhui Liu", "Zhiwu Lu", "Mingyu Ding"], "abstract": "Vision-language-action (VLA) models have shown remarkable capabilities in robotic manipulation, but their performance is sensitive to the $\\textbf{action chunk length}$ used during training, termed $\\textbf{horizon}$. Our empirical study reveals an inherent trade-off: longer horizons provide stronger global foresight but degrade fine-grained accuracy, while shorter ones sharpen local control yet struggle on long-term tasks, implying fixed choice of single horizons being suboptimal. To mitigate the trade-off, we propose a $\\textbf{mixture of horizons (MoH)}$ strategy. MoH rearranges the action chunk into several segments with different horizons, processes them in parallel with a shared action transformer, and fuses outputs with a light linear gate. It has three appealing benefits. 1) MoH exploits long-term foresight and short-term precision jointly within a single model, improving both performance and generalizability to complex tasks. 2) MoH is plug-and-play for full-attention action modules with minimal training or inference overhead. 3) MoH enables dynamic inference with adaptive horizons, which selects stable actions through cross-horizon consensus, achieving 2.5$\\times$ higher throughput than baselines while preserving superior performance. Extensive experiments over flow-based policies $\u03c0_0$, $\u03c0_{0.5}$, and one-step regression policy $\u03c0_{\\text{reg}}$ demonstrate that MoH yields consistent and significant gains on both simulations and real-world tasks. Notably, under mixed-task setting, $\u03c0_{0.5}$ with MoH reaches a new state-of-the-art with 99$\\%$ average success rate on LIBERO after only $30k$ training iterations. Project page: https://github.com/Timsty1/MixtureOfHorizons", "summary": "**Structured Summary of \"Mixture of Horizons in Action Chunking\"**\n\n---\n\n## 1. Key Contributions\n\n- **Identification of Horizon Trade-off:** Empirical demonstration that action chunk length (\"horizon\") in VLA models affects performance: longer horizons aid global foresight but harm precision, while shorter horizons do the reverse.\n- **Mixture of Horizons (MoH):** Introduction of a novel MoH strategy that leverages multiple horizons simultaneously in action chunking, overcoming the single-horizon trade-off.\n- **Plug-and-Play Mechanism:** MoH integrates with existing full-attention action modules with minimal changes and overhead.\n- **Dynamic, Adaptive Inference:** MoH allows adaptive selection of stable actions through consensus across horizons, boosting throughput and reliability.\n\n---\n\n## 2. Methodology\n\n- **Action Chunk Segmentation:** Action chunks are split into segments with different horizon lengths.\n- **Parallel Processing:** Each segment is processed in parallel using a shared action transformer network.\n- **Output Fusion:** A lightweight linear gating mechanism fuses the outputs from different horizons.\n- **Evaluation:** Experiments conducted on multiple policy types ($\u03c0_0$, $\u03c0_{0.5}$, $\u03c0_{\\text{reg}}$) in simulation and real-world robotic tasks, including tests on the mixed-task LIBERO benchmark.\n\n---\n\n## 3. Results\n\n- **Joint Performance Gain:** MoH improves both long-term global task completion and short-term precision compared to baselines with fixed horizons.\n- **Throughput:** Enables up to 2.5\u00d7 higher inference throughput while maintaining accuracy.\n- **State-of-the-Art Results:** On LIBERO mixed-task benchmark, $\u03c0_{0.5}$ with MoH achieves 99% average success rate after just 30k training iterations.\n- **Generalizability:** Consistent gains observed for various policies and tasks, both in simulation and on real robots.\n\n---\n\n## 4. Answered Questions\n\n- **How does action chunk length affect VLA model performance?**\n  - Demonstrates the fundamental trade-off between foresight and precision.\n- **Is single-horizon action chunking optimal?**\n  - Shows joint modeling of multiple horizons outperforms single-horizon strategies.\n- **Can dynamic horizon selection improve inference?**\n  - Establishes that adaptive horizon selection via MoH increases stability and throughput.\n\n---\n\n## 5. Future Research\n\n- **Extension to More Complex Tasks:** Applying MoH to broader and more challenging robotic scenarios.\n- **Integration with Other Actions/Pipelines:** Exploring MoH with different model architectures and multitask learning setups.\n- **Advanced Fusion Strategies:** Investigating more sophisticated output fusion beyond linear gating.\n- **Robustness and Adaptivity:** Joint horizon selection strategies that further adapt to task or environment changes.\n\n---", "keywords": ["action chunk length", "horizon", "mixture of horizons", "vision-language-action models", "robotic manipulation", "action transformer", "long-term foresight", "short-term precision", "dynamic inference", "cross-horizon consensus"], "categories": ["cs.RO", "cs.AI", "cs.CV"], "primary_category": "cs.RO", "published": "2025-11-24T18:59:51Z", "updated": "2025-11-24T18:59:51Z", "pdf_url": "https://arxiv.org/pdf/2511.19433v1", "full_text": "", "status": "new", "notes": "", "manual_tags": [], "created_at": "2025-11-25T07:47:12.534832", "updated_at": "2025-11-25T07:47:12.534833", "has_placeholder_summary": false, "has_placeholder_keywords": false, "has_embedding": true}, "2": {"id": "2511.19427", "arxiv_id": "2511.19427", "title": "Prompt Less, Smile More: MTP with Semantic Engineering in Lieu of Prompt Engineering", "authors": ["Jayanaka L. Dantanarayana", "Savini Kashmira", "Thakee Nathees", "Zichen Zhang", "Krisztian Flautner", "Lingjia Tang", "Jason Mars"], "abstract": "AI-Integrated programming is emerging as a foundational paradigm for building intelligent systems with large language models (LLMs). Recent approaches such as Meaning Typed Programming (MTP) automate prompt generation by leveraging the semantics already present in code. However, many real-world applications depend on contextual cues, developer intent, and domain-specific reasoning that extend beyond what static code semantics alone can express. To address this limitation, we introduce Semantic Engineering, a lightweight method for enriching program semantics so that LLM-based systems can more accurately reflect developer intent without requiring full manual prompt design. We present Semantic Context Annotations (SemTexts), a language-level mechanism that allows developers to embed natural-language context directly into program constructs. Integrated into the Jac programming language, Semantic Engineering extends MTP to incorporate these enriched semantics during prompt generation. We further introduce a benchmark suite designed to reflect realistic AI-Integrated application scenarios. Our evaluation shows that Semantic Engineering substantially improves prompt fidelity, achieving performance comparable to Prompt Engineering while requiring significantly less developer effort.", "summary": "**Structured Summary:**\n\n---\n\n### 1. Key Contributions\n\n- **Semantic Engineering Approach:** Introduces a method to enrich program semantics with developer intent and contextual information, going beyond what static code provides.\n- **Semantic Context Annotations (SemTexts):** Proposes a new language-level mechanism for embedding natural-language context directly within code constructs.\n- **Integration with Jac Language:** Implements Semantic Engineering within the Jac programming language, extending Meaning Typed Programming (MTP).\n- **Benchmark Suite:** Develops benchmarks tailored to realistic AI-Integrated programming scenarios.\n- **Efficiency and Performance:** Demonstrates that Semantic Engineering matches the fidelity of prompt engineering, with less manual effort required from developers.\n\n---\n\n### 2. Methodology\n\n- **Semantic Context Embedding:** Allows developers to annotate code with SemTexts (natural-language context).\n- **MTP Extension:** Extends MTP by incorporating these annotations into prompt generation workflows for LLMs.\n- **Implementation:** Deploys the approach in Jac, an AI-Integrated programming language.\n- **Benchmark Evaluation:** Assesses performance using a custom benchmark suite representing practical, real-world AI-integrated tasks.\n\n---\n\n### 3. Results\n\n- **Improved Prompt Fidelity:** Semantic Engineering produces prompts that closely align with developer intent.\n- **Comparable Accuracy:** Achieves performance on par with traditional manual prompt engineering across benchmarks.\n- **Reduced Developer Effort:** Substantially lowers the manual effort needed to elicit high-quality LLM outputs.\n\n---\n\n### 4. Answered Questions\n\n- **Can semantic context embedded directly in code replace manual prompt engineering for LLM programming?**  \n  **Yes**\u2014Semantic Engineering via SemText annotations enables accurate prompt generation without full manual prompt specification.\n- **Does enriching code semantics improve LLM-driven systems?**  \n  **Yes**\u2014Enhanced semantics help LLM systems interpret developer intention more faithfully.\n\n---\n\n### 5. Future Research\n\n- **Generalization to Other Languages:** Applying Semantic Engineering principles to additional programming languages and development platforms.\n- **Automated Annotation Extraction:** Investigating ways to semi-automatically derive or suggest SemTexts from code or documentation.\n- **Dynamic Context Usage:** Exploring runtime integration of dynamic context and developer interaction to further refine semantic enrichment.\n- **Benchmark Expansion:** Creating more diverse and complex benchmarks for broader evaluation.\n\n---", "keywords": ["AI-Integrated programming", "Meaning Typed Programming (MTP)", "Semantic Engineering", "Semantic Context Annotations (SemTexts)", "Jac programming language", "enriched program semantics", "prompt generation", "developer intent", "large language models (LLMs)", "prompt fidelity"], "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "published": "2025-11-24T18:58:22Z", "updated": "2025-11-24T18:58:22Z", "pdf_url": "https://arxiv.org/pdf/2511.19427v1", "full_text": "", "status": "new", "notes": "", "manual_tags": [], "created_at": "2025-11-25T07:47:32.857180", "updated_at": "2025-11-25T07:47:32.857181", "has_placeholder_summary": false, "has_placeholder_keywords": false, "has_embedding": true}, "3": {"id": "2511.19423", "arxiv_id": "2511.19423", "title": "Beyond Protein Language Models: An Agentic LLM Framework for Mechanistic Enzyme Design", "authors": ["Bruno Jacob", "Khushbu Agarwal", "Marcel Baer", "Peter Rice", "Simone Raugei"], "abstract": "We present Genie-CAT, a tool-augmented large-language-model (LLM) system designed to accelerate scientific hypothesis generation in protein design. Using metalloproteins (e.g., ferredoxins) as a case study, Genie-CAT integrates four capabilities -- literature-grounded reasoning through retrieval-augmented generation (RAG), structural parsing of Protein Data Bank files, electrostatic potential calculations, and machine-learning prediction of redox properties -- into a unified agentic workflow. By coupling natural-language reasoning with data-driven and physics-based computation, the system generates mechanistically interpretable, testable hypotheses linking sequence, structure, and function. In proof-of-concept demonstrations, Genie-CAT autonomously identifies residue-level modifications near [Fe--S] clusters that affect redox tuning, reproducing expert-derived hypotheses in a fraction of the time. The framework highlights how AI agents combining language models with domain-specific tools can bridge symbolic reasoning and numerical simulation, transforming LLMs from conversational assistants into partners for computational discovery.", "summary": "**1. Key Contributions:**\n- Introduces **Genie-CAT**, an agentic large language model (LLM) framework that integrates domain-specific tools to automate hypothesis generation in protein (enzyme) design.\n- Demonstrates the synergy of **language-based reasoning** with structural, computational, and machine-learning components in a unified agent workflow.\n- Highlights a shift from LLMs as conversational tools to active scientific collaborators in computational discovery.\n\n**2. Methodology:**\n- Implements Genie-CAT by coupling an LLM with four specialized modules:\n  1. Retrieval-augmented generation (RAG) for literature-based reasoning.\n  2. Structural parsing of Protein Data Bank files.\n  3. Electrostatic potential calculations.\n  4. Machine learning predictions for protein redox properties.\n- Applies the integrated framework to metalloproteins (such as ferredoxins), focusing on mechanistic hypothesis generation about sequence, structure, and function.\n- Validates the system's outputs against expert-derived protein design hypotheses.\n\n**3. Results:**\n- Genie-CAT autonomously and efficiently identifies residue-level modifications near [Fe\u2013S] clusters that can fine-tune protein redox properties.\n- The generated hypotheses are mechanistically interpretable and experimentally testable.\n- The system reproduced expert-level insights in a fraction of the time, demonstrating both accuracy and speed.\n\n**4. Answered Questions:**\n- Can LLMs be effectively transformed from passive, conversational models to **active scientific agents** in protein design?\n- How can combining LLMs with domain-specific computational tools lead to **mechanistically interpretable and actionable hypotheses** in enzyme engineering?\n- Can such systems autonomously reproduce expert-level scientific hypothesis generation in molecular biology?\n\n**5. Future Research:**\n- Extending Genie-CAT to broader protein families and other biophysical properties or catalytic functions.\n- Integrating more diverse and advanced computational tools (e.g., molecular dynamics, quantum calculations).\n- Exploring human-in-the-loop systems for collaborative discovery.\n- Investigating scalability, generalizability, and interpretability of AI-agent driven design workflows across other scientific and engineering domains.", "keywords": ["Genie-CAT", "protein design", "large language model", "retrieval-augmented generation", "mechanistic enzyme design", "metalloproteins", "redox tuning", "electrostatic potential calculations", "structural parsing", "agentic workflow"], "categories": ["q-bio.QM", "cs.AI"], "primary_category": "q-bio.QM", "published": "2025-11-24T18:57:07Z", "updated": "2025-11-24T18:57:07Z", "pdf_url": "https://arxiv.org/pdf/2511.19423v1", "full_text": "", "status": "new", "notes": "", "manual_tags": [], "created_at": "2025-11-25T07:47:52.358175", "updated_at": "2025-11-25T07:47:52.358176", "has_placeholder_summary": false, "has_placeholder_keywords": false, "has_embedding": true}, "4": {"id": "2511.19422", "arxiv_id": "2511.19422", "title": "SLMFix: Leveraging Small Language Models for Error Fixing with Reinforcement Learning", "authors": ["David Jiahao Fu", "Aryan Gupta", "Aaron Councilman", "David Grove", "Yu-Xiong Wang", "Vikram Adve"], "abstract": "Recent advancements in large language models (LLMs) have shown very impressive capabilities in code generation across many programming languages. However, even state-of-the-art LLMs generate programs that contains syntactic errors and fail to complete the given tasks, especially for low-resource programming languages (LRPLs). In addition, high training cost makes finetuning LLMs unaffordable with constrained computational resources, further undermining the effectiveness of LLMs for code generation. In this work, we propose SLMFix, a novel code generation pipeline that leverages a small language model (SLM) finetuned using reinforcement learning (RL) techniques to fix syntactic errors in LLM-generated programs to improve the quality of LLM-generated programs for domain-specific languages (DSLs). In specific, we applied RL on the SLM for the program repair task using a reward calculated using both a static validator and a static semantic similarity metric. Our experimental results demonstrate the effectiveness and generalizability of our approach across multiple DSLs, achieving more than 95% pass rate on the static validator. Notably, SLMFix brings substantial improvement to the base model and outperforms supervised finetuning approach even for 7B models on a LRPL, showing the potential of our approach as an alternative to traditional finetuning approaches.", "summary": "**Structured Summary of \"SLMFix: Leveraging Small Language Models for Error Fixing with Reinforcement Learning\"**\n\n---\n\n### 1. Key Contributions\n- Proposes **SLMFix**, a novel code generation pipeline leveraging **small language models (SLMs)** fine-tuned with **reinforcement learning (RL)** to repair syntactic errors in code generated by large language models (LLMs).\n- Demonstrates that SLMFix effectively improves code quality, particularly for **domain-specific languages (DSLs)** and **low-resource programming languages (LRPLs)**.\n- Shows that SLMFix can be a **cost-effective alternative to traditional supervised finetuning**, outperforming even large models (e.g., 7B parameters).\n\n---\n\n### 2. Methodology\n- Deploys a **small language model** pretrained for code generation.\n- Uses **reinforcement learning** for program repair, with:\n  - **Reward signal** based on a static validator (checking syntax correctness)\n  - **Semantic similarity metric** to assess functional equivalence.\n- Integrates SLMFix into the code generation process to fix errors in LLM-generated outputs for DSLs.\n- Compares performance with supervised finetuning and base models across multiple DSLs.\n\n---\n\n### 3. Results\n- SLMFix achieves **over 95% pass rate** on static validators for multiple DSLs.\n- Provides **substantial improvements** over:\n  - Base (untuned) models\n  - Supervised finetuning approaches, even for large 7B LLMs on LRPLs\n- Demonstrates **generalizability** across DSLs and effectiveness despite restricted computational resources.\n\n---\n\n### 4. Answered Questions\n- Can SLMs, when fine-tuned with RL, effectively **fix errors in LLM-generated code** for low-resource and domain-specific languages?\n  - **Yes; SLMFix improves pass rates and fixes syntactic errors efficiently.**\n- Is RL-based finetuning a **viable and cost-effective alternative** to supervised finetuning for code repair tasks?\n  - **Yes; RL-based SLM can outperform supervised approaches and larger models.**\n\n---\n\n### 5. Future Research\n- Extension to **broader classes of errors** (not just syntactic, but semantic and runtime errors).\n- Application to **other programming languages and non-code domains**.\n- Improvement and automation of **reward functions** for RL.\n- Investigation of **scalability with even smaller models or resource-constrained deployments**.\n- Exploration of **human-in-the-loop** and interactive error repair within the pipeline.\n\n---", "keywords": ["Small Language Models", "Reinforcement Learning", "Code Generation", "Program Repair", "Syntactic Error Fixing", "Low-resource Programming Languages", "Domain-specific Languages", "Static Validator", "Semantic Similarity Metric", "LLM Finetuning"], "categories": ["cs.SE", "cs.AI", "cs.PL"], "primary_category": "cs.SE", "published": "2025-11-24T18:56:47Z", "updated": "2025-11-24T18:56:47Z", "pdf_url": "https://arxiv.org/pdf/2511.19422v1", "full_text": "", "status": "new", "notes": "", "manual_tags": [], "created_at": "2025-11-25T07:48:12.373753", "updated_at": "2025-11-25T07:48:12.373754", "has_placeholder_summary": false, "has_placeholder_keywords": false, "has_embedding": true}, "5": {"id": "2511.19418", "arxiv_id": "2511.19418", "title": "Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens", "authors": ["Yiming Qin", "Bomin Wei", "Jiaxin Ge", "Konstantinos Kallidromitis", "Stephanie Fu", "Trevor Darrell", "Xudong Wang"], "abstract": "Vision-Language Models (VLMs) excel at reasoning in linguistic space but struggle with perceptual understanding that requires dense visual perception, e.g., spatial reasoning and geometric awareness. This limitation stems from the fact that current VLMs have limited mechanisms to capture dense visual information across spatial dimensions. We introduce Chain-of-Visual-Thought (COVT), a framework that enables VLMs to reason not only in words but also through continuous visual tokens-compact latent representations that encode rich perceptual cues. Within a small budget of roughly 20 tokens, COVT distills knowledge from lightweight vision experts, capturing complementary properties such as 2D appearance, 3D geometry, spatial layout, and edge structure. During training, the VLM with COVT autoregressively predicts these visual tokens to reconstruct dense supervision signals (e.g., depth, segmentation, edges, and DINO features). At inference, the model reasons directly in the continuous visual token space, preserving efficiency while optionally decoding dense predictions for interpretability. Evaluated across more than ten diverse perception benchmarks, including CV-Bench, MMVP, RealWorldQA, MMStar, WorldMedQA, and HRBench, integrating COVT into strong VLMs such as Qwen2.5-VL and LLaVA consistently improves performance by 3% to 16% and demonstrates that compact continuous visual thinking enables more precise, grounded, and interpretable multimodal intelligence.", "summary": "**Structured Summary of \"Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens\"**\n\n---\n\n### 1. Key Contributions\n\n- **Introduction of Chain-of-Visual-Thought (COVT):** Proposes a new framework that augments Vision-Language Models (VLMs) with the ability to reason using continuous visual tokens representing dense perceptual information (e.g., geometry, spatial layout, edge structures).\n- **Efficient Visual Token Representation:** Demonstrates that rich visual knowledge (2D/3D features, edges, spatial layouts) can be distilled into a compact set (about 20) of continuous visual tokens.\n- **Dual-Space Reasoning:** Enables VLMs to reason jointly in both language and visual token space, facilitating more nuanced, grounded, and interpretable understanding.\n- **Dense Supervision via Visual Tokens:** Provides a new supervision method where VLMs autoregressively predict visual tokens to reconstruct dense, expert supervision signals.\n- **Broad Benchmark Validation:** Validates COVT across a range of perception and reasoning benchmarks, demonstrating significant and consistent improvements.\n\n---\n\n### 2. Methodology\n\n- **Knowledge Distillation from Vision Experts:** Visual experts (specialized models for segmentation, depth, edges, etc.) generate dense perceptual features.\n- **Tokenization:** These diverse features are compressed into a small number of continuous visual tokens.\n- **Autoregressive Training:** VLMs are trained to predict these visual tokens in sequence, reconstructing the original dense supervision signals (such as depth maps, segmentations, DINO features).\n- **Dual Inference Strategies:** At inference, the model reasons in visual token space for efficiency; tokens can optionally be decoded back into detailed perceptual signals for interpretability.\n- **Benchmark Evaluation:** The approach is integrated into strong foundation models (like Qwen2.5-VL, LLaVA) and tested on over ten multimodal and perception benchmarks.\n\n---\n\n### 3. Results\n\n- **Significant Performance Gains:** COVT integration leads to consistent improvements across all evaluated benchmarks, with gains ranging from 3% to 16%.\n- **Enhanced Perceptual and Spatial Reasoning:** Models exhibit improved ability to handle tasks requiring fine-grained spatial, geometric, and perceptual understanding.\n- **Efficiency and Interpretability:** Achieves these improvements with only a small increase in token budget (ca. 20 visual tokens); tokens provide direct avenues for interpretability of model reasoning.\n\n---\n\n### 4. Answered Questions\n\n- **Can VLMs be enhanced to reason with dense visual information, not just language?**  \n  Yes\u2014COVT shows that integrating compact continuous visual tokens allows VLMs to incorporate and reason with dense perceptual cues.\n- **Does visual-token-based reasoning improve multitask performance?**  \n  Yes\u2014COVT delivers notable accuracy gains across a variety of benchmarks and tasks.\n- **Is efficient, interpretable dense visual reasoning feasible in VLMs?**  \n  Yes\u2014COVT achieves strong results with a small token overhead and provides interpretable intermediate visual representations.\n\n---\n\n### 5. Future Research Directions\n\n- **Scalability of Visual Tokenization:** Exploring more efficient or richer forms of continuous visual tokenization for even denser or high-resolution predictions.\n- **Cross-Modal Reasoning Mechanisms:** Further developing algorithms to more tightly couple language and visual token reasoning within VLMs.\n- **Transfer to Other Multimodal Tasks:** Applying and adapting COVT to video, robotics, embodied agents, or real-time perception tasks.\n- **Unsupervised or Self-Supervised Visual Token Discovery:** Reducing reliance on explicit visual experts or labeled data.\n- **Human-Interpretable Reasoning Traces:** Enhancing the interpretability of visual reasoning steps, enabling debugging and user oversight.\n\n---", "keywords": ["Vision-Language Models", "Chain-of-Visual-Thought", "continuous visual tokens", "perceptual reasoning", "spatial reasoning", "dense visual perception", "2D appearance", "3D geometry", "edge structure", "multimodal intelligence"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV", "published": "2025-11-24T18:55:19Z", "updated": "2025-11-24T18:55:19Z", "pdf_url": "https://arxiv.org/pdf/2511.19418v1", "full_text": "", "status": "new", "notes": "", "manual_tags": [], "created_at": "2025-11-25T07:48:35.194460", "updated_at": "2025-11-25T07:48:35.194461", "has_placeholder_summary": false, "has_placeholder_keywords": false, "has_embedding": true}, "6": {"id": "2511.19417", "arxiv_id": "2511.19417", "title": "Be My Eyes: Extending Large Language Models to New Modalities Through Multi-Agent Collaboration", "authors": ["James Y. Huang", "Sheng Zhang", "Qianchu Liu", "Guanghui Qin", "Tinghui Zhu", "Tristan Naumann", "Muhao Chen", "Hoifung Poon"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in challenging, knowledge-intensive reasoning tasks. However, extending LLMs to perceive and reason over a new modality (e.g., vision), often requires costly development of large-scale vision language models (VLMs) with LLMs as backbones. Smaller VLMs are more efficient and adaptable but often lack the broad knowledge and reasoning capabilities of frontier LLMs. In this work, we propose BeMyEyes, a modular, multi-agent framework for extending LLMs to multimodal reasoning by orchestrating collaboration between efficient, adaptable VLMs as perceivers and powerful LLMs as reasoners through conversations. We then introduce a data synthesis and supervised fine-tuning pipeline to train the perceiver agent to effectively collaborate with the reasoner agent. By combining the complementary strengths of perception and reasoning agents, BeMyEyes avoids the need for training large-scale multimodal models, preserves the generalization and reasoning capabilities of LLMs, and allows flexible extension to new domains and modalities. Experiments show that our framework unlocks the multimodal reasoning capabilities for LLMs, enabling a lightweight and fully open-source solution, i.e. equipping text-only DeepSeek-R1 with Qwen2.5-VL-7B perceiver, to outperform large-scale proprietary VLMs such as GPT-4o on a wide range of knowledge-intensive multimodal tasks. These results demonstrate the effectiveness, modularity, and scalability of our multi-agent approach for building future multimodal reasoning systems.", "summary": "**1. Key Contributions**\n\n- **Modular Multi-Agent Framework:** Proposes \u201cBeMyEyes,\u201d a system that enables large language models (LLMs) to perform multimodal reasoning by orchestrating collaboration between a lightweight visual-language model (VLM) as a \u201cperceiver\u201d agent and a powerful LLM as a \u201creasoner\u201d agent through conversational exchanges.\n- **Data Synthesis & Supervised Tuning Pipeline:** Introduces a novel data generation and tuning process to train the perceiver VLM to interact and collaborate effectively with the LLM reasoner.\n- **Efficient, Scalable, and Open-Source Solution:** Demonstrates a fully open-source, lightweight multimodal reasoning system (e.g., using DeepSeek-R1 LLM and Qwen2.5-VL-7B VLM) that rivals or outperforms proprietary, large-scale models like GPT-4o.\n- **Flexible Modality Extension:** Highlights a modular architecture allowing easy extensions to new domains and modalities without costly monolithic retraining.\n\n**2. Methodology**\n\n- **Multi-Agent Collaboration:** Separates perception (handled by small, efficient VLMs) and reasoning (handled by large, advanced LLMs) into distinct agents that communicate in natural language.\n- **Training Pipeline:** Develops a data synthesis process to create collaborative dialogues and applies supervised fine-tuning to teach the perceiver agent to provide useful, contextual information to the reasoner agent.\n- **System Architecture:** Maintains LLM as text-only, leveraging the VLM for vision tasks, and facilitating task-solving via agent-to-agent conversations.\n\n**3. Results**\n\n- **Enhanced Multimodal Reasoning:** The proposed BeMyEyes framework enables text-only LLMs, when paired with an efficient VLM perceiver, to perform knowledge-intensive multimodal reasoning tasks effectively.\n- **Superior Performance:** The approach achieves state-of-the-art results, outperforming proprietary and much larger models (e.g., GPT-4o) on diverse multimodal benchmarks using purely open-source components.\n- **Scalability and Adaptability:** Demonstrates that combining smaller, specialized models through collaboration achieves efficiency, scalability, and improved transfer to new modalities.\n\n**4. Answered Questions**\n\n- **Can LLMs be efficiently extended to new modalities without large-scale multimodal training?**  \n  \u2013 Yes; by modularly combining efficient VLM perceivers with LLM reasoners via agent collaboration.\n- **Is it possible to preserve the broad reasoning abilities of LLMs when adding multimodality?**  \n  \u2013 Yes; this approach retains the LLM\u2019s generalization and reasoning strength without retraining on multimodal data.\n- **How does a modular, multi-agent framework compare with monolithic VLMs?**  \n  \u2013 It offers competitive or superior performance, with greater efficiency and flexibility.\n\n**5. Future Research**\n\n- **Expanding Modalities:** Extending the framework to include other modalities, such as audio or video, or new sensor inputs.\n- **Autonomous Agent Collaboration:** Enhancing the collaboration protocol between perceiver and reasoner agents for more autonomous and adaptive task-solving.\n- **Broader Applications:** Applying the framework to more complex real-world multimodal scenarios and domains (e.g., robotics, healthcare).\n- **Further Optimization:** Improving data synthesis and tuning pipelines to enhance alignment and reduce supervision needs.\n- **Exploring More Agents:** Incorporating additional specialized agents (e.g., for temporal reasoning, memory) for even broader capabilities.", "keywords": ["Large Language Models", "multimodal reasoning", "vision language models", "multi-agent collaboration", "BeMyEyes framework", "perceiver agent", "reasoner agent", "data synthesis", "supervised fine-tuning", "open-source multimodal systems"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "published": "2025-11-24T18:55:16Z", "updated": "2025-11-24T18:55:16Z", "pdf_url": "https://arxiv.org/pdf/2511.19417v1", "full_text": "", "status": "new", "notes": "", "manual_tags": [], "created_at": "2025-11-25T07:49:01.080759", "updated_at": "2025-11-25T07:49:01.080760", "has_placeholder_summary": false, "has_placeholder_keywords": false, "has_embedding": true}, "7": {"id": "2511.19401", "arxiv_id": "2511.19401", "title": "In-Video Instructions: Visual Signals as Generative Control", "authors": ["Gongfan Fang", "Xinyin Ma", "Xinchao Wang"], "abstract": "Large-scale video generative models have recently demonstrated strong visual capabilities, enabling the prediction of future frames that adhere to the logical and physical cues in the current observation. In this work, we investigate whether such capabilities can be harnessed for controllable image-to-video generation by interpreting visual signals embedded within the frames as instructions, a paradigm we term In-Video Instruction. In contrast to prompt-based control, which provides textual descriptions that are inherently global and coarse, In-Video Instruction encodes user guidance directly into the visual domain through elements such as overlaid text, arrows, or trajectories. This enables explicit, spatial-aware, and unambiguous correspondences between visual subjects and their intended actions by assigning distinct instructions to different objects. Extensive experiments on three state-of-the-art generators, including Veo 3.1, Kling 2.5, and Wan 2.2, show that video models can reliably interpret and execute such visually embedded instructions, particularly in complex multi-object scenarios.", "summary": "**Structured Summary: \"In-Video Instructions: Visual Signals as Generative Control\"**\n\n---\n\n### 1. Key Contributions\n\n- **Introduces the concept of \"In-Video Instruction\":** A novel paradigm for controlling video generation by embedding visual signals (e.g., overlaid text, arrows, trajectories) directly into frames, rather than relying solely on textual prompts.\n- **Demonstrates spatially explicit and object-specific control:** Shows that visual instructions allow for fine-grained, unambiguous direction of actions for multiple objects, overcoming limitations of global, coarse text prompts.\n- **Empirical validation across multiple state-of-the-art models:** Provides comprehensive experiments on Veo 3.1, Kling 2.5, and Wan 2.2, demonstrating generalizability and effectiveness in complex, multi-object settings.\n\n### 2. Methodology\n\n- **Research Approach:** The authors modify input frames to include explicit visual instructions (arrows, text, trajectories) indicating desired object actions.\n- **Evaluation:** They test whether leading video generative models can accurately interpret these embedded signals and generate future frames that properly execute the instructions.\n- **Comparative Analysis:** Conduct extensive evaluations on three top-performing generators, in both basic and multi-object scenarios, to assess controllability and fidelity.\n\n### 3. Results\n\n- **Models successfully interpret in-frame visual signals:** State-of-the-art generators accurately follow embedded instructions, including object-specific actions in scenes with multiple subjects.\n- **Improved control and flexibility:** In-Video Instructions outperform traditional text-based prompting in spatial specificity and clarity, especially in scenarios requiring differentiated object behaviors.\n\n### 4. Answered Questions\n\n- **Can visual generative models be controlled by visual (rather than text) instructions embedded in frames?**  \n  *Yes; the paper shows models can reliably detect and respond to these cues.*\n- **Do in-frame visual instructions enable better multi-object and spatially detailed control over generation?**  \n  *Yes; embedded instructions provide explicit and localized guidance not achievable with global textual prompts.*\n\n### 5. Future Research\n\n- **Broader instruction formats:** Exploration of additional or more complex visual signals for instructions.\n- **Generalization:** Testing on a wider variety of models and real-world, in-the-wild videos.\n- **Robustness and reliability:** Improving model sensitivity to more subtle or occluded signals.\n- **User-friendly tools:** Development of interfaces for non-experts to create effective visual instructions.\n- **Hybrid control:** Combining in-video and text-based controls for richer and more flexible video generation paradigms.", "keywords": ["in-video instruction", "video generative models", "image-to-video generation", "visual signals", "generative control", "visual instructions", "spatial-aware guidance", "multi-object scenarios", "controllable video synthesis", "visual domain user guidance"], "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV", "published": "2025-11-24T18:38:45Z", "updated": "2025-11-24T18:38:45Z", "pdf_url": "https://arxiv.org/pdf/2511.19401v1", "full_text": "", "status": "new", "notes": "", "manual_tags": [], "created_at": "2025-11-25T07:49:10.535016", "updated_at": "2025-11-25T07:49:10.535017", "has_placeholder_summary": false, "has_placeholder_keywords": false, "has_embedding": true}, "8": {"id": "2511.19399", "arxiv_id": "2511.19399", "title": "DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research", "authors": ["Rulin Shao", "Akari Asai", "Shannon Zejiang Shen", "Hamish Ivison", "Varsha Kishore", "Jingming Zhuo", "Xinran Zhao", "Molly Park", "Samuel G. Finlayson", "David Sontag", "Tyler Murray", "Sewon Min", "Pradeep Dasigi", "Luca Soldaini", "Faeze Brahman", "Wen-tau Yih", "Tongshuang Wu", "Luke Zettlemoyer", "Yoon Kim", "Hannaneh Hajishirzi", "Pang Wei Koh"], "abstract": "Deep research models perform multi-step research to produce long-form, well-attributed answers. However, most open deep research models are trained on easily verifiable short-form QA tasks via reinforcement learning with verifiable rewards (RLVR), which does not extend to realistic long-form tasks. We address this with Reinforcement Learning with Evolving Rubrics (RLER), in which we construct and maintain rubrics that co-evolve with the policy model during training; this allows the rubrics to incorporate information that the model has newly explored and to provide discriminative, on-policy feedback. Using RLER, we develop Deep Research Tulu (DR Tulu-8B), the first open model that is directly trained for open-ended, long-form deep research. Across four long-form deep research benchmarks in science, healthcare and general domains, DR Tulu substantially outperforms existing open deep research models, and matches or exceeds proprietary deep research systems, while being significantly smaller and cheaper per query. To facilitate future research, we release all data, models, and code, including our new MCP-based agent infrastructure for deep research systems.", "summary": "**Structured Summary of \"DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research\"**\n\n---\n\n### 1. Key Contributions\n\n- **Introduction of RLER**: Proposes Reinforcement Learning with Evolving Rubrics (RLER), a novel training paradigm where evaluation rubrics co-evolve with the policy model, allowing for dynamic, adaptive, and more discriminative feedback during reinforcement learning.\n- **DR Tulu-8B Model**: Presents DR Tulu-8B, the first open deep research model directly trained for long-form, open-ended research tasks instead of short-form QA.\n- **Benchmarks & Performance**: Demonstrates that DR Tulu-8B outperforms all open deep research models and rivals proprietary systems on four diverse long-form benchmarks\u2014in science, healthcare, and general domains.\n- **Resource Efficiency & Openness**: DR Tulu-8B is much smaller and more cost-effective per query than proprietary alternatives, and all related code, data, models, and a new agent infrastructure (MCP-based) are open-sourced for reproducibility and development.\n\n---\n\n### 2. Methodology\n\n- **RLER Training Framework**: Integrates evolving rubrics into the RL training loop, where rubrics adapt based on newly discovered information and model progress, yielding improved reward signals for long-form research.\n- **Model Architecture**: Builds an 8B-parameter model (DR Tulu-8B) specifically trained for open-ended research tasks rather than standard, easily verifiable short-form QA.\n- **Benchmark Evaluation**: Tests DR Tulu-8B on four comprehensive long-form benchmarks spanning multiple domains, comparing against open and proprietary models.\n- **Infrastructure**: Develops and releases MCP-based agent infrastructure tailored for deep research systems, supporting further research in this area.\n\n---\n\n### 3. Results\n\n- **Superior Long-form Performance**: DR Tulu-8B achieves significantly higher accuracy and attribution on long-form research tasks compared to existing open models.\n- **Competitive with Proprietary Models**: On several benchmarks, DR Tulu-8B matches or even surpasses proprietary research systems\u2014despite its smaller size and cost.\n- **Efficiency**: The model achieves these results while being much more resource-efficient (smaller, faster, and less expensive per query).\n- **Open Tools**: All models, training data, and infrastructure have been made publicly available.\n\n---\n\n### 4. Answered Questions\n\n- **Can open models excel at long-form deep research?**  \n  Yes: With RLER, DR Tulu-8B shows that open models can achieve and even exceed proprietary standards on long-form, open-ended research tasks.\n- **Is RLVR sufficient for long-form research?**  \n  No: Standard RLVR approaches trained on short-form QA tasks do not generalize well to realistic long-form scenarios.\n- **Can evolving rubrics improve RL for research models?**  \n  Yes: RLER enables more adaptive and relevant feedback, leading to stronger research abilities in the model.\n- **Can deep research models be trained efficiently at scale?**  \n  Yes: DR Tulu-8B demonstrates competitive results with much lower compute requirements.\n\n---\n\n### 5. Future Research Directions\n\n- **Generalization to More Domains**: Expanding RLER and DR Tulu to additional subject areas and complex research tasks.\n- **Rubric Evolution Strategies**: Exploring more sophisticated methods for rubric adaptation and reward shaping during model training.\n- **Scalability**: Scaling up model size and analyzing the limits of RLER\u2019s performance improvements.\n- **Human-in-the-Loop Systems**: Combining rubric evolution with active human feedback to further enhance research quality.\n- **Agent Collaboration**: Using MCP-based infrastructure to study multi-agent deep research and collaborative problem-solving.\n\n---", "keywords": ["Deep research models", "reinforcement learning", "evolving rubrics", "long-form QA tasks", "RLVR", "RLER", "on-policy feedback", "DR Tulu-8B", "open deep research models", "MCP-based agent infrastructure"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "published": "2025-11-24T18:35:54Z", "updated": "2025-11-24T18:35:54Z", "pdf_url": "https://arxiv.org/pdf/2511.19399v1", "full_text": "", "status": "new", "notes": "", "manual_tags": [], "created_at": "2025-11-25T07:49:32.401867", "updated_at": "2025-11-25T07:49:32.401868", "has_placeholder_summary": false, "has_placeholder_keywords": false, "has_embedding": true}, "9": {"id": "2511.19342", "arxiv_id": "2511.19342", "title": "Explicit Tonal Tension Conditioning via Dual-Level Beam Search for Symbolic Music Generation", "authors": ["Maral Ebrahimzadeh", "Gilberto Bernardes", "Sebastian Stober"], "abstract": "State-of-the-art symbolic music generation models have recently achieved remarkable output quality, yet explicit control over compositional features, such as tonal tension, remains challenging. We propose a novel approach that integrates a computational tonal tension model, based on tonal interval vector analysis, into a Transformer framework. Our method employs a two-level beam search strategy during inference. At the token level, generated candidates are re-ranked using model probability and diversity metrics to maintain overall quality. At the bar level, a tension-based re-ranking is applied to ensure that the generated music aligns with a desired tension curve. Objective evaluations indicate that our approach effectively modulates tonal tension, and subjective listening tests confirm that the system produces outputs that align with the target tension. These results demonstrate that explicit tension conditioning through a dual-level beam search provides a powerful and intuitive tool to guide AI-generated music. Furthermore, our experiments demonstrate that our method can generate multiple distinct musical interpretations under the same tension condition.", "summary": "**Structured Summary of \"Explicit Tonal Tension Conditioning via Dual-Level Beam Search for Symbolic Music Generation\"**\n\n---\n\n### 1. **Key Contributions**\n\n- Introduces an explicit tonal tension conditioning mechanism for symbolic music generation.\n- Develops a dual-level beam search strategy integrating token-level and bar-level re-ranking.\n- Incorporates a computational tonal tension model (based on tonal interval vectors) directly into the generation process.\n- Demonstrates that the system can produce multiple distinct musical interpretations that conform to a specified tension profile.\n\n---\n\n### 2. **Methodology**\n\n- **Framework**: Uses a Transformer-based symbolic music generation model.\n- **Tonal Tension Model**: Computes tonal tension via tonal interval vector analysis.\n- **Dual-Level Beam Search**:\n  - **Token-level**: Reranking of generated token candidates using model probabilities and diversity metrics to maintain output quality.\n  - **Bar-level**: Applies tension-based reranking to ensure alignment with a pre-defined tension curve across musical bars.\n- **Evaluation**:\n  - **Objective**: Quantitative analysis of match between generated tension and target tension profiles.\n  - **Subjective**: Listening tests to validate perceptual alignment with desired tension.\n\n---\n\n### 3. **Results**\n\n- Achieves effective modulation of tonal tension in generated music.\n- Objective metrics confirm close adherence to specified tension curves.\n- Subjective listening tests indicate that listeners perceive outputs as aligning with the target tension.\n- The model supports the generation of multiple distinct musical outputs under the same tension specification.\n\n---\n\n### 4. **Answered Questions**\n\n- **How can explicit compositional features such as tonal tension be controlled in symbolic music generation?**\n- **Can dual-level beam search and computational tension modeling enhance controllability and expressive diversity in AI-generated music?**\n- **Does explicit tension conditioning improve alignment between generated music and desired musical profiles from both objective and subjective perspectives?**\n\n---\n\n### 5. **Future Research**\n\n- Extend tension conditioning to other compositional dimensions (e.g., rhythm, timbre, motif development).\n- Adapt approach for real-time interactive or improvisational AI music systems.\n- Explore integration with audio (not just symbolic) generation models.\n- Investigate user-guided or adaptive tension curves for personalized music creation.\n- Evaluate impact on broader stylistic and genre diversity.\n\n---", "keywords": ["symbolic music generation", "tonal tension", "Transformer framework", "tonal interval vector analysis", "dual-level beam search", "tension conditioning", "token-level re-ranking", "bar-level re-ranking", "diversity metrics", "music generation control"], "categories": ["cs.SD", "cs.AI"], "primary_category": "cs.SD", "published": "2025-11-24T17:41:04Z", "updated": "2025-11-24T17:41:04Z", "pdf_url": "https://arxiv.org/pdf/2511.19342v1", "full_text": "", "status": "new", "notes": "", "manual_tags": [], "created_at": "2025-11-25T07:51:02.581911", "updated_at": "2025-11-25T07:51:02.581912", "has_placeholder_summary": false, "has_placeholder_keywords": false, "has_embedding": true}, "10": {"id": "2511.19324", "arxiv_id": "2511.19324", "title": "What Drives Cross-lingual Ranking? Retrieval Approaches with Multilingual Language Models", "authors": ["Roksana Goworek", "Olivia Macmillan-Scott", "Eda B. \u00d6zyi\u011fit"], "abstract": "Cross-lingual information retrieval (CLIR) enables access to multilingual knowledge but remains challenging due to disparities in resources, scripts, and weak cross-lingual semantic alignment in embedding models. Existing pipelines often rely on translation and monolingual retrieval heuristics, which add computational overhead and noise, degrading performance. This work systematically evaluates four intervention types, namely document translation, multilingual dense retrieval with pretrained encoders, contrastive learning at word, phrase, and query-document levels, and cross-encoder re-ranking, across three benchmark datasets. We find that dense retrieval models trained specifically for CLIR consistently outperform lexical matching methods and derive little benefit from document translation. Contrastive learning mitigates language biases and yields substantial improvements for encoders with weak initial alignment, and re-ranking can be effective, but depends on the quality of the cross-encoder training data. Although high-resource languages still dominate overall performance, gains over lexical and document-translated baselines are most pronounced for low-resource and cross-script pairs. These findings indicate that cross-lingual search systems should prioritise semantic multilingual embeddings and targeted learning-based alignment over translation-based pipelines, particularly for cross-script and under-resourced languages.", "summary": "**1. Key Contributions**\n\n- Systematic evaluation of four prominent approaches for cross-lingual information retrieval (CLIR): document translation, multilingual dense retrieval, contrastive learning, and cross-encoder re-ranking.\n- Demonstration that dense retrieval models, when specifically trained for CLIR, outperform traditional lexical matching and translation-based methods.\n- Identification that contrastive learning is especially beneficial for improving cross-lingual alignment in weakly aligned encoders.\n- Highlighting that document translation provides minimal benefits for dense retrieval setups.\n- Showing that gains from modern embedding-based approaches are most prominent in low-resource and cross-script language scenarios.\n\n**2. Methodology**\n\n- Empirical evaluation across three benchmark CLIR datasets.\n- Comparative analysis of four intervention types:\n    - Document translation pipelines.\n    - Dense retrieval using multilingual pretrained encoders.\n    - Integration of contrastive learning at multiple levels (word, phrase, query-document).\n    - Cross-encoder re-ranking.\n- Analysis of results differentiating between high- and low-resource languages as well as cross-script versus same-script language pairs.\n\n**3. Results**\n\n- Dense retrieval models with multilingual encoders trained for CLIR significantly outperform:\n    - Lexical matching retrieval methods.\n    - Translation + monolingual retrieval heuristics.\n- Document translation offers limited or negligible improvement to dense retrieval.\n- Contrastive learning addresses language bias, showing the greatest gains for encoders with otherwise poor cross-lingual alignment.\n- Re-ranking can help improve performance, contingent on the quality of training data for cross-encoders.\n- Performance improvements over baselines are especially pronounced for low-resource and cross-script (different writing system) language pairs.\n\n**4. Answered Questions**\n\n- Which retrieval approaches are most effective for CLIR with multilingual language models?\n- Does document translation enhance dense retrieval in multilingual settings?\n- How can language bias and weak alignment in multilingual encoders be addressed?\n- How do these methods perform differently for high-resource vs. low-resource and cross-script language pairs?\n\n**5. Future Research**\n\n- Developing more robust multilingual encoders for truly low-resource and cross-script languages.\n- Improving training data quality and methods for cross-encoder re-ranking, especially in under-resourced contexts.\n- Exploring more advanced or targeted contrastive learning strategies for alignment.\n- Investigating the integration of external linguistic or structural knowledge into semantic retrieval models.\n- Scaling and adapting methods to open-domain and highly diverse multilingual settings.", "keywords": ["Cross-lingual information retrieval", "multilingual language models", "semantic alignment", "document translation", "dense retrieval models", "contrastive learning", "cross-encoder re-ranking", "lexical matching methods", "cross-script language pairs", "low-resource languages"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR", "published": "2025-11-24T17:17:40Z", "updated": "2025-11-24T17:17:40Z", "pdf_url": "https://arxiv.org/pdf/2511.19324v1", "full_text": "", "status": "new", "notes": "", "manual_tags": [], "created_at": "2025-11-25T07:51:28.849224", "updated_at": "2025-11-25T07:51:28.849225", "has_placeholder_summary": false, "has_placeholder_keywords": false, "has_embedding": true}, "11": {"id": "2511.19314", "arxiv_id": "2511.19314", "title": "PRInTS: Reward Modeling for Long-Horizon Information Seeking", "authors": ["Jaewoo Lee", "Archiki Prasad", "Justin Chih-Yao Chen", "Zaid Khan", "Elias Stengel-Eskin", "Mohit Bansal"], "abstract": "Information-seeking is a core capability for AI agents, requiring them to gather and reason over tool-generated information across long trajectories. However, such multi-step information-seeking tasks remain challenging for agents backed by language models. While process reward models (PRMs) can guide agents by ranking candidate steps at test-time, existing PRMs, designed for short reasoning with binary judgment, cannot capture richer dimensions of information-seeking steps, such as tool interactions and reasoning over tool outputs, nor handle the rapidly growing context in long-horizon tasks. To address these limitations, we introduce PRInTS, a generative PRM trained with dual capabilities: (1) dense scoring based on the PRM's reasoning across multiple step quality dimensions (e.g., interpretation of tool outputs, tool call informativeness) and (2) trajectory summarization that compresses the growing context while preserving essential information for step evaluation. Extensive evaluations across FRAMES, GAIA (levels 1-3), and WebWalkerQA (easy-hard) benchmarks on multiple models, along with ablations, reveal that best-of-n sampling with PRInTS enhances information-seeking abilities of open-source models as well as specialized agents, matching or surpassing the performance of frontier models with a much smaller backbone agent and outperforming other strong reward modeling baselines.", "summary": "**Structured Summary of \"PRInTS: Reward Modeling for Long-Horizon Information Seeking\"**\n\n---\n\n### 1. Key Contributions\n\n- **PRInTS Framework:** Introduction of PRInTS, a new process reward model (PRM) designed for long-horizon, multi-step information-seeking tasks in AI agents.\n- **Dual Capabilities:** PRInTS uniquely combines (a) dense step-scoring across multiple dimensions (like informativeness of tool calls and interpretation of tool outputs) and (b) trajectory summarization, enabling agents to manage and compress growing contexts.\n- **Performance Boost:** Demonstration that PRInTS, when paired with best-of-n sampling, significantly improves agent performance on a range of benchmarks, enabling small and open-source models to match or exceed the capabilities of larger, frontier proprietary agents.\n\n---\n\n### 2. Methodology\n\n- **Generative PRM Training:** PRInTS is trained to provide detailed quality assessments (not just binary) on each reasoning step of an agent through dense, multi-dimensional reward modeling.\n- **Trajectory Summarization:** The model learns to generate succinct summaries of interaction histories, ensuring essential context is retained for subsequent step evaluation.\n- **Evaluation:** PRInTS is tested on three benchmark suites\u2014FRAMES, GAIA (levels 1-3), and WebWalkerQA (easy-hard)\u2014using both open-source and specialized agent backbones.\n- **Comparative Studies:** Conducted ablation studies and compared PRInTS against strong baseline reward models, using best-of-n candidate sampling to select agent actions.\n\n---\n\n### 3. Results\n\n- **Enhanced Information-Seeking:** PRInTS markedly improves long-horizon information-seeking performance, allowing smaller or open-source agents to match or surpass the accuracy of leading frontier LLM agents.\n- **Superiority over Baselines:** PRInTS outperforms existing process reward models and other baselines, particularly in handling challenging multi-step reasoning scenarios and growing tool-generated contexts.\n- **Generalizability:** The approach is effective across different benchmarks and model types, indicating strong generalizability.\n\n---\n\n### 4. Answered Questions\n\n- **How can reward modeling better support long-horizon information-seeking in AI agents?**\n  - By designing PRMs (like PRInTS) that provide dense, multi-dimensional feedback and context summarization, agents can reason more effectively over long trajectories involving tool use.\n- **Are small or open-source agents capable of matching state-of-the-art performance in information-seeking?**\n  - Yes; with PRInTS guidance, such agents can achieve performance on par with, or better than, much larger commercial models.\n\n---\n\n### 5. Future Research\n\n- **Scalability:** Extending PRInTS to handle even longer and more complex trajectories or real-world environments.\n- **Broadening Application:** Applying the PRInTS framework to new domains involving complex tool use or additional reasoning modalities.\n- **Iterative Improvement:** Investigating self-improving or online training regimes for PRMs as agent behaviors evolve.\n- **Integration:** Exploring tighter integration with retrieval-augmented LLMs or multi-modal agents.\n- **Automated Summarization:** Improving automated summarization techniques to further optimize context compression without information loss.", "keywords": ["reward modeling", "information-seeking", "process reward models", "long-horizon tasks", "language model agents", "tool interactions", "trajectory summarization", "step quality dimensions", "dense scoring", "agent reasoning"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "published": "2025-11-24T17:09:43Z", "updated": "2025-11-24T17:09:43Z", "pdf_url": "https://arxiv.org/pdf/2511.19314v1", "full_text": "", "status": "new", "notes": "", "manual_tags": [], "created_at": "2025-11-25T07:51:43.537445", "updated_at": "2025-11-25T07:51:43.537445", "has_placeholder_summary": false, "has_placeholder_keywords": false, "has_embedding": true}, "12": {"id": "2511.19436", "arxiv_id": "2511.19436", "title": "VDC-Agent: When Video Detailed Captioners Evolve Themselves via Agentic Self-Reflection", "authors": ["Qiang Wang", "Xinyuan Gao", "SongLin Dong", "Jizhou Han", "Jiangyang Li", "Yuhang He", "Yihong Gong"], "abstract": "We present VDC-Agent, a self-evolving framework for Video Detailed Captioning that requires neither human annotations nor larger teacher models. The agent forms a closed loop of caption generation, principle-guided scoring (score and textual suggestions), and prompt refinement. When caption quality regresses, a self-reflection path leverages the previous chain-of-thought to amend the update. Running this process on unlabeled videos produces trajectories of (caption, score) pairs. We convert the trajectories into preference tuples and filter out samples with JSON parsing errors, resulting in VDC-Agent-19K, which contains 18,886 automatically constructed pairs. We then fine-tune the base MLLM on this dataset using an easy-to-hard curriculum direct preference optimization. Built on Qwen2.5-VL-7B-Instruct, our VDC-Agent-7B attains state-of-the-art performance on the VDC benchmark with 49.08% average accuracy and 2.50 score, surpassing specialized video captioners and improving over the base model by +5.13% accuracy and +0.27 score at similar inference cost.", "summary": "**1. Key Contributions:**\n- Introduces VDC-Agent, a novel, self-evolving framework for video detailed captioning that operates without human annotations or large teacher models.\n- Proposes a closed-loop process involving caption generation, principle-guided scoring, prompt refinement, and agentic self-reflection to improve captions autonomously.\n- Releases VDC-Agent-19K, a large-scale dataset of 18,886 automatically constructed (caption, score) pairs obtained from unlabeled videos.\n- Demonstrates state-of-the-art performance in video captioning using a relatively compact model (VDC-Agent-7B) via direct preference optimization.\n\n**2. Methodology:**\n- The agent generates captions for unlabeled videos and scores them using a principle-guided approach that includes textual suggestions.\n- When caption quality drops (\u201cregresses\u201d), the agent self-reflects using previous chains-of-thought to improve prompt updates.\n- The process results in learning trajectories, which are refined into preference tuples and filtered for parsing errors.\n- The dataset (VDC-Agent-19K) is used to fine-tune a base Multimodal Large Language Model (MLLM), specifically Qwen2.5-VL-7B-Instruct, using an easy-to-hard curriculum with direct preference optimization.\n\n**3. Results:**\n- VDC-Agent-7B achieves a new state-of-the-art on the VDC benchmark: 49.08% average accuracy and a score of 2.50.\n- Surpasses specialized video captioners and the base model by +5.13% accuracy and +0.27 score, with comparable inference costs.\n\n**4. Answered Questions:**\n- Can high-quality, detailed video captioning be achieved without human annotations or large teacher models?\n  - Yes, through self-evolving agentic processes.\n- How can preference data be autonomously generated from unlabeled videos for model training?\n  - Via the agent\u2019s closed-loop generation, scoring, and self-improvement scheme.\n\n**5. Future Research:**\n- Extension to larger, more diverse unlabeled video datasets for broader generalization.\n- Application of agentic self-reflection frameworks to other multimodal tasks or domains.\n- Investigation into further augmenting the principle-guided scoring and self-reflection mechanisms for richer feedback.\n- Exploring interpretability and transparency of agent trajectories and decision-making processes.", "keywords": ["Video Detailed Captioning", "self-evolving framework", "agentic self-reflection", "principle-guided scoring", "prompt refinement", "unlabeled videos", "preference tuples", "curriculum direct preference optimization", "Qwen2.5-VL-7B-Instruct", "state-of-the-art performance"], "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "primary_category": "cs.CV", "published": "2025-11-24T18:59:56Z", "updated": "2025-11-24T18:59:56Z", "pdf_url": "https://arxiv.org/pdf/2511.19436v1", "full_text": "", "status": "new", "notes": "", "manual_tags": [], "created_at": "2025-11-25T07:55:59.483330", "updated_at": "2025-11-25T07:55:59.483331", "has_placeholder_summary": false, "has_placeholder_keywords": false, "has_embedding": true}, "13": {"id": "2511.19413", "arxiv_id": "2511.19413", "title": "UniGame: Turning a Unified Multimodal Model Into Its Own Adversary", "authors": ["Zhaolong Su", "Wang Lu", "Hao Chen", "Sharon Li", "Jindong Wang"], "abstract": "Unified Multimodal Models (UMMs) have shown impressive performance in both understanding and generation with a single architecture. However, UMMs still exhibit a fundamental inconsistency: understanding favors compact embeddings, whereas generation favors reconstruction-rich representations. This structural trade-off produces misaligned decision boundaries, degraded cross-modal coherence, and heightened vulnerability under distributional and adversarial shifts. In this paper, we present UniGame, a self-adversarial post-training framework that directly targets the inconsistencies. By applying a lightweight perturber at the shared token interface, UniGame enables the generation branch to actively seek and challenge fragile understanding, turning the model itself into its own adversary. Experiments demonstrate that UniGame significantly improves the consistency (+4.6%). Moreover, it also achieves substantial improvements in understanding (+3.6%), generation (+0.02), out-of-distribution and adversarial robustness (+4.8% and +6.2% on NaturalBench and AdVQA). The framework is architecture-agnostic, introduces less than 1% additional parameters, and is complementary to existing post-training methods. These results position adversarial self-play as a general and effective principle for enhancing the coherence, stability, and unified competence of future multimodal foundation models. The official code is available at: https://github.com/AIFrontierLab/UniGame", "summary": "**Structured Summary of \"UniGame: Turning a Unified Multimodal Model Into Its Own Adversary\"**\n\n---\n\n### 1. **Key Contributions**\n- **Identification of Inconsistency**: Highlights the fundamental misalignment between the needs of understanding (favoring compact embeddings) and generation (favoring rich, reconstructive representations) in Unified Multimodal Models (UMMs).\n- **UniGame Framework**: Introduces \"UniGame\", a novel, architecture-agnostic self-adversarial post-training method that uses the model itself as an adversary to resolve these inconsistencies.\n- **Lightweight and Complementary**: UniGame minimally increases model complexity (<1% additional parameters) and is compatible with existing post-training approaches.\n- **Adversarial Self-Play Principle**: Demonstrates adversarial self-play as a general technique for improving UMM coherence and robustness.\n\n---\n\n### 2. **Methodology**\n- **Self-Adversarial Training**: UniGame adds a lightweight perturbation mechanism (\"perturber\") at the shared token interface of the UMM, targeting areas with fragile understanding.\n- **Post-Training Strategy**: Rather than changing the main architecture or training from scratch, UniGame operates after initial training, focusing on aligning representation spaces for both understanding and generation.\n- **Performance Evaluation**: The framework is tested across standard benchmarks for understanding, generation, out-of-distribution, and adversarial robustness.\n\n---\n\n### 3. **Results**\n- **Improved Consistency**: +4.6% better alignment between understanding and generation branches.\n- **Enhanced Understanding**: +3.6% improvement in understanding tasks.\n- **Generation Quality**: Marginal increase in generation performance (+0.02).\n- **Robustness**: Significant gains in out-of-distribution (+4.8% NaturalBench) and adversarial (+6.2% AdVQA) robustness.\n- **Efficient Implementation**: Achieves these results with minimal additional parameters (<1%).\n\n---\n\n### 4. **Answered Questions**\n- **How can representation misalignments in UMMs be addressed without architectural changes?**\n- **Can self-adversarial mechanisms improve cross-modal coherence and robustness in foundation models?**\n- **Is it possible to enhance model performance post hoc and with low overhead?**\n\n---\n\n### 5. **Future Research**\n- **Broader Application**: Extending adversarial self-play to other multimodal and foundation model architectures.\n- **Perturber Design**: Exploring more sophisticated or task-adaptive perturber mechanisms.\n- **Integration with Pretraining**: Studying the effects of adversarial self-play during full model training rather than post-training alone.\n- **Combinatorial Approaches**: Assessing the integration of UniGame with other post-training and regularization strategies.\n\n---\n\n**Summary**:  \nUniGame is a lightweight, architecture-agnostic post-training framework that uses adversarial self-play to realign understanding and generation in unified multimodal models, improving consistency, robustness, and coherence with minimal overhead and broad compatibility.", "keywords": ["Unified Multimodal Models", "self-adversarial post-training", "adversarial self-play", "cross-modal coherence", "out-of-distribution robustness", "adversarial robustness", "compact embeddings", "reconstruction-rich representations", "lightweight perturber", "architecture-agnostic"], "categories": ["cs.LG", "cs.AI", "cs.CV"], "primary_category": "cs.LG", "published": "2025-11-24T18:50:01Z", "updated": "2025-11-24T18:50:01Z", "pdf_url": "https://arxiv.org/pdf/2511.19413v1", "full_text": "", "status": "new", "notes": "", "manual_tags": [], "created_at": "2025-11-25T07:56:24.954836", "updated_at": "2025-11-25T07:56:24.954836", "has_placeholder_summary": false, "has_placeholder_keywords": false, "has_embedding": true}, "14": {"id": "2511.19396", "arxiv_id": "2511.19396", "title": "Real-Time Object Tracking with On-Device Deep Learning for Adaptive Beamforming in Dynamic Acoustic Environments", "authors": ["Jorge Ortigoso-Narro", "Jose A. Belloch", "Adrian Amor-Martin", "Sandra Roger", "Maximo Cobos"], "abstract": "Advances in object tracking and acoustic beamforming are driving new capabilities in surveillance, human-computer interaction, and robotics. This work presents an embedded system that integrates deep learning-based tracking with beamforming to achieve precise sound source localization and directional audio capture in dynamic environments. The approach combines single-camera depth estimation and stereo vision to enable accurate 3D localization of moving objects. A planar concentric circular microphone array constructed with MEMS microphones provides a compact, energy-efficient platform supporting 2D beam steering across azimuth and elevation. Real-time tracking outputs continuously adapt the array's focus, synchronizing the acoustic response with the target's position. By uniting learned spatial awareness with dynamic steering, the system maintains robust performance in the presence of multiple or moving sources. Experimental evaluation demonstrates significant gains in signal-to-interference ratio, making the design well-suited for teleconferencing, smart home devices, and assistive technologies.", "summary": "**Structured Summary**\n\n---\n\n### 1. Key Contributions\n\n- **Integrated Embedded System:** Presents a novel, real-time embedded system uniting deep-learning-based object tracking and adaptive acoustic beamforming.\n- **3D Localization with Depth & Stereo Vision:** Combines single-camera depth estimation and stereo vision for accurate 3D positioning of moving sound sources.\n- **MEMS Microphone Array Design:** Introduces a compact, planar concentric circular array using MEMS microphones for efficient 2D beam steering (azimuth + elevation).\n- **Dynamic Acoustic Response:** Enables continuous, real-time adaptation of beamforming based on live tracking, synchronizing audio capture with target movement.\n- **Robustness in Dynamic Environments:** Demonstrates maintained performance in challenging scenarios with multiple and/or moving sound sources.\n\n---\n\n### 2. Methodology\n\n- **Computer Vision-Based Tracking:** Utilizes deep neural networks for object localization, estimating 3D positions via fused single-camera depth and stereo vision.\n- **Embedded Implementation:** All processing\u2014tracking and beamforming\u2014is performed on-device for real-time, low-latency operation.\n- **Acoustic Beamforming:** Employs a planar MEMS microphone array enabling dynamic 2D steering, controlled by the object tracker to focus on target sound sources.\n- **Synchronization:** Real-time feedback loop links vision-based localization with microphone array steering, allowing adaptive audio directionality.\n- **Experimental Evaluation:** Benchmarked performance in real acoustic environments with varying source movement and interference.\n\n---\n\n### 3. Results\n\n- **Improved Signal-to-Interference Ratio (SIR):** System achieves substantial SIR gains, demonstrating effectiveness in isolating moving sources over static or conventional approaches.\n- **Real-Time Adaptation:** Successfully tracks and maintains acoustic focus on dynamic targets amidst multiple sources, validating robustness.\n- **Applicability:** Shown suitable for teleconferencing, smart home, and assistive tech scenarios, indicating broad practical utility.\n\n---\n\n### 4. Answered Questions\n\n- **Can deep learning-based tracking be effectively integrated with beamforming on embedded hardware for dynamic environments?**  \n  *Yes; the proposed system achieves low-latency, real-time adaptation, and improved sound source localization.*\n- **Is accurate 3D localization feasible on resource-constrained platforms?**  \n  *Yes; fusion of monocular depth and stereo vision enables this capability on-device.*\n- **Does synchronizing object tracking with adaptive beamforming yield tangible performance gains?**  \n  *Yes; demonstrated by marked increases in SIR and source selectivity in experiments.*\n\n---\n\n### 5. Future Research\n\n- **Multi-Object Tracking & Segregation:** Extension to simultaneous, independent tracking and steering for multiple sources.\n- **Enhanced Depth Estimation:** Incorporation of advanced 3D sensing or improved neural methods for more precise localization.\n- **Scalability & Array Design:** Investigation of different array geometries or larger microphone grids for greater directionality or coverage.\n- **Model Optimization:** Further reduction of computational and energy costs for deployment in ultra-low-power or mobile devices.\n- **User Interaction & Control:** Integration with intuitive control interfaces or voice-activated systems for user-driven steering.", "keywords": ["Real-time object tracking", "on-device deep learning", "adaptive beamforming", "dynamic acoustic environments", "3D localization", "MEMS microphone array", "single-camera depth estimation", "stereo vision", "directional audio capture", "signal-to-interference ratio"], "categories": ["cs.SD", "cs.AI", "cs.CV"], "primary_category": "cs.SD", "published": "2025-11-24T18:33:50Z", "updated": "2025-11-24T18:33:50Z", "pdf_url": "https://arxiv.org/pdf/2511.19396v1", "full_text": "", "status": "new", "notes": "", "manual_tags": [], "created_at": "2025-11-25T07:56:47.405909", "updated_at": "2025-11-25T07:56:47.405910", "has_placeholder_summary": false, "has_placeholder_keywords": false, "has_embedding": true}, "15": {"id": "2511.19390", "arxiv_id": "2511.19390", "title": "Predicting partially observable dynamical systems via diffusion models with a multiscale inference scheme", "authors": ["Rudy Morel", "Francesco Pio Ramunno", "Jeff Shen", "Alberto Bietti", "Kyunghyun Cho", "Miles Cranmer", "Siavash Golkar", "Olexandr Gugnin", "Geraud Krawezik", "Tanya Marwah", "Michael McCabe", "Lucas Meyer", "Payel Mukhopadhyay", "Ruben Ohana", "Liam Parker", "Helen Qu", "Fran\u00e7ois Rozet", "K. D. Leka", "Fran\u00e7ois Lanusse", "David Fouhey", "Shirley Ho"], "abstract": "Conditional diffusion models provide a natural framework for probabilistic prediction of dynamical systems and have been successfully applied to fluid dynamics and weather prediction. However, in many settings, the available information at a given time represents only a small fraction of what is needed to predict future states, either due to measurement uncertainty or because only a small fraction of the state can be observed. This is true for example in solar physics, where we can observe the Sun's surface and atmosphere, but its evolution is driven by internal processes for which we lack direct measurements. In this paper, we tackle the probabilistic prediction of partially observable, long-memory dynamical systems, with applications to solar dynamics and the evolution of active regions. We show that standard inference schemes, such as autoregressive rollouts, fail to capture long-range dependencies in the data, largely because they do not integrate past information effectively. To overcome this, we propose a multiscale inference scheme for diffusion models, tailored to physical processes. Our method generates trajectories that are temporally fine-grained near the present and coarser as we move farther away, which enables capturing long-range temporal dependencies without increasing computational cost. When integrated into a diffusion model, we show that our inference scheme significantly reduces the bias of the predicted distributions and improves rollout stability.", "summary": "**Structured Summary**\n\n---\n\n### 1. Key Contributions\n\n- **Multiscale Inference Scheme**: Introduces a novel inference approach for conditional diffusion models, specifically designed for probabilistic prediction in partially observable and long-memory dynamical systems.\n- **Temporal Granularity Adaptation**: Proposes generating model predictions that are fine-grained near the present and coarser in the future, capturing long-range dependencies efficiently.\n- **Improvement in Physical System Modeling**: Demonstrates applicability to solar physics, enabling better predictions of solar dynamics and evolution of active regions with incomplete observations.\n- **Enhanced Prediction Stability**: Shows that the multiscale approach reduces bias and improves the stability of long-term rollouts in diffusion-based models.\n\n---\n\n### 2. Methodology\n\n- **Conditional Diffusion Models**: Builds on probabilistic generative models conditioned on observed states.\n- **Partially Observable Setting**: Focuses on scenarios with incomplete or uncertain observations, typical in complex physical systems.\n- **Multiscale Inference**: Designs an inference routine where short-term predictions are made with high temporal resolution, while long-term predictions use coarser time steps.\n- **Comparative Analysis**: Evaluates proposed method against standard autoregressive rollout schemes to assess improvements in capturing long-range dependencies.\n\n---\n\n### 3. Results\n\n- **Reduced Predictive Bias**: The multiscale inference scheme leads to more accurate probability distributions over future system states.\n- **Improved Long-Range Prediction**: Demonstrates greater rollout stability and better preservation of long-term dependencies compared to standard approaches.\n- **Application to Solar Physics**: Provides empirical results indicating the method\u2019s effectiveness for predicting solar dynamics with limited observations.\n\n---\n\n### 4. Answered Questions\n\n- **How can we improve probabilistic predictions of partially observable, long-memory dynamical systems?**\n  - By integrating a multiscale inference scheme into diffusion models, which adapts temporal resolution according to predictive distance.\n- **Why do standard autoregressive rollouts fail in such settings?**\n  - Because they inadequately incorporate historical information, leading to loss of long-range dependencies and biased predictions.\n\n---\n\n### 5. Future Research\n\n- **Extension to Other Domains**: Applying the multiscale inference scheme to different partially observable dynamical systems beyond solar physics (e.g., climate modeling, biological processes).\n- **Integration with Other Model Types**: Exploring compatibility with other probabilistic sequence models (e.g., transformers, recurrent neural networks).\n- **Adaptive Granularity**: Investigating methods for dynamically determining the optimal temporal resolution based on system state or prediction uncertainty.\n- **Observational Uncertainty Modeling**: Further analysis on handling severe measurement uncertainty or missing data scenarios.\n\n---", "keywords": ["partial observability", "dynamical systems", "diffusion models", "probabilistic prediction", "multiscale inference scheme", "long-range temporal dependencies", "solar dynamics", "active region evolution", "trajectory generation", "rollout stability"], "categories": ["cs.LG", "astro-ph.SR", "cs.AI", "stat.ML"], "primary_category": "cs.LG", "published": "2025-11-24T18:30:04Z", "updated": "2025-11-24T18:30:04Z", "pdf_url": "https://arxiv.org/pdf/2511.19390v1", "full_text": "", "status": "new", "notes": "", "manual_tags": [], "created_at": "2025-11-25T07:56:50.750642", "updated_at": "2025-11-25T07:56:50.750643", "has_placeholder_summary": false, "has_placeholder_keywords": false, "has_embedding": true}, "16": {"id": "2511.19367", "arxiv_id": "2511.19367", "title": "An Anatomy Aware Hybrid Deep Learning Framework for Lung Cancer Tumor Stage Classification", "authors": ["Saniah Kayenat Chowdhury", "Rusab Sarmun", "Muhammad E. H. Chowdhury", "Sohaib Bassam Zoghoul", "Israa Al-Hashimi", "Adam Mushtak", "Amith Khandakar"], "abstract": "Accurate lung cancer tumor staging is crucial for prognosis and treatment planning. However, it remains challenging for end-to-end deep learning approaches, as such approaches often overlook spatial and anatomical information that are central to the tumor-node-metastasis system. The tumor stage depends on multiple quantitative criteria, including the tumor size and its proximity to the nearest anatomical structures, and small variations can alter the staging outcome. We propose a medically grounded hybrid pipeline that performs staging by explicitly measuring the tumor's size and distance properties rather than treating it as a pure image classification task. Our method employs specialized encoder-decoder networks to precisely segment the lung and adjacent anatomy, including the lobes, tumor, mediastinum, and diaphragm. Subsequently, we extract the necessary tumor properties, i.e. measure the largest tumor dimension and calculate the distance between the tumor and neighboring anatomical structures by a quantitative analysis of the segmentation masks. Finally, we apply rule-based tumor staging aligned with the medical guidelines. This novel framework has been evaluated on the Lung-PET-CT-Dx dataset, demonstrating superior performance compared to traditional deep learning models, achieving an overall classification accuracy of 91.36%. We report the per-stage F1-scores of 0.93 (T1), 0.89 (T2), 0.96 (T3), and 0.90 (T4), a critical evaluation aspect often omitted in prior literature. To our knowledge, this is the first study that embeds explicit clinical context into tumor stage classification. Unlike standard convolutional neural networks that operate in an uninterpretable \"black box\" manner, our method offers both state-of-the-art performance and transparent decision support.", "summary": "**1. Key Contributions**\n- Introduces a hybrid deep learning framework for lung cancer tumor stage classification that explicitly incorporates anatomical and spatial information.\n- Utilizes clinically relevant, quantitative metrics (tumor size and proximity to anatomical structures) to drive staging decisions, improving interpretability and clinical relevance.\n- Achieves transparent, rule-based staging aligned with medical guidelines, moving beyond traditional \u201cblack box\u201d CNN image classifiers.\n- Provides detailed per-stage evaluation (F1-scores), addressing a common gap in prior literature.\n- Is, to the authors' knowledge, the first model embedding explicit clinical context into automated tumor staging.\n\n**2. Methodology**\n- Employs specialized encoder-decoder networks to segment lung CT images into regions: lung, lobes, tumor, mediastinum, and diaphragm.\n- Extracts quantitative tumor properties (largest dimension and distances to critical anatomical structures) from segmentation masks.\n- Classifies tumor stage using rule-based logic based on extracted measurements, consistent with established medical guidelines.\n- Evaluates the proposed pipeline on the Lung-PET-CT-Dx dataset, comparing it to conventional deep learning models.\n\n**3. Results**\n- The hybrid framework achieves an overall stage classification accuracy of 91.36% on the test dataset.\n- Reports strong per-stage F1-scores: 0.93 (T1), 0.89 (T2), 0.96 (T3), 0.90 (T4).\n- Outperforms traditional deep learning-based classifiers in both accuracy and clinical interpretability.\n\n**4. Answered Questions**\n- Demonstrates that incorporating explicit anatomical and spatial information significantly improves lung cancer stage classification.\n- Shows that a hybrid approach combining deep learning segmentation and rule-based staging offers superior performance and interpretability over end-to-end image classifiers.\n- Validates the feasibility and benefit of aligning automated tumor staging frameworks with clinical practice and guidelines.\n\n**5. Future Research**\n- Extending the framework to include additional anatomical or functional image features (e.g., PET uptake).\n- Adapting the approach to other cancer types or anatomical staging systems.\n- Integrating the framework into clinical decision support tools and validating in prospective, real-world clinical studies.\n- Exploring semi- or fully-automated end-to-end architectures that maintain both interpretability and performance.", "keywords": ["lung cancer staging", "hybrid deep learning", "anatomical segmentation", "tumor-node-metastasis system", "encoder-decoder networks", "quantitative tumor analysis", "rule-based staging", "Lung-PET-CT-Dx dataset", "classification accuracy", "interpretable decision support"], "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV", "published": "2025-11-24T18:01:47Z", "updated": "2025-11-24T18:01:47Z", "pdf_url": "https://arxiv.org/pdf/2511.19367v1", "full_text": "", "status": "new", "notes": "", "manual_tags": [], "created_at": "2025-11-25T07:57:05.689666", "updated_at": "2025-11-25T07:57:05.689667", "has_placeholder_summary": false, "has_placeholder_keywords": false, "has_embedding": true}}}