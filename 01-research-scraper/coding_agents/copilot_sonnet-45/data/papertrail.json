{
  "papers": {
    "1": {
      "id": "2103.00020",
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "authors": [
        {
          "name": "Alec Radford",
          "affiliation": null
        },
        {
          "name": "Jong Wook Kim",
          "affiliation": null
        },
        {
          "name": "Chris Hallacy",
          "affiliation": null
        },
        {
          "name": "Aditya Ramesh",
          "affiliation": null
        },
        {
          "name": "Gabriel Goh",
          "affiliation": null
        },
        {
          "name": "Sandhini Agarwal",
          "affiliation": null
        },
        {
          "name": "Girish Sastry",
          "affiliation": null
        },
        {
          "name": "Amanda Askell",
          "affiliation": null
        },
        {
          "name": "Pamela Mishkin",
          "affiliation": null
        },
        {
          "name": "Jack Clark",
          "affiliation": null
        },
        {
          "name": "Gretchen Krueger",
          "affiliation": null
        },
        {
          "name": "Ilya Sutskever",
          "affiliation": null
        }
      ],
      "abstract": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",
      "arxiv_id": "2103.00020",
      "arxiv_url": "http://arxiv.org/abs/2103.00020v1",
      "pdf_url": "https://arxiv.org/pdf/2103.00020v1",
      "published": "2021-02-26T19:04:58Z",
      "updated": "2021-02-26T19:04:58Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "full_text": "Learning Transferable Visual Models From Natural Language Supervision\nAlec Radford* 1Jong Wook Kim* 1Chris Hallacy1Aditya Ramesh1Gabriel Goh1Sandhini Agarwal1\nGirish Sastry1Amanda Askell1Pamela Mishkin1Jack Clark1Gretchen Krueger1Ilya Sutskever1\nAbstract\nState-of-the-art computer vision systems are\ntrained to predict a \ufb01xed set of predetermined\nobject categories. This restricted form of super-\nvision limits their generality and usability since\nadditional labeled data is needed to specify any\nother visual concept. Learning directly from raw\ntext about images is a promising alternative which\nleverages a much broader source of supervision.\nWe demonstrate that the simple pre-training task\nof predicting which caption goes with which im-\nage is an ef\ufb01cient and scalable way to learn SOTA\nimage representations from scratch on a dataset\nof 400 million (image, text) pairs collected from\nthe internet. After pre-training, natural language\nis used to reference learned visual concepts (or\ndescribe new ones) enabling zero-shot transfer\nof the model to downstream tasks. We study\nthe performance of this approach by benchmark-\ning on over 30 different existing computer vi-\nsion datasets, spanning tasks such as OCR, ac-\ntion recognition in videos, geo-localization, and\nmany types of \ufb01ne-grained object classi\ufb01cation.\nThe model transfers non-trivially to most tasks\nand is often competitive with a fully supervised\nbaseline without the need for any dataset spe-\nci\ufb01c training. For instance, we match the ac-\ncuracy of the original ResNet-50 on ImageNet\nzero-shot without needing to use any of the 1.28\nmillion training examples it was trained on. We\nrelease our code and pre-trained model weights at\nhttps://github.com/OpenAI/CLIP .\n1. Introduction and Motivating Work\nPre-training methods which learn directly from raw text\nhave revolutionized NLP over the last few years (Dai &\nLe, 2015; Peters et al., 2018; Howard & Ruder, 2018; Rad-\nford et al., 2018; Devlin et al., 2018; Raffel et al., 2019).\n*Equal contribution1OpenAI, San Francisco, CA 94110, USA.\nCorrespondence to: <falec, jongwookg@openai.com >.Task-agnostic objectives such as autoregressive and masked\nlanguage modeling have scaled across many orders of mag-\nnitude in compute, model capacity, and data, steadily im-\nproving capabilities. The development of \u201ctext-to-text\u201d as\na standardized input-output interface (McCann et al., 2018;\nRadford et al., 2019; Raffel et al., 2019) has enabled task-\nagnostic architectures to zero-shot transfer to downstream\ndatasets removing the need for specialized output heads or\ndataset speci\ufb01c customization. Flagship systems like GPT-3\n(Brown et al., 2020) are now competitive across many tasks\nwith bespoke models while requiring little to no dataset\nspeci\ufb01c training data.\nThese results suggest that the aggregate supervision acces-\nsible to modern pre-training methods within web-scale col-\nlections of text surpasses that of high-quality crowd-labeled\nNLP datasets. However, in other \ufb01elds such as computer\nvision it is still standard practice to pre-train models on\ncrowd-labeled datasets such as ImageNet (Deng et al., 2009).\nCould scalable pre-training methods which learn directly\nfrom web text result in a similar breakthrough in computer\nvision? Prior work is encouraging.\nOver 20 years ago Mori et al. (1999) explored improving\ncontent based image retrieval by training a model to pre-\ndict the nouns and adjectives in text documents paired with\nimages. Quattoni et al. (2007) demonstrated it was possi-\nble to learn more data ef\ufb01cient image representations via\nmanifold learning in the weight space of classi\ufb01ers trained\nto predict words in captions associated with images. Sri-\nvastava & Salakhutdinov (2012) explored deep represen-\ntation learning by training multimodal Deep Boltzmann\nMachines on top of low-level image and text tag features.\nJoulin et al. (2016) modernized this line of work and demon-\nstrated that CNNs trained to predict words in image cap-\ntions learn useful image representations. They converted\nthe title, description, and hashtag metadata of images in the\nYFCC100M dataset (Thomee et al., 2016) into a bag-of-\nwords multi-label classi\ufb01cation task and showed that pre-\ntraining AlexNet (Krizhevsky et al., 2012) to predict these\nlabels learned representations which preformed similarly\nto ImageNet-based pre-training on transfer tasks. Li et al.\n(2017) then extended this approach to predicting phrase n-\ngrams in addition to individual words and demonstrated the\nability of their system to zero-shot transfer to other imagearXiv:2103.00020v1  [cs.CV]  26 Feb 2021\nLearning Transferable Visual Models From Natural Language Supervision 2\nI1 \u00b7T2 I1 \u00b7T3\u2026\nI2 \u00b7T1 I2 \u00b7T3\u2026\nI 3 \u00b7T 1 I 3 \u00b7T 2 \u2026\n\u22ee \u22ee \u22eeI1 \u00b7T1\nI2 \u00b7T2\nI 3 \u00b7T 3(1) Contrastive pre-training\nImage\nEncoderT ext\nEncoderPepper\tthe\naussie\tpupPepper\tthe\naussie\tpupPepper\tthe\naussie\tpupPepper\tthe\naussie\tpup\nT1 T2 T3\u2026\nI1\nI2\nI 3\n\u22ee(2) Create dataset classi\ufb01er from label text\nplane\ncar\ndog\n\u22ee\nbirdA\tphoto\tof\na\t{object} .\n\u22eeT ext\nEncoder\nT1 T2 T3 TN\u2026\n(3) Use for zero-shot prediction\nImage\nEncoderI1 I1 \u00b7T2 I1 \u00b7TN I1 \u00b7T1\u2026\n\u2026\nA\tphoto\tof\n\ta\tdog.TN\nIN \u00b7T1 IN \u00b7T2 IN \u00b7T3I1 \u00b7TN\nI2 \u00b7TN\nI 3 \u00b7T N\n\u22ee\n\u2026 IN\u2026\n\u22ee \u22f1\nIN \u00b7TNI1 \u00b7T3\nFigure 1. Summary of our approach. While standard image models jointly train an image feature extractor and a linear classi\ufb01er to predict\nsome label, CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training\nexamples. At test time the learned text encoder synthesizes a zero-shot linear classi\ufb01er by embedding the names or descriptions of the\ntarget dataset\u2019s classes.\nclassi\ufb01cation datasets by scoring target classes based on\ntheir dictionary of learned visual n-grams and predicting the\none with the highest score. Adopting more recent architec-\ntures and pre-training approaches, VirTex (Desai & Johnson,\n2020), ICMLM (Bulent Sariyildiz et al., 2020), and Con-\nVIRT (Zhang et al., 2020) have recently demonstrated the\npotential of transformer-based language modeling, masked\nlanguage modeling, and contrastive objectives to learn im-\nage representations from text.\nWhile exciting as proofs of concept, using natural language\nsupervision for image representation learning is still rare.\nThis is likely because demonstrated performance on com-\nmon benchmarks is much lower than alternative approaches.\nFor example, Li et al. (2017) reach only 11.5% accuracy\non ImageNet in a zero-shot setting. This is well below the\n88.4% accuracy of the current state of the art (Xie et al.,\n2020). It is even below the 50% accuracy of classic com-\nputer vision approaches (Deng et al., 2012). Instead, more\nnarrowly scoped but well-targeted uses of weak supervision\nhave improved performance. Mahajan et al. (2018) showed\nthat predicting ImageNet-related hashtags on Instagram im-\nages is an effective pre-training task. When \ufb01ne-tuned to\nImageNet these pre-trained models increased accuracy by\nover 5% and improved the overall state of the art at the time.\nKolesnikov et al. (2019) and Dosovitskiy et al. (2020) have\nalso demonstrated large gains on a broader set of transfer\nbenchmarks by pre-training models to predict the classes of\nthe noisily labeled JFT-300M dataset.\nThis line of work represents the current pragmatic middle\nground between learning from a limited amount of super-\nvised \u201cgold-labels\u201d and learning from practically unlimited\namounts of raw text. However, it is not without compro-mises. Both works carefully design, and in the process limit,\ntheir supervision to 1000 and 18291 classes respectively.\nNatural language is able to express, and therefore supervise,\na much wider set of visual concepts through its general-\nity. Both approaches also use static softmax classi\ufb01ers to\nperform prediction and lack a mechanism for dynamic out-\nputs. This severely curtails their \ufb02exibility and limits their\n\u201czero-shot\u201d capabilities.\nA crucial difference between these weakly supervised mod-\nels and recent explorations of learning image representations\ndirectly from natural language is scale. While Mahajan et al.\n(2018) and Kolesnikov et al. (2019) trained their models for\naccelerator years on millions to billions of images, VirTex,\nICMLM, and ConVIRT trained for accelerator days on one\nto two hundred thousand images. In this work, we close\nthis gap and study the behaviors of image classi\ufb01ers trained\nwith natural language supervision at large scale. Enabled\nby the large amounts of publicly available data of this form\non the internet, we create a new dataset of 400 million (im-\nage, text) pairs and demonstrate that a simpli\ufb01ed version of\nConVIRT trained from scratch, which we call CLIP, for Con-\ntrastive Language-Image Pre-training, is an ef\ufb01cient method\nof learning from natural language supervision. We study\nthe scalability of CLIP by training a series of eight models\nspanning almost 2 orders of magnitude of compute and ob-\nserve that transfer performance is a smoothly predictable\nfunction of compute (Hestness et al., 2017; Kaplan et al.,\n2020). We \ufb01nd that CLIP, similar to the GPT family, learns\nto perform a wide set of tasks during pre-training including\nOCR, geo-localization, action recognition, and many others.\nWe measure this by benchmarking the zero-shot transfer\nperformance of CLIP on over 30 existing datasets and \ufb01nd\nLearning Transferable Visual Models From Natural Language Supervision 3\n2M33M 67M 134M 268M 400M\n# of images processed0510152025303540Zero-Shot ImageNet Accuracy\n3X efficiency 4X efficiency\nBag of Words Contrastive (CLIP)\nBag of Words Prediction\nTransformer Language Model\nFigure 2. CLIP is much more ef\ufb01cient at zero-shot transfer\nthan our image caption baseline. Although highly expressive,\nwe found that transformer-based language models are relatively\nweak at zero-shot ImageNet classi\ufb01cation. Here, we see that it\nlearns 3x slower than a baseline which predicts a bag-of-words\n(BoW) encoding of the text (Joulin et al., 2016). Swapping the\nprediction objective for the contrastive objective of CLIP further\nimproves ef\ufb01ciency another 4x.\nit can be competitive with prior task-speci\ufb01c supervised\nmodels. We also con\ufb01rm these \ufb01ndings with linear-probe\nrepresentation learning analysis and show that CLIP out-\nperforms the best publicly available ImageNet model while\nalso being more computationally ef\ufb01cient. We additionally\n\ufb01nd that zero-shot CLIP models are much more robust than\nequivalent accuracy supervised ImageNet models which\nsuggests that zero-shot evaluation of task-agnostic models is\nmuch more representative of a model\u2019s capability. These re-\nsults have signi\ufb01cant policy and ethical implications, which\nwe consider in Section 7.\n2. Approach\n2.1. Natural Language Supervision\nAt the core of our approach is the idea of learning percep-\ntion from supervision contained in natural language. As\ndiscussed in the introduction, this is not at all a new idea,\nhowever terminology used to describe work in this space\nis varied, even seemingly contradictory, and stated motiva-\ntions are diverse. Zhang et al. (2020), Gomez et al. (2017),\nJoulin et al. (2016), and Desai & Johnson (2020) all intro-\nduce methods which learn visual representations from text\npaired with images but describe their approaches as unsuper-\nvised, self-supervised, weakly supervised, and supervised\nrespectively.\nWe emphasize that what is common across this line of work\nis not any of the details of the particular methods used but\nthe appreciation of natural language as a training signal. All\nthese approaches are learning from natural language super-vision . Although early work wrestled with the complexity\nof natural language when using topic model and n-gram\nrepresentations, improvements in deep contextual represen-\ntation learning suggest we now have the tools to effectively\nleverage this abundant source of supervision (McCann et al.,\n2017).\nLearning from natural language has several potential\nstrengths over other training methods. It\u2019s much easier\nto scale natural language supervision compared to standard\ncrowd-sourced labeling for image classi\ufb01cation since it does\nnot require annotations to be in a classic \u201cmachine learning\ncompatible format\u201d such as the canonical 1-of-N majority\nvote \u201cgold label\u201d. Instead, methods which work on natural\nlanguage can learn passively from the supervision contained\nin the vast amount of text on the internet. Learning from\nnatural language also has an important advantage over most\nunsupervised or self-supervised learning approaches in that\nit doesn\u2019t \u201cjust\u201d learn a representation but also connects that\nrepresentation to language which enables \ufb02exible zero-shot\ntransfer. In the following subsections, we detail the speci\ufb01c\napproach we settled on.\n2.2. Creating a Suf\ufb01ciently Large Dataset\nExisting work has mainly used three datasets, MS-COCO\n(Lin et al., 2014), Visual Genome (Krishna et al., 2017), and\nYFCC100M (Thomee et al., 2016). While MS-COCO and\nVisual Genome are high quality crowd-labeled datasets, they\nare small by modern standards with approximately 100,000\ntraining photos each. By comparison, other computer vision\nsystems are trained on up to 3.5 billion Instagram photos\n(Mahajan et al., 2018). YFCC100M, at 100 million photos,\nis a possible alternative, but the metadata for each image is\nsparse and of varying quality. Many images use automati-\ncally generated \ufb01lenames like 20160716 113957.JPG\nas \u201ctitles\u201d or contain \u201cdescriptions\u201d of camera exposure\nsettings. After \ufb01ltering to keep only images with natural\nlanguage titles and/or descriptions in English, the dataset\nshrunk by a factor of 6 to only 15 million photos. This is\napproximately the same size as ImageNet.\nA major motivation for natural language supervision is the\nlarge quantities of data of this form available publicly on the\ninternet. Since existing datasets do not adequately re\ufb02ect\nthis possibility, considering results only on them would un-\nderestimate the potential of this line of research. To address\nthis, we constructed a new dataset of 400 million (image,\ntext) pairs collected form a variety of publicly available\nsources on the Internet. To attempt to cover as broad a set\nof visual concepts as possible, we search for (image, text)\npairs as part of the construction process whose text includes\none of a set of 500,000 queries.1We approximately class\n1The base query list is all words occurring at least 100 times in\nthe English version of Wikipedia. This is augmented with bi-grams\nLearning Transferable Visual Models From Natural Language Supervision 4\nbalance the results by including up to 20,000 (image, text)\npairs per query. The resulting dataset has a similar total\nword count as the WebText dataset used to train GPT-2. We\nrefer to this dataset as WIT for WebImageText.\n2.3. Selecting an Ef\ufb01cient Pre-Training Method\nState-of-the-art computer vision systems use very large\namounts of compute. Mahajan et al. (2018) required 19\nGPU years to train their ResNeXt101-32x48d and Xie et al.\n(2020) required 33 TPUv3 core-years to train their Noisy\nStudent Ef\ufb01cientNet-L2. When considering that both these\nsystems were trained to predict only 1000 ImageNet classes,\nthe task of learning an open set of visual concepts from\nnatural language seems daunting. In the course of our ef-\nforts, we found training ef\ufb01ciency was key to successfully\nscaling natural language supervision and we selected our\n\ufb01nal pre-training method based on this metric.\nOur initial approach, similar to VirTex, jointly trained an\nimage CNN and text transformer from scratch to predict the\ncaption of an image. However, we encountered dif\ufb01culties\nef\ufb01ciently scaling this method. In Figure 2 we show that a\n63 million parameter transformer language model, which\nalready uses twice the compute of its ResNet-50 image\nencoder, learns to recognize ImageNet classes three times\nslower than a much simpler baseline that predicts a bag-of-\nwords encoding of the same text.\nBoth these approaches share a key similarity. They try to pre-\ndict the exact words of the text accompanying each image.\nThis is a dif\ufb01cult task due to the wide variety of descriptions,\ncomments, and related text that co-occur with images. Re-\ncent work in contrastive representation learning for images\nhas found that contrastive objectives can learn better repre-\nsentations than their equivalent predictive objective (Tian\net al., 2019). Other work has found that although generative\nmodels of images can learn high quality image representa-\ntions, they require over an order of magnitude more compute\nthan contrastive models with the same performance (Chen\net al., 2020a). Noting these \ufb01ndings, we explored training\na system to solve the potentially easier proxy task of pre-\ndicting only which text as a whole is paired with which\nimage and not the exact words of that text. Starting with\nthe same bag-of-words encoding baseline, we swapped the\npredictive objective for a contrastive objective in Figure 2\nand observed a further 4x ef\ufb01ciency improvement in the rate\nof zero-shot transfer to ImageNet.\nGiven a batch of N(image, text) pairs, CLIP is trained to\npredict which of the N\u0002Npossible (image, text) pairings\nacross a batch actually occurred. To do this, CLIP learns a\nwith high pointwise mutual information as well as the names of\nall Wikipedia articles above a certain search volume. Finally all\nWordNet synsets not already in the query list are added.multi-modal embedding space by jointly training an image\nencoder and text encoder to maximize the cosine similar-\nity of the image and text embeddings of the Nreal pairs\nin the batch while minimizing the cosine similarity of the\nembeddings of the N2\u0000Nincorrect pairings. We opti-\nmize a symmetric cross entropy loss over these similarity\nscores. In Figure 3 we include pseudocode of the core of an\nimplementation of CLIP. To our knowledge this batch con-\nstruction technique and objective was \ufb01rst introduced in the\narea of deep metric learning as the multi-class N-pair loss\nSohn (2016), was popularized for contrastive representation\nlearning by Oord et al. (2018) as the InfoNCE loss, and was\nrecently adapted for contrastive (text, image) representation\nlearning in the domain of medical imaging by Zhang et al.\n(2020).\nDue to the large size of our pre-training dataset, over-\ufb01tting\nis not a major concern and the details of training CLIP are\nsimpli\ufb01ed compared to the implementation of Zhang et al.\n(2020). We train CLIP from scratch without initializing the\nimage encoder with ImageNet weights or the text encoder\nwith pre-trained weights. We do not use the non-linear\nprojection between the representation and the contrastive\nembedding space, a change which was introduced by Bach-\nman et al. (2019) and popularized by Chen et al. (2020b).\nWe instead use only a linear projection to map from each en-\ncoder\u2019s representation to the multi-modal embedding space.\nWe did not notice a difference in training ef\ufb01ciency between\nthe two versions and speculate that non-linear projections\nmay be co-adapted with details of current image only in\nself-supervised representation learning methods. We also\nremove the text transformation function tufrom Zhang et al.\n(2020) which samples a single sentence at uniform from\nthe text since many of the (image, text) pairs in CLIP\u2019s pre-\ntraining dataset are only a single sentence. We also simplify\nthe image transformation function tv. A random square\ncrop from resized images is the only data augmentation\nused during training. Finally, the temperature parameter\nwhich controls the range of the logits in the softmax, \u001c, is\ndirectly optimized during training as a log-parameterized\nmultiplicative scalar to avoid turning as a hyper-parameter.\n2.4. Choosing and Scaling a Model\nWe consider two different architectures for the image en-\ncoder. For the \ufb01rst, we use ResNet-50 (He et al., 2016a)\nas the base architecture for the image encoder due to its\nwidespread adoption and proven performance. We make sev-\neral modi\ufb01cations to the original version using the ResNet-\nD improvements from He et al. (2019) and the antialiased\nrect-2 blur pooling from Zhang (2019). We also replace\nthe global average pooling layer with an attention pooling\nmechanism. The attention pooling is implemented as a sin-\ngle layer of \u201ctransformer-style\u201d multi-head QKV attention\nwhere the query is conditioned on the global average-pooled\nLearning Transferable Visual Models From Natural Language Supervision 5\n# image_encoder - ResNet or Vision Transformer \n# text_encoder  - CBOW or Text Transformer \n# I[n, h, w, c] - minibatch of aligned images \n# T[n, l]       - minibatch of aligned texts \n# W_i[d_i, d_e] - learned proj of image to embed \n# W_t[d_t, d_e] - learned proj of text to embed \n# t             - learned temperature parameter \n# extract feature representations of each modality \nI_f = image_encoder(I) #[n, d_i] \nT_f = text_encoder(T)  #[n, d_t] \n# joint multimodal embedding [n, d_e] \nI_e = l2_normalize(np.dot(I_f, W_i), axis=1) \nT_e = l2_normalize(np.dot(T_f, W_t), axis=1) \n# scaled pairwise cosine similarities [n, n] \nlogits = np.dot(I_e, T_e.T) * np.exp(t) \n# symmetric loss function \nlabels = np.arange(n) \nloss_i = cross_entropy_loss(logits, labels, axis=0) \nloss_t = cross_entropy_loss(logits, labels, axis=1) \nloss   = (loss_i + loss_t)/2 \nFigure 3. Numpy-like pseudocode for the core of an implementa-\ntion of CLIP.\nrepresentation of the image. For the second architecture, we\nexperiment with the recently introduced Vision Transformer\n(ViT) (Dosovitskiy et al., 2020). We closely follow their\nimplementation with only the minor modi\ufb01cation of adding\nan additional layer normalization to the combined patch\nand position embeddings before the transformer and use a\nslightly different initialization scheme.\nThe text encoder is a Transformer (Vaswani et al., 2017)\nwith the architecture modi\ufb01cations described in Radford\net al. (2019). As a base size we use a 63M-parameter 12-\nlayer 512-wide model with 8 attention heads. The trans-\nformer operates on a lower-cased byte pair encoding (BPE)\nrepresentation of the text with a 49,152 vocab size (Sen-\nnrich et al., 2015). For computational ef\ufb01ciency, the max\nsequence length was capped at 76. The text sequence is\nbracketed with [SOS] and[EOS] tokens and the activa-\ntions of the highest layer of the transformer at the [EOS]\ntoken are treated as the feature representation of the text\nwhich is layer normalized and then linearly projected into\nthe multi-modal embedding space. Masked self-attention\nwas used in the text encoder to preserve the ability to ini-\ntialize with a pre-trained language model or add language\nmodeling as an auxiliary objective, though exploration of\nthis is left as future work.\nWhile previous computer vision research has often scaled\nmodels by increasing the width (Mahajan et al., 2018) or\ndepth (He et al., 2016a) in isolation, for the ResNet image\nencoders we adapt the approach of Tan & Le (2019) which\nfound that allocating additional compute across all of width,\ndepth, and resolution outperforms only allocating it to onlyone dimension of the model. While Tan & Le (2019) tune\nthe ratio of compute allocated to each dimension for their\nEf\ufb01cientNet architecture, we use a simple baseline of allo-\ncating additional compute equally to increasing the width,\ndepth, and resolution of the model. For the text encoder, we\nonly scale the width of the model to be proportional to the\ncalculated increase in width of the ResNet and do not scale\nthe depth at all, as we found CLIP\u2019s performance to be less\nsensitive to the capacity of the text encoder.\n2.5. Training\nWe train a series of 5 ResNets and 3 Vision Transformers.\nFor the ResNets we train a ResNet-50, a ResNet-101, and\nthen 3 more which follow Ef\ufb01cientNet-style model scaling\nand use approximately 4x, 16x, and 64x the compute of a\nResNet-50. They are denoted as RN50x4, RN50x16, and\nRN50x64 respectively. For the Vision Transformers we\ntrain a ViT-B/32, a ViT-B/16, and a ViT-L/14. We train all\nmodels for 32 epochs. We use the Adam optimizer (Kingma\n& Ba, 2014) with decoupled weight decay regularization\n(Loshchilov & Hutter, 2017) applied to all weights that are\nnot gains or biases, and decay the learning rate using a\ncosine schedule (Loshchilov & Hutter, 2016). Initial hyper-\nparameters were set using a combination of grid searches,\nrandom search, and manual tuning on the baseline ResNet-\n50 model when trained for 1 epoch. Hyper-parameters were\nthen adapted heuristically for larger models due to compu-\ntational constraints. The learnable temperature parameter\n\u001cwas initialized to the equivalent of 0.07 from (Wu et al.,\n2018) and clipped to prevent scaling the logits by more\nthan 100 which we found necessary to prevent training in-\nstability. We use a very large minibatch size of 32,768.\nMixed-precision (Micikevicius et al., 2017) was used to ac-\ncelerate training and save memory. To save additional mem-\nory, gradient checkpointing (Griewank & Walther, 2000;\nChen et al., 2016), half-precision Adam statistics (Dhariwal\net al., 2020), and half-precision stochastically rounded text\nencoder weights were used. The calculation of embedding\nsimilarities was also sharded with individual GPUs comput-\ning only the subset of the pairwise similarities necessary for\ntheir local batch of embeddings. The largest ResNet model,\nRN50x64, took 18 days to train on 592 V100 GPUs while\nthe largest Vision Transformer took 12 days on 256 V100\nGPUs. For the ViT-L/14 we also pre-train at a higher 336\npixel resolution for one additional epoch to boost perfor-\nmance similar to FixRes (Touvron et al., 2019). We denote\nthis model as ViT-L/14@336px. Unless otherwise speci\ufb01ed,\nall results reported in this paper as \u201cCLIP\u201d use this model\nwhich we found to perform best.\nLearning Transferable Visual Models From Natural Language Supervision 6\n3. Experiments\n3.1. Zero-Shot Transfer\n3.1.1. M OTIVATION\nIn computer vision, zero-shot learning usually refers to the\nstudy of generalizing to unseen object categories in image\nclassi\ufb01cation (Lampert et al., 2009). We instead use the\nterm in a broader sense and study generalization to unseen\ndatasets. We motivate this as a proxy for performing un-\nseen tasks, as aspired to in the zero-data learning paper of\nLarochelle et al. (2008). While much research in the \ufb01eld of\nunsupervised learning focuses on the representation learn-\ningcapabilities of machine learning systems, we motivate\nstudying zero-shot transfer as a way of measuring the task-\nlearning capabilities of machine learning systems. In this\nview, a dataset evaluates performance on a task on a spe-\nci\ufb01c distribution. However, many popular computer vision\ndatasets were created by the research community primarily\nas benchmarks to guide the development of generic image\nclassi\ufb01cation methods rather than measuring performance\non a speci\ufb01c task. While it is reasonable to say that the\nSVHN dataset measures the task of street number transcrip-\ntion on the distribution of Google Street View photos, it is\nunclear what \u201creal\u201d task the CIFAR-10 dataset measures.\nIt is clear, however, what distribution CIFAR-10 is drawn\nfrom - TinyImages (Torralba et al., 2008). On these kinds of\ndatasets, zero-shot transfer is more an evaluation of CLIP\u2019s\nrobustness to distribution shift and domain generalization\nrather than task generalization. Please see Section 3.3 for\nanalysis focused on this.\nTo our knowledge, Visual N-Grams (Li et al., 2017) \ufb01rst\nstudied zero-shot transfer to existing image classi\ufb01cation\ndatasets in the manner described above. It is also the only\nother work we are aware of that has studied zero-shot trans-\nfer to standard image classi\ufb01cation datasets using a gener-\nically pre-trained model and serves as the best reference\npoint for contextualizing CLIP. Their approach learns the\nparameters of a dictionary of 142,806 visual n-grams (span-\nning 1- to 5- grams) and optimizes these n-grams using a\ndifferential version of Jelinek-Mercer smoothing to maxi-\nmize the probability of all text n-grams for a given image.\nIn order to perform zero-shot transfer, they \ufb01rst convert the\ntext of each of the dataset\u2019s class names into its n-gram\nrepresentation and then compute its probability according\nto their model, predicting the one with the highest score.\nOur focus on studying zero-shot transfer as an evaluation of\ntask learning is inspired by work demonstrating task learn-\ning in the \ufb01eld of NLP. To our knowledge Liu et al. (2018)\n\ufb01rst identi\ufb01ed task learning as an \u201cunexpected side-effect\u201d\nwhen a language model trained to generate Wikipedia ar-\nticles learned to reliably transliterate names between lan-\nguages. While GPT-1 (Radford et al., 2018) focused on pre-training as a transfer learning method to improve supervised\n\ufb01ne-tuning, it also included an ablation study demonstrat-\ning that the performance of four heuristic zero-shot transfer\nmethods improved steadily over the course of pre-training,\nwithout any supervised adaption. This analysis served as the\nbasis for GPT-2 (Radford et al., 2019) which focused exclu-\nsively on studying the task-learning capabilities of language\nmodels via zero-shot transfer.\n3.1.2. U SING CLIP FOR ZERO-SHOT TRANSFER\nCLIP is pre-trained to predict if an image and a text snippet\nare paired together in its dataset. To perform zero-shot clas-\nsi\ufb01cation, we reuse this capability. For each dataset, we use\nthe names of all the classes in the dataset as the set of poten-\ntial text pairings and predict the most probable (image, text)\npair according to CLIP. In a bit more detail, we \ufb01rst compute\nthe feature embedding of the image and the feature embed-\nding of the set of possible texts by their respective encoders.\nThe cosine similarity of these embeddings is then calculated,\nscaled by a temperature parameter \u001c, and normalized into a\nprobability distribution via a softmax. Note that this predic-\ntion layer is a multinomial logistic regression classi\ufb01er with\nL2-normalized inputs, L2-normalized weights, no bias, and\ntemperature scaling. When interpreted this way, the image\nencoder is the computer vision backbone which computes a\nfeature representation for the image and the text encoder is a\nhypernetwork (Ha et al., 2016) which generates the weights\nof a linear classi\ufb01er based on the text specifying the visual\nconcepts that the classes represent. Lei Ba et al. (2015) \ufb01rst\nintroduced a zero-shot image classi\ufb01er of this form while\nthe idea of generating a classi\ufb01er from natural language\ndates back to at least Elhoseiny et al. (2013). Continuing\nwith this interpretation, every step of CLIP pre-training can\nbe viewed as optimizing the performance of a randomly\ncreated proxy to a computer vision dataset which contains 1\nexample per class and has 32,768 total classes de\ufb01ned via\nnatural language descriptions. For zero-shot evaluation, we\ncache the zero-shot classi\ufb01er once it has been computed by\nthe text encoder and reuse it for all subsequent predictions.\nThis allows the cost of generating it to be amortized across\nall the predictions in a dataset.\n3.1.3. I NITIAL COMPARISON TO VISUAL N-G RAMS\nIn Table 1 we compare Visual N-Grams to CLIP. The best\nCLIP model improves accuracy on ImageNet from a proof\nof concept 11.5% to 76.2% and matches the performance\nof the original ResNet-50 despite using none of the 1.28\nmillion crowd-labeled training examples available for this\ndataset. Additionally, the top-5 accuracy of CLIP models\nare noticeably higher than their top-1, and this model has a\n95% top-5 accuracy, matching Inception-V4 (Szegedy et al.,\n2016). The ability to match the performance of a strong,\nfully supervised baselines in a zero-shot setting suggests\nLearning Transferable Visual Models From Natural Language Supervision 7\naYahoo ImageNet SUN\nVisual N-Grams 72.4 11.5 23.0\nCLIP 98.4 76.2 58.5\nTable 1. Comparing CLIP to prior zero-shot transfer image classi-\n\ufb01cation results. CLIP improves performance on all three datasets\nby a large amount. This improvement re\ufb02ects many differences\nin the 4 years since the development of Visual N-Grams (Li et al.,\n2017).\nCLIP is a signi\ufb01cant step towards \ufb02exible and practical\nzero-shot computer vision classi\ufb01ers. As mentioned above,\nthe comparison to Visual N-Grams is meant for contextu-\nalizing the performance of CLIP and should not be inter-\npreted as a direct methods comparison between CLIP and\nVisual N-Grams as many performance relevant differences\nbetween the two systems were not controlled for. For in-\nstance, we train on a dataset that is 10x larger, use a vision\nmodel that requires nearly 100x more compute per predic-\ntion, likely used over 1000x their training compute, and\nuse a transformer-based model which did not exist when\nVisual N-Grams was published. As a closer comparison, we\ntrained a CLIP ResNet-50 on the same YFCC100M dataset\nthat Visual N-Grams was trained on and found it matched\ntheir reported ImageNet performance within a V100 GPU\nday. This baseline was also trained from scratch instead of\nbeing initialized from pre-trained ImageNet weights as in\nVisual N-Grams.\nCLIP also outperforms Visual N-Grams on the other 2 re-\nported datasets. On aYahoo, CLIP achieves a 95% reduction\nin the number of errors, and on SUN, CLIP more than dou-\nbles the accuracy of Visual N-Grams. To conduct a more\ncomprehensive analysis and stress test, we implement a\nmuch larger evaluation suite detailed in Appendix A. In\ntotal we expand from the 3 datasets reported in Visual N-\nGrams to include over 30 datasets and compare to over 50\nexisting computer vision systems to contextualize results.\n3.1.4. P ROMPT ENGINEERING AND ENSEMBLING\nMost standard image classi\ufb01cation datasets treat the infor-\nmation naming or describing classes which enables natural\nlanguage based zero-shot transfer as an afterthought. The\nvast majority of datasets annotate images with just a numeric\nid of the label and contain a \ufb01le mapping these ids back to\ntheir names in English. Some datasets, such as Flowers102\nand GTSRB, don\u2019t appear to include this mapping at all\nin their released versions preventing zero-shot transfer en-\ntirely.2For many datasets, we observed these labels may be\n2Alec learned much more about \ufb02ower species and German\ntraf\ufb01c signs over the course of this project than he originally antic-\nipated.\n6.1 9.9 21.5 75.3 265.9\nModel GFLOPs455055606570Average Score (%)\n4X efficiency gain5 point\nimprovement\nRN50RN101RN50x4RN50x16RN50x64\nPrompt engineering and ensembling\nContextless class names (Li et al. 2017)Figure 4. Prompt engineering and ensembling improve zero-\nshot performance. Compared to the baseline of using contextless\nclass names, prompt engineering and ensembling boost zero-shot\nclassi\ufb01cation performance by almost 5 points on average across\n36 datasets. This improvement is similar to the gain from using\n4 times more compute with the baseline zero-shot method but is\n\u201cfree\u201d when amortized over many predictions.\nchosen somewhat haphazardly and do not anticipate issues\nrelated to zero-shot transfer which relies on task description\nin order to transfer successfully.\nA common issue is polysemy. When the name of a class\nis the only information provided to CLIP\u2019s text encoder it\nis unable to differentiate which word sense is meant due to\nthe lack of context. In some cases multiple meanings of the\nsame word might be included as different classes in the same\ndataset! This happens in ImageNet which contains both\nconstruction cranes and cranes that \ufb02y. Another example is\nfound in classes of the Oxford-IIIT Pet dataset where the\nword boxer is, from context, clearly referring to a breed of\ndog, but to a text encoder lacking context could just as likely\nrefer to a type of athlete.\nAnother issue we encountered is that it\u2019s relatively rare in\nour pre-training dataset for the text paired with the image\nto be just a single word. Usually the text is a full sentence\ndescribing the image in some way. To help bridge this\ndistribution gap, we found that using the prompt template\n\u201cA photo of a flabelg.\u201d to be a good default that\nhelps specify the text is about the content of the image. This\noften improves performance over the baseline of using only\nthe label text. For instance, just using this prompt improves\naccuracy on ImageNet by 1.3%.\nLearning Transferable Visual Models From Natural Language Supervision 8\nSimilar to the \u201cprompt engineering\u201d discussion around GPT-\n3 (Brown et al., 2020; Gao et al., 2020), we have also\nobserved that zero-shot performance can be signi\ufb01cantly\nimproved by customizing the prompt text to each task. A\nfew, non exhaustive, examples follow. We found on several\n\ufb01ne-grained image classi\ufb01cation datasets that it helped to\nspecify the category. For example on Oxford-IIIT Pets, us-\ning \u201cA photo of a flabelg, a type of pet. \u201d\nto help provide context worked well. Likewise, on Food101\nspecifying a type of food and on FGVC Aircraft a type of\naircraft helped too. For OCR datasets, we found that putting\nquotes around the text or number to be recognized improved\nperformance. Finally, we found that on satellite image classi-\n\ufb01cation datasets it helped to specify that the images were of\nthis form and we use variants of \u201c a satellite photo\nof aflabelg.\u201d.\nWe also experimented with ensembling over multiple zero-\nshot classi\ufb01ers as another way of improving performance.\nThese classi\ufb01ers are computed by using different context\nprompts such as \u2018 A photo of a big flabelg\u201d and\n\u201cA photo of a small flabelg\u201d. We construct the\nensemble over the embedding space instead of probability\nspace. This allows us to cache a single set of averaged text\nembeddings so that the compute cost of the ensemble is the\nsame as using a single classi\ufb01er when amortized over many\npredictions. We\u2019ve observed ensembling across many gen-\nerated zero-shot classi\ufb01ers to reliably improve performance\nand use it for the majority of datasets. On ImageNet, we\nensemble 80 different context prompts and this improves\nperformance by an additional 3.5% over the single default\nprompt discussed above. When considered together, prompt\nengineering and ensembling improve ImageNet accuracy\nby almost 5%. In Figure 4 we visualize how prompt engi-\nneering and ensembling change the performance of a set of\nCLIP models compared to the contextless baseline approach\nof directly embedding the class name as done in Li et al.\n(2017).\n3.1.5. A NALYSIS OF ZERO-SHOT CLIP P ERFORMANCE\nSince task-agnostic zero-shot classi\ufb01ers for computer vision\nhave been understudied, CLIP provides a promising oppor-\ntunity to gain a better understanding of this type of model.\nIn this section, we conduct a study of various properties of\nCLIP\u2019s zero-shot classi\ufb01ers. As a \ufb01rst question, we look\nsimply at how well zero-shot classi\ufb01ers perform. To con-\ntextualize this, we compare to the performance of a simple\noff-the-shelf baseline: \ufb01tting a fully supervised, regularized,\nlogistic regression classi\ufb01er on the features of the canonical\nResNet-50. In Figure 5 we show this comparison across 27\ndatasets. Please see Appendix A for details of datasets and\nsetup.\nZero-shot CLIP outperforms this baseline slightly more of-\n40\n 30\n 20\n 10\n 0 10 20 30 40\n Score (%)\nZero-Shot CLIP vs. Linear Probe on ResNet50EuroSAT -37.1KITTI Distance -34.0PatchCamelyon -19.5GTSRB -18.4CLEVRCounts -18.2DTD -16.6Flowers102 -12.5RESISC45 -11.9FGVCAircraft -11.3MNIST -10.0Birdsnap -3.2+0.5 PascalVOC2007+1.1 OxfordPets+1.9 ImageNet+2.0 Caltech101+2.8 FER2013+3.0 STL10+3.0 CIFAR100+3.9 CIFAR10+6.7 HatefulMemes+7.7 UCF101+7.8 SUN397+12.4 SST2+14.5 Kinetics700+22.5 Food101+23.2 Country211+28.9 StanfordCarsFigure 5. Zero-shot CLIP is competitive with a fully super-\nvised baseline. Across a 27 dataset eval suite, a zero-shot CLIP\nclassi\ufb01er outperforms a fully supervised linear classi\ufb01er \ufb01tted on\nResNet-50 features on 16 datasets, including ImageNet.\nten than not and wins on 16 of the 27 datasets. Looking at\nindividual datasets reveals some interesting behavior. On\n\ufb01ne-grained classi\ufb01cation tasks, we observe a wide spread\nin performance. On two of these datasets, Stanford Cars and\nFood101, zero-shot CLIP outperforms logistic regression\non ResNet-50 features by over 20% while on two others,\nFlowers102 and FGVCAircraft, zero-shot CLIP underper-\nforms by over 10%. On OxfordPets and Birdsnap, per-\nformance is much closer. We suspect these difference are\nprimarily due to varying amounts of per-task supervision\nbetween WIT and ImageNet. On \u201cgeneral\u201d object classi\ufb01ca-\ntion datasets such as ImageNet, CIFAR10/100, STL10, and\nPascalVOC2007 performance is relatively similar with a\nslight advantage for zero-shot CLIP in all cases. On STL10,\nCLIP achieves 99.3% overall which appears to be a new\nstate of the art despite not using any training examples. Zero-\nshot CLIP signi\ufb01cantly outperforms a ResNet-50 on two\ndatasets measuring action recognition in videos. On Kinet-\nics700, CLIP outperforms a ResNet-50 by 14.5%. Zero-\nshot CLIP also outperforms a ResNet-50\u2019s features by 7.7%\non UCF101. We speculate this is due to natural language\nproviding wider supervision for visual concepts involving\nverbs, compared to the noun-centric object supervision in\nImageNet.\nLooking at where zero-shot CLIP notably underperforms,\nLearning Transferable Visual Models From Natural Language Supervision 9\n012 4 8 16\n# of labeled training examples per class30354045505560657075Average Score (%)Zero-Shot\nCLIP\nBiT-M (ImageNet-21K)Linear Probe CLIP\nSimCLRv2\nResNet50\nFigure 6. Zero-shot CLIP outperforms few-shot linear probes.\nZero-shot CLIP matches the average performance of a 4-shot linear\nclassi\ufb01er trained on the same feature space and nearly matches the\nbest results of a 16-shot linear classi\ufb01er across publicly available\nmodels. For both BiT-M and SimCLRv2, the best performing\nmodel is highlighted. Light gray lines are other models in the eval\nsuite. The 20 datasets with at least 16 examples per class were\nused in this analysis.\nwe see that zero-shot CLIP is quite weak on several spe-\ncialized, complex, or abstract tasks such as satellite image\nclassi\ufb01cation (EuroSAT and RESISC45), lymph node tumor\ndetection (PatchCamelyon), counting objects in synthetic\nscenes (CLEVRCounts), self-driving related tasks such as\nGerman traf\ufb01c sign recognition (GTSRB), recognizing dis-\ntance to the nearest car (KITTI Distance). These results\nhighlight the poor capability of zero-shot CLIP on more\ncomplex tasks. By contrast, non-expert humans can robustly\nperform several of these tasks, such as counting, satellite\nimage classi\ufb01cation, and traf\ufb01c sign recognition, suggesting\nsigni\ufb01cant room for improvement. However, we caution\nthat it is unclear whether measuring zero-shot transfer, as\nopposed to few-shot transfer, is a meaningful evaluation for\ndif\ufb01cult tasks that a learner has no prior experience with,\nsuch as lymph node tumor classi\ufb01cation for almost all hu-\nmans (and possibly CLIP).\nWhile comparing zero-shot performance to fully supervised\nmodels contextualizes the task-learning capabilities of CLIP,\ncomparing to few-shot methods is a more direct compari-\nson, since zero-shot is its limit. In Figure 6, we visualize\nhow zero-shot CLIP compares to few-shot logistic regres-\nsion on the features of many image models including the\nbest publicly available ImageNet models, self-supervised\nlearning methods, and CLIP itself. While it is intuitive toexpect zero-shot to underperform one-shot, we instead \ufb01nd\nthat zero-shot CLIP matches the performance of 4-shot lo-\ngistic regression on the same feature space. This is likely\ndue to an important difference between the zero-shot and\nfew-shot approach. First, CLIP\u2019s zero-shot classi\ufb01er is gen-\nerated via natural language which allows for visual concepts\nto be directly speci\ufb01ed (\u201ccommunicated\u201d). By contrast,\n\u201cnormal\u201d supervised learning must infer concepts indirectly\nfrom training examples. Context-less example-based learn-\ning has the drawback that many different hypotheses can\nbe consistent with the data, especially in the one-shot case.\nA single image often contains many different visual con-\ncepts. Although a capable learner is able to exploit visual\ncues and heuristics, such as assuming that the concept being\ndemonstrated is the primary object in an image, there is no\nguarantee.\nA potential resolution of this discrepancy between zero-\nshot and few-shot performance is to use CLIP\u2019s zero-shot\nclassi\ufb01er as a prior for the weights of the few-shot classi\ufb01er.\nWhile adding an L2 penalty towards the generated weights\nis a straightforward implementation of this idea, we found\nthat hyperparameter optimization would often select for\nsuch a large value of this regularizer that the resulting few-\nshot classi\ufb01er was \u201cjust\u201d the zero-shot classi\ufb01er. Research\ninto better methods of combining the strength of zero-shot\ntransfer with \ufb02exibility of few-shot learning is a promising\ndirection for future work.\nWhen comparing zero-shot CLIP to few-shot logistic re-\ngression on the features of other models, zero-shot CLIP\nroughly matches the performance of the best performing\n16-shot classi\ufb01er in our evaluation suite, which uses the fea-\ntures of a BiT-M ResNet-152x2 trained on ImageNet-21K.\nWe are certain that a BiT-L model trained on JFT-300M\nwould perform even better but these models have not been\npublicly released. That a BiT-M ResNet-152x2 performs\nbest in a 16-shot setting is somewhat surprising since, as\nanalyzed in Section 3.2, the Noisy Student Ef\ufb01cientNet-L2\noutperforms it in a fully supervised setting by almost 5% on\naverage across 27 datasets.\nIn addition to studying the average performance of zero-shot\nCLIP and few-shot logistic regression, we also examine\nperformance on individual datasets. In Figure 7, we show\nestimates for the number of labeled examples per class that\na logistic regression classi\ufb01er on the same feature space\nrequires to match the performance of zero-shot CLIP. Since\nzero-shot CLIP is also a linear classi\ufb01er, this estimates the\neffective data ef\ufb01ciency of zero-shot transfer in this setting.\nIn order to avoid training thousands of linear classi\ufb01ers,\nwe estimate the effective data ef\ufb01ciency based on a log-\nlinear interpolation of the performance of a 1, 2, 4, 8, 16-\nshot (when possible), and a fully supervised linear classi\ufb01er\ntrained on each dataset. We \ufb01nd that zero-shot transfer can\nLearning Transferable Visual Models From Natural Language Supervision 10\n0 25 50 75 100 125 150 175 200\n# of labeled examples per class\nrequired to match zero-shotFlowers102EuroSATRESISC45CLEVRCountsGTSRBFGVCAircraftDTDBirdsnapUCF101KITTI DistanceCaltech101SUN397MNISTStanfordCarsHatefulMemesCIFAR100STL10Kinetics700SST2PCamImageNetCountry211OxfordPetsFood101CIFAR10FER2013\n0.90.91.51.51.62.02.62.72.92.93.53.94.86.09.812.012.713.614.414.716.032486481184\nMedian: 5.4Mean:  20.8\nFigure 7. The data ef\ufb01ciency of zero-shot transfer varies\nwidely. Calculating the number of labeled examples per class\na linear classi\ufb01er on the same CLIP feature space requires to match\nthe performance of the zero-shot classi\ufb01er contextualizes the ef-\nfectiveness of zero-shot transfer. Values are estimated based on\nlog-linear interpolation of 1, 2, 4, 8, 16-shot and fully supervised\nresults. Performance varies widely from still underperforming a\none-shot classi\ufb01er on two datasets to matching an estimated 184\nlabeled examples per class.\nhave widely varying ef\ufb01ciency per dataset from less than 1\nlabeled example per class to 184. Two datasets, Flowers102\nand EuroSAT underperform one-shot models. Half of the\ndatasets require less than 5 examples per class with a median\nof 5.4. However, the mean estimated data ef\ufb01ciency is 20.8\nexamples per class. This is due to the 20% of datasets\nwhere supervised classi\ufb01ers require many labeled examples\nper class in order to match performance. On ImageNet,\nzero-shot CLIP matches the performance of a 16-shot linear\nclassi\ufb01er trained on the same feature space.\nIf we assume that evaluation datasets are large enough that\nthe parameters of linear classi\ufb01ers trained on them are well\nestimated, then, because CLIP\u2019s zero-shot classi\ufb01er is also\na linear classi\ufb01er, the performance of the fully supervised\nclassi\ufb01ers roughly sets an upper bound for what zero-shot\ntransfer can achieve. In Figure 8 we compare CLIP\u2019s zero-\nshot performance with fully supervised linear classi\ufb01ers\nacross datasets. The dashed, y=xline represents an \u201cop-\ntimal\u201d zero-shot classi\ufb01er that matches the performance of\nits fully supervised equivalent. For most datasets, the per-\nformance of zero-shot classi\ufb01ers still underperform fully su-\npervised classi\ufb01ers by 10% to 25%, suggesting that there is\nstill plenty of headroom for improving CLIP\u2019s task-learning\nand zero-shot transfer capabilities.\nThere is a positive correlation of 0.82 (p-value <10\u00006)\nbetween zero-shot performance and fully supervised perfor-\n20 30 40 50 60 70 80 90 100\nLinear Probe CLIP Performance2030405060708090100Zero-Shot CLIP Performance\nr = 0.82\nVOC2007\nCountry211HatefulMemesMNISTCIFAR10\nSST2\nDTDPCAMRESISC45\nEuroSAT\nGTSRB\nCLEVRCountsFER2013UCF101\nBirdsnapOxfordPets\nCIFAR100\nFGVCAircraftFood101\nFlowers102Stanford CarsCaltech101\nSUN397ImageNetSTL10\nKITTI DistanceKinetics700Figure 8. Zero-shot performance is correlated with linear\nprobe performance but still mostly sub-optimal. Comparing\nzero-shot and linear probe performance across datasets shows a\nstrong correlation with zero-shot performance mostly shifted 10 to\n25 points lower. On only 5 datasets does zero-shot performance\napproach linear probe performance ( \u00143 point difference).\nmance, suggesting that CLIP is relatively consistent at con-\nnecting underlying representation and task learning to zero-\nshot transfer. However, zero-shot CLIP only approaches\nfully supervised performance on 5 datasets: STL10, CI-\nFAR10, Food101, OxfordPets, and Caltech101. On all 5\ndatasets, both zero-shot accuracy and fully supervised accu-\nracy are over 90%. This suggests that CLIP may be more\neffective at zero-shot transfer for tasks where its underly-\ning representations are also high quality. The slope of a\nlinear regression model predicting zero-shot performance\nas a function of fully supervised performance estimates that\nfor every 1% improvement in fully supervised performance,\nzero-shot performance improves by 1.28%. However, the\n95th-percentile con\ufb01dence intervals still include values of\nless than 1 (0.93-1.79).\nOver the past few years, empirical studies of deep learning\nsystems have documented that performance is predictable as\na function of important quantities such as training compute\nand dataset size (Hestness et al., 2017; Kaplan et al., 2020).\nThe GPT family of models has so far demonstrated consis-\ntent improvements in zero-shot performance across a 1000x\nincrease in training compute. In Figure 9, we check whether\nthe zero-shot performance of CLIP follows a similar scaling\npattern. We plot the average error rate of the 5 ResNet CLIP\nmodels across 39 evaluations on 36 different datasets and\n\ufb01nd that a similar log-log linear scaling trend holds for CLIP\nacross a 44x increase in model compute. While the overall\ntrend is smooth, we found that performance on individual\nevaluations can be much noisier. We are unsure whether\nLearning Transferable Visual Models From Natural Language Supervision 11\n6.1 9.9 21.5 75.3 265.9\nModel GFLOPs30354045Error (%)RN50\nRN101\nRN50x4\nRN50x16\nRN50x64\nFigure 9. Zero-shot CLIP performance scales smoothly as a\nfunction of model compute. Across 39 evals on 36 different\ndatasets, average zero-shot error is well modeled by a log-log lin-\near trend across a 44x range of compute spanning 5 different CLIP\nmodels. Lightly shaded lines are performance on individual evals,\nshowing that performance is much more varied despite the smooth\noverall trend.\nthis is caused by high variance between individual training\nruns on sub-tasks (as documented in D\u2019Amour et al. (2020))\nmasking a steadily improving trend or whether performance\nis actually non-monotonic as a function of compute on some\ntasks.\n3.2. Representation Learning\nWhile we have extensively analyzed the task-learning ca-\npabilities of CLIP through zero-shot transfer in the previ-\nous section, it is more common to study the representation\nlearning capabilities of a model. There exist many ways to\nevaluate the quality of representations as well as disagree-\nments over what properties an \u201cideal\u201d representation should\nhave (Locatello et al., 2020). Fitting a linear classi\ufb01er on\na representation extracted from the model and measuring\nits performance on various datasets is a common approach.\nAn alternative is measuring the performance of end-to-end\n\ufb01ne-tuning of the model. This increases \ufb02exibility, and\nprior work has convincingly demonstrated that \ufb01ne-tuning\noutperforms linear classi\ufb01cation on most image classi\ufb01-\ncation datasets (Kornblith et al., 2019; Zhai et al., 2019).\nWhile the high performance of \ufb01ne-tuning motivates its\nstudy for practical reasons, we still opt for linear classi\ufb01er\nbased evaluation for several reasons. Our work is focused\non developing a high-performing task and dataset-agnostic\npre-training approach. Fine-tuning, because it adapts rep-\nresentations to each dataset during the \ufb01ne-tuning phase,\ncan compensate for and potentially mask failures to learn\ngeneral and robust representations during the pre-training\nphase. Linear classi\ufb01ers, because of their limited \ufb02exibility,\ninstead highlight these failures and provide clear feedback\nduring development. For CLIP, training supervised linearclassi\ufb01ers has the added bene\ufb01t of being very similar to the\napproach used for its zero-shot classi\ufb01ers which enables\nextensive comparisons and analysis in Section 3.1. Finally,\nwe aim to compare CLIP to a comprehensive set of existing\nmodels across many tasks. Studying 66 different models on\n27 different datasets requires tuning 1782 different evalua-\ntions. Fine-tuning opens up a much larger design and hyper-\nparameter space, which makes it dif\ufb01cult to fairly evaluate\nand computationally expensive to compare a diverse set of\ntechniques as discussed in other large scale empirical studies\n(Lucic et al., 2018; Choi et al., 2019). By comparison, linear\nclassi\ufb01ers require minimal hyper-parameter tuning and have\nstandardized implementations and evaluation procedures.\nPlease see Appendix A for further details on evaluation.\nFigure 10 summarizes our \ufb01ndings. To minimize selection\neffects that could raise concerns of con\ufb01rmation or reporting\nbias, we \ufb01rst study performance on the 12 dataset evaluation\nsuite from Kornblith et al. (2019). While small CLIP mod-\nels such as a ResNet-50 and ResNet-101 outperform other\nResNets trained on ImageNet-1K (BiT-S and the originals),\nthey underperform ResNets trained on ImageNet-21K (BiT-\nM). These small CLIP models also underperform models\nin the Ef\ufb01cientNet family with similar compute require-\nments. However, models trained with CLIP scale very well\nand the largest model we trained (ResNet-50x64) slightly\noutperforms the best performing existing model (a Noisy\nStudent Ef\ufb01cientNet-L2) on both overall score and compute\nef\ufb01ciency. We also \ufb01nd that CLIP vision transformers are\nabout 3x more compute ef\ufb01cient than CLIP ResNets, which\nallows us to reach higher overall performance within our\ncompute budget. These results qualitatively replicate the\n\ufb01ndings of Dosovitskiy et al. (2020) which reported that\nvision transformers are more compute ef\ufb01cient than con-\nvnets when trained on suf\ufb01ciently large datasets. Our best\noverall model is a ViT-L/14 that is \ufb01ne-tuned at a higher res-\nolution of 336 pixels on our dataset for 1 additional epoch.\nThis model outperforms the best existing model across this\nevaluation suite by an average of 2.6%.\nAs Figure 21 qualitatively shows, CLIP models learn a wider\nset of tasks than has previously been demonstrated in a sin-\ngle computer vision model trained end-to-end from random\ninitialization. These tasks include geo-localization, optical\ncharacter recognition, facial emotion recognition, and action\nrecognition. None of these tasks are measured in the evalua-\ntion suite of Kornblith et al. (2019). This could be argued\nto be a form of selection bias in Kornblith et al. (2019)\u2019s\nstudy towards tasks that overlap with ImageNet. To address\nthis, we also measure performance on a broader 27 dataset\nevaluation suite. This evaluation suite, detailed in Appendix\nA includes datasets representing the aforementioned tasks,\nGerman Traf\ufb01c Signs Recognition Benchmark (Stallkamp\net al., 2011), as well as several other datasets adapted from\nVTAB (Zhai et al., 2019).\nLearning Transferable Visual Models From Natural Language Supervision 12\n100101102\nForward-pass GFLOPs/image75808590Average Score (%)\nLinear probe average over Kornblith et al.'s 12 datasets\n100101102\nForward-pass GFLOPs/image70758085Average Score (%)\nLinear probe average over all 27 datasets\nCLIP-ViT\nCLIP-ResNet\nEfficientNet-NoisyStudent\nEfficientNetInstagram-pretrained\nSimCLRv2\nBYOL\nMoCoViT (ImageNet-21k)\nBiT-M\nBiT-S\nResNet\nFigure 10. Linear probe performance of CLIP models in comparison with state-of-the-art computer vision models , including\nEf\ufb01cientNet (Tan & Le, 2019; Xie et al., 2020), MoCo (Chen et al., 2020d), Instagram-pretrained ResNeXt models (Mahajan et al., 2018;\nTouvron et al., 2019), BiT (Kolesnikov et al., 2019), ViT (Dosovitskiy et al., 2020), SimCLRv2 (Chen et al., 2020c), BYOL (Grill et al.,\n2020), and the original ResNet models (He et al., 2016b). (Left) Scores are averaged over 12 datasets studied by Kornblith et al. (2019).\n(Right) Scores are averaged over 27 datasets that contain a wider variety of distributions. Dotted lines indicate models \ufb01ne-tuned or\nevaluated on images at a higher-resolution than pre-training. See Table 10 for individual scores and Figure 20 for plots for each dataset.\nOn this broader evaluation suite, the bene\ufb01ts of CLIP are\nmore clear. All CLIP models, regardless of scale, outper-\nform all evaluated systems in terms of compute ef\ufb01ciency.\nThe improvement in average score of the best model over\nprevious systems increases from 2.6% to 5%. We also \ufb01nd\nthat self-supervised systems do noticeably better on our\nbroader evaluation suite. For instance, while SimCLRv2\nstill underperforms BiT-M on average on the 12 datasets\nof Kornblith et al. (2019), SimCLRv2 outperforms BiT-M\non our 27 dataset evaluation suite. These \ufb01ndings suggest\ncontinuing to expand task diversity and coverage in order\nto better understand the \u201cgeneral\u201d performance of systems.\nWe suspect additional evaluation efforts along the lines of\nVTAB to be valuable.\nIn addition to the aggregate analysis above, we visualize\nper-dataset differences in the performance of the best CLIP\nmodel and the best model in our evaluation suite across\nall 27 datasets in Figure 11. CLIP outperforms the Noisy\nStudent Ef\ufb01cientNet-L2 on 21 of the 27 datasets. CLIP\nimproves the most on tasks which require OCR (SST2and HatefulMemes), geo-localization and scene recognition\n(Country211, SUN397), and activity recognition in videos\n(Kinetics700 and UCF101). In addition CLIP also does\nmuch better on \ufb01ne-grained car and traf\ufb01c sign recognition\n(Stanford Cars and GTSRB). This may re\ufb02ect a problem\nwith overly narrow supervision in ImageNet. A result such\nas the 14.7% improvement on GTSRB could be indicative\nof an issue with ImageNet-1K, which has only a single la-\nbel for all traf\ufb01c and street signs. This could encourage\na supervised representation to collapse intra-class details\nand hurt accuracy on a \ufb01ne-grained downstream task. As\nmentioned, CLIP still underperforms the Ef\ufb01cientNet on\nseveral datasets. Unsurprisingly, the dataset that the Ef\ufb01-\ncientNet does best relative to CLIP on is the one it was\ntrained on: ImageNet. The EffcientNet also slightly outper-\nforms CLIP on low-resolution datasets such as CIFAR10\nand CIFAR100. We suspect this is at least partly due to the\nlack of scale-based data augmentation in CLIP. The Ef\ufb01-\ncientNet also does slightly better on PatchCamelyon and\nCLEVRCounts, datasets where overall performance is still\nLearning Transferable Visual Models From Natural Language Supervision 13\n10\n 5\n 0 5 10 15 20 25\n Score (%)\nLogistic Regression on CLIP vs. EfficientNet L2 NSImageNet -3.0CLEVRCounts -2.4CIFAR100 -1.7PatchCamelyon -1.2CIFAR10 -0.8OxfordPets -0.5+0.0 STL10+0.5 VOC2007+0.5 DTD+0.6 MNIST+0.9 EuroSAT+1.3 Caltech101+1.4 Flowers102+1.4 Birdsnap+2.3 KITTI Distance+3.1 UCF101+3.2 FGVCAircraft+3.9 Food101+4.5 FER2013+5.1 RESISC45+6.2 Kinetics700+6.5 SUN397+14.7 GTSRB+15.9 StanfordCars+18.8 HatefulMemes+22.7 Country211+23.6 SST2\nFigure 11. CLIP\u2019s features outperform the features of the best\nImageNet model on a wide variety of datasets. Fitting a linear\nclassi\ufb01er on CLIP\u2019s features outperforms using the Noisy Student\nEf\ufb01cientNet-L2 on 21 out of 27 datasets.\nlow for both approaches.\n3.3. Robustness to Natural Distribution Shift\nIn 2015, it was announced that a deep learning model ex-\nceeded human performance on the ImageNet test set (He\net al., 2015). However, research in the subsequent years\nhas repeatedly found that these models still make many sim-\nple mistakes (Dodge & Karam, 2017; Geirhos et al., 2018;\nAlcorn et al., 2019), and new benchmarks testing these sys-\ntems has often found their performance to be much lower\nthan both their ImageNet accuracy and human accuracy\n(Recht et al., 2019; Barbu et al., 2019). What explains this\ndiscrepancy? Various ideas have been suggested and stud-\nied (Ilyas et al., 2019; Geirhos et al., 2020). A common\ntheme of proposed explanations is that deep learning models\nare exceedingly adept at \ufb01nding correlations and patterns\nwhich hold across their training dataset and thus improve\nin-distribution performance. However many of these corre-\nlations and patterns are actually spurious and do not hold for\nother distributions and result in large drops in performance\non other datasets.\nWe caution that, to date, most of these studies limit their\nevaluation to models trained on ImageNet. Recalling the\ntopic of discussion, it may be a mistake to generalize too\nfar from these initial \ufb01ndings. To what degree are these\nfailures attributable to deep learning, ImageNet, or somecombination of the two? CLIP models, which are trained via\nnatural language supervision on a very large dataset and are\ncapable of high zero-shot performance, are an opportunity\nto investigate this question from a different angle.\nTaori et al. (2020) is a recent comprehensive study mov-\ning towards quantifying and understanding these behaviors\nfor ImageNet models. Taori et al. (2020) study how the\nperformance of ImageNet models change when evaluated\nonnatural distribution shifts . They measure performance\non a set of 7 distribution shifts: ImageNetV2 (Recht et al.,\n2019), ImageNet Sketch (Wang et al., 2019), Youtube-BB\nand ImageNet-Vid (Shankar et al., 2019), ObjectNet (Barbu\net al., 2019), ImageNet Adversarial (Hendrycks et al., 2019),\nand ImageNet Rendition (Hendrycks et al., 2020a). They\ndistinguish these datasets, which all consist of novel images\ncollected from a variety of sources, from synthetic distri-\nbution shifts such as ImageNet-C (Hendrycks & Dietterich,\n2019), Stylized ImageNet (Geirhos et al., 2018), or adver-\nsarial attacks (Goodfellow et al., 2014) which are created by\nperturbing existing images in various ways. They propose\nthis distinction because in part because they \ufb01nd that while\nseveral techniques have been demonstrated to improve per-\nformance on synthetic distribution shifts, they often fail to\nyield consistent improvements on natural distributions.3\nAcross these collected datasets, the accuracy of ImageNet\nmodels drop well below the expectation set by the Ima-\ngeNet validation set. For the following summary discussion\nwe report average accuracy across all 7 natural distribution\nshift datasets and average accuracy across the correspond-\ning class subsets of ImageNet unless otherwise speci\ufb01ed.\nAdditionally, for Youtube-BB and ImageNet-Vid, which\nhave two different evaluation settings, we use the average\nof pm-0 and pm-10 accuracy.\nA ResNet-101 makes 5 times as many mistakes when eval-\nuated on these natural distribution shifts compared to the\nImageNet validation set. Encouragingly however, Taori et al.\n(2020) \ufb01nd that accuracy under distribution shift increases\npredictably with ImageNet accuracy and is well modeled\nas a linear function of logit-transformed accuracy. Taori\net al. (2020) use this \ufb01nding to propose that robustness\nanalysis should distinguish between effective andrelative\nrobustness. Effective robustness measures improvements\nin accuracy under distribution shift above what is predicted\nby the documented relationship between in-distribution and\nout-of-distribution accuracy. Relative robustness captures\nany improvement in out-of-distribution accuracy. Taori et al.\n(2020) argue that robustness techniques should aim to im-\nprove both effective robustness and relative robustness.\nAlmost all models studied in Taori et al. (2020) are trained\n3We refer readers to Hendrycks et al. (2020a) for additional\nexperiments and discussion on this claim.\nLearning Transferable Visual Models From Natural Language Supervision 14\n65 70 75 80 85 90\nImageNet Score (%)657075808590Transfer Score (%)\nLinear probe average over Kornblith et al.'s 12 datasets\n65 70 75 80 85 90\nImageNet Score (%)657075808590Transfer Score (%)\nLinear probe average over 26 datasets\nCLIP-ViT\nCLIP-ResNet\nEfficientNet-NoisyStudent\nEfficientNetInstagram\nSimCLRv2\nBYOL\nMoCoViT (ImageNet-21k)\nBiT-M\nBiT-S\nResNet\nFigure 12. CLIP\u2019s features are more robust to task shift when compared to models pre-trained on ImageNet. For both dataset\nsplits, the transfer scores of linear probes trained on the representations of CLIP models are higher than other models with similar\nImageNet performance. This suggests that the representations of models trained on ImageNet are somewhat over\ufb01t to their task.\nor \ufb01ne-tuned on the ImageNet dataset. Returning to the\ndiscussion in the introduction to this section - is training\nor adapting to the ImageNet dataset distribution the cause\nof the observed robustness gap? Intuitively, a zero-shot\nmodel should not be able to exploit spurious correlations\nor patterns that hold only on a speci\ufb01c distribution, since it\nis not trained on that distribution.4Thus it is reasonable\nto expect zero-shot models to have much higher effective\nrobustness. In Figure 13, we compare the performance of\nzero-shot CLIP with existing ImageNet models on natural\ndistribution shifts. All zero-shot CLIP models improve\neffective robustness by a large amount and reduce the size\nof the gap between ImageNet accuracy and accuracy under\ndistribution shift by up to 75%.\nWhile these results show that zero-shot models can be much\nmore robust, they do not necessarily mean that supervised\nlearning on ImageNet causes a robustness gap. Other details\nof CLIP, such as its large and diverse pre-training dataset\nor use of natural language supervision could also result\n4We caution that a zero-shot model can still exploit spurious\ncorrelations that are shared between the pre-training and evaluation\ndistributions.in much more robust models regardless of whether they\nare zero-shot or \ufb01ne-tuned. As an initial experiment to\npotentially begin narrowing this down, we also measure\nhow the performance of CLIP models change after adapting\nto the ImageNet distribution via a L2 regularized logistic\nregression classi\ufb01er \ufb01t to CLIP features on the ImageNet\ntraining set. We visualize how performance changes from\nthe zero-shot classi\ufb01er in Figure 14. Although adapting\nCLIP to the ImageNet distribution increases its ImageNet\naccuracy by 9.2% to 85.4% overall, and ties the accuracy\nof the 2018 SOTA from Mahajan et al. (2018), average\naccuracy under distribution shift slightly decreases .\nIt is surprising to see a 9.2% increase in accuracy, which cor-\nresponds to roughly 3 years of improvement in SOTA, fail\nto translate into any improvement in average performance\nunder distribution shift. We also break down the differences\nbetween zero-shot accuracy and linear classi\ufb01er accuracy\nper dataset in Figure 14 and \ufb01nd performance still increases\nsigni\ufb01cantly on one dataset, ImageNetV2. ImageNetV2\nclosely followed the creation process of the original Ima-\ngeNet dataset which suggests that gains in accuracy from\nsupervised adaptation are closely concentrated around the\nImageNet distribution. Performance decreases by 4.7% on\nLearning Transferable Visual Models From Natural Language Supervision 15\n65 70 75 80 85 90 95 100\nAverage on class subsampled ImageNet (top-1, %)20253035404550556065707580859095100Average on 7 natural distribution shift datasets (top-1, %)Ideal robust model (y = x)\nZero-Shot CLIP\nStandard ImageNet training\nExisiting robustness techniques\nImageNet \nImageNetV2 \nImageNet-A ImageNet-R \nObjectNet \nImageNet \nSketch 76.2 76.2\n64.3 70.1\n2.7 77.137.7 88.9\n32.6 72.3\n25.2 60.2ImageNet \nResNet101 Zero-Shot\nCLIP \n0%\n+5.8% \n+74.4% +51.2% \n+39.7% \n+35.0% \n\u0394 Score \nDataset Examples \nFigure 13. Zero-shot CLIP is much more robust to distribution shift than standard ImageNet models. (Left) An ideal robust model\n(dashed line) performs equally well on the ImageNet distribution and on other natural image distributions. Zero-shot CLIP models shrink\nthis \u201crobustness gap\u201d by up to 75%. Linear \ufb01ts on logit transformed values are shown with bootstrap estimated 95% con\ufb01dence intervals.\n(Right) Visualizing distribution shift for bananas, a class shared across 5 of the 7 natural distribution shift datasets. The performance of\nthe best zero-shot CLIP model, ViT-L/14@336px, is compared with a model that has the same performance on the ImageNet validation\nset, ResNet-101.\nImageNet-R, 3.8% on ObjectNet, 2.8% on ImageNet Sketch,\nand 1.9% on ImageNet-A. The change in accuracy on the\ntwo other datasets, Youtube-BB and ImageNet Vid, is in-\nsigni\ufb01cant.\nHow is it possible to improve accuracy by 9.2% on the Im-\nageNet dataset with little to no increase in accuracy under\ndistribution shift? Is the gain primarily from \u201cexploiting\nspurious correlations\u201d? Is this behavior unique to some com-\nbination of CLIP, the ImageNet datatset, and the distribution\nshifts studied, or a more general phenomena? Does it hold\nfor end-to-end \ufb01netuning as well as linear classi\ufb01ers? We\ndo not have con\ufb01dent answers to these questions at this time.\nPrior work has also pre-trained models on distributions other\nthan ImageNet, but it is common to study and release mod-\nels only after they have been \ufb01ne-tuned to ImageNet. As a\nstep towards understanding whether pre-trained zero-shot\nmodels consistently have higher effective robustness than\n\ufb01ne-tuned models, we encourage the authors of Mahajan\net al. (2018), Kolesnikov et al. (2019), and Dosovitskiy et al.\n(2020) to, if possible, study these questions on their models\nas well.\nWe also investigate another robustness intervention enabled\nby \ufb02exible zero-shot natural-language-based image classi-\n\ufb01ers. The target classes across the 7 transfer datasets are\nnot always perfectly aligned with those of ImageNet. Two\ndatasets, Youtube-BB and ImageNet-Vid, consist of super-\nclasses of ImageNet. This presents a problem when trying\nto use the \ufb01xed 1000-way classi\ufb01er of an ImageNet model\nto make predictions. Taori et al. (2020) handle this by max-pooling predictions across all sub-classes according to the\nImageNet class hierarchy. Sometimes this mapping is much\nless than perfect. For the person class in Youtube-BB, pre-\ndictions are made by pooling over the ImageNet classes for\na baseball player, a bridegroom, and a scuba diver. With\nCLIP we can instead generate a custom zero-shot classi-\n\ufb01er for each dataset directly based on its class names. In\nFigure 14 we see that this improves average effective ro-\nbustness by 5% but is concentrated in large improvements\non only a few datasets. Curiously, accuracy on ObjectNet\nalso increases by 2.3%. Although the dataset was designed\nto closely overlap with ImageNet classes, using the names\nprovided for each class by ObjectNet\u2019s creators still helps a\nsmall amount compared to using ImageNet class names and\npooling predictions when necessary.\nWhile zero-shot CLIP improves effective robustness, Figure\n14 shows that the bene\ufb01t is almost entirely gone in a fully\nsupervised setting. To better understand this difference, we\ninvestigate how effective robustness changes on the contin-\nuum from zero-shot to fully supervised. In Figure 15 we\nvisualize the performance of 0-shot, 1-shot, 2-shot, 4-shot\n..., 128-shot, and fully supervised logistic regression classi-\n\ufb01ers on the best CLIP model\u2019s features. We see that while\nfew-shot models also show higher effective robustness than\nexisting models, this bene\ufb01t fades as in-distribution per-\nformance increases with more training data and is mostly,\nthough not entirely, gone for the fully supervised model.\nAdditionally, zero-shot CLIP is notably more robust than\na few-shot model with equivalent ImageNet performance.\nLearning Transferable Visual Models From Natural Language Supervision 16\n70 75 80 85 90 95\nAverage on class subsampled ImageNet (top-1, %)253035404550556065707580Average on 7 natural distribution shift datasets (top-1, %)Adapt to class shift\nAdapt to ImageNet\nIdeal robust model (y = x)\nAdaptive Zero-Shot CLIP\nImageNet Zero-Shot CLIP\nLogistic Regression CLIP\nStandard ImageNet training\nRobustness intervention\nTrained with more data\n10\n 5\n 0 5 10 15 20 25 30\nChange from zero-shot ImageNet classifier accuracy (%)ImageNet-R -4.7ObjectNet -3.8ImageNet Sketch -2.8ImageNet-A -1.9ImageNet Vid -0.5+0.6 Youtube-BB+5.8 ImageNetV2+9.2 ImageNetAdapt to ImageNet\n10\n 5\n 0 5 10 15 20 25 30\nChange from zero-shot ImageNet classifier accuracy (%)0 ImageNet0 ImageNetV20 ImageNet-A0 ImageNet-R0 ImageNet Sketch+2.3 ObjectNet+8.3 ImageNet Vid+26.9 Youtube-BBAdapt to class shift\nFigure 14. While supervised adaptation to ImageNet increases ImageNet accuracy by 9.2%, it slightly reduces average robustness.\n(Left) Customizing zero-shot CLIP to each dataset improves robustness compared to using a single static zero-shot ImageNet classi\ufb01er\nand pooling predictions across similar classes as in Taori et al. (2020). CLIP models adapted to ImageNet have similar effective robustness\nas the best prior ImageNet models. (Right) Details of per dataset changes in accuracy for the two robustness interventions. Adapting to\nImageNet increases accuracy on ImageNetV2 noticeably but trades off accuracy on several other distributions. Dataset speci\ufb01c zero-shot\nclassi\ufb01ers can improve accuracy by a large amount but are limited to only a few datasets that include classes which don\u2019t perfectly align\nwith ImageNet categories.\nAcross our experiments, high effective robustness seems to\nresult from minimizing the amount of distribution speci\ufb01c\ntraining data a model has access to, but this comes at a cost\nof reducing dataset-speci\ufb01c performance.\nTaken together, these results suggest that the recent shift\ntowards large-scale task and dataset agnostic pre-training\ncombined with a reorientation towards zero-shot and few-\nshot benchmarking on broad evaluation suites (as advocated\nby Yogatama et al. (2019) and Linzen (2020)) promotes the\ndevelopment of more robust systems and provides a more\naccurate assessment of performance. We are curious to see\nif the same results hold for zero-shot models in the \ufb01eld\nof NLP such as the GPT family. While Hendrycks et al.\n(2020b) has reported that pre-training improves relative ro-\nbustness on sentiment analysis, Miller et al. (2020)\u2019s study\nof the robustness of question answering models under nat-\nural distribution shift \ufb01nds, similar to Taori et al. (2020),\nlittle evidence of effective robustness improvements to date.\n4. Comparison to Human Performance\nHow does CLIP compare to human performance and human\nlearning? To get a better understanding of how well humans\nperform in similar evaluation settings to CLIP, we evaluatedhumans on one of our tasks. We wanted to get a sense of\nhow strong human zero-shot performance is at these tasks,\nand how much human performance is improved if they are\nshown one or two image samples. This can help us to\ncompare task dif\ufb01culty for humans and CLIP, and identify\ncorrelations and differences between them.\nWe had \ufb01ve different humans look at each of 3669 images\nin the test split of the Oxford IIT Pets dataset (Parkhi et al.,\n2012) and select which of the 37 cat or dog breeds best\nmatched the image (or \u2018I don\u2019t know\u2019 if they were com-\npletely uncertain). In the zero-shot case the humans were\ngiven no examples of the breeds and asked to label them\nto the best of their ability without an internet search. In\nthe one-shot experiment the humans were given one sample\nimage of each breed and in the two-shot experiment they\nwere given two sample images of each breed.5\nOne possible concern was that the human workers were not\nsuf\ufb01ciently motivated in the zero-shot task. High human\naccuracy of 94% on the STL-10 dataset (Coates et al., 2011)\n5There is not a perfect correspondence between the human\nfew-shot tasks and the model\u2019s few-shot performance since the\nmodel cannot refer to sample images in the way that the humans\ncan.\nLearning Transferable Visual Models From Natural Language Supervision 17\n65 70 75 80 85 90 95\nAverage on class subsampled ImageNet (top-1, %)202530354045505560657075Average on 7 natural distribution shift datasets (top-1, %)1 shot2 shot4 shot8 shot16 shot3264128all0 shot\nIdeal robust model (y = x)\nFew-Shot CLIP (best model)\nZero-Shot CLIP (best model)\nStandard ImageNet training\nRobustness intervention\nTrained with more data\nFigure 15. Few-shot CLIP also increases effective robustness\ncompared to existing ImageNet models but is less robust than\nzero-shot CLIP. Minimizing the amount of ImageNet training\ndata used for adaption increases effective robustness at the cost of\ndecreasing relative robustness. 16-shot logistic regression CLIP\nmatches zero-shot CLIP on ImageNet, as previously reported in\nFigure 7, but is less robust.\nand 97-100% accuracy on the subset of attention check\nimages increased our trust in the human workers.\nInterestingly, humans went from a performance average of\n54% to 76% with just one training example per class, and\nthe marginal gain from an additional training example is\nminimal. The gain in accuracy going from zero to one shot\nis almost entirely on images that humans were uncertain\nabout. This suggests that humans \u201cknow what they don\u2019t\nknow\u201d and are able to update their priors on the images they\nare most uncertain in based on a single example. Given this,\nit seems that while CLIP is a promising training strategy\nfor zero-shot performance (Figure 5) and does well on tests\nof natural distribution shift (Figure 13), there is a large\ndifference between how humans learn from a few examples\nand the few-shot methods in this paper.\nThis suggests that there are still algorithmic improvements\nwaiting to be made to decrease the gap between machine\nand human sample ef\ufb01ciency, as noted by Lake et al. (2016)\nand others. Because these few-shot evaluations of CLIP\ndon\u2019t make effective use of prior knowledge and the humans\ndo, we speculate that \ufb01nding a method to properly integrate\nprior knowledge into few-shot learning is an important step\nin algorithmic improvements to CLIP. To our knowledge,\nusing a linear classi\ufb01er on top of the features of a high-AccuracyMajority V ote\non Full DatasetAccuracy\non GuessesMajority V ote\nAccuracy\non Guesses\nZero-shot human 53.7 57.0 69.7 63.9\nZero-shot CLIP 93.5 93.5 93.5 93.5\nOne-shot human 75.7 80.3 78.5 81.2\nTwo-shot human 75.7 85.0 79.2 86.1\nTable 2. Comparison of human performance on Oxford IIT Pets.\nAs in Parkhi et al. (2012), the metric is average per-class classi\ufb01ca-\ntion accuracy. Most of the gain in performance when going from\nthe human zero shot case to the human one shot case is on images\nthat participants were highly uncertain on. \u201cGuesses\u201d refers to\nrestricting the dataset to where participants selected an answer\nother than \u201cI don\u2019t know\u201d, the \u201cmajority vote\u201d is taking the most\nfrequent (exclusive of ties) answer per image.\nquality pre-trained model is near state-of-the-art for few\nshot learning (Tian et al., 2020), which suggests that there is\na gap between the best few-shot machine learning methods\nand human few-shot learning.\nIf we plot human accuracy vs CLIP\u2019s zero shot accuracy\n(Figure 16), we see that the hardest problems for CLIP are\nalso hard for humans. To the extent that errors are consistent,\nour hypothesis is that this is due to at least a two factors:\nnoise in the dataset (including mislabeled images) and out of\ndistribution images being hard for both humans and models.\n5. Data Overlap Analysis\nA concern with pre-training on a very large internet dataset\nis unintentional overlap with downstream evals. This is\nimportant to investigate since, in a worst-case scenario, a\ncomplete copy of an evaluation dataset could leak into the\npre-training dataset and invalidate the evaluation as a mean-\ningful test of generalization. One option to prevent this is to\nidentify and remove all duplicates before training a model.\nWhile this guarantees reporting true hold-out performance,\nit requires knowing all possible data which a model might\nbe evaluated on ahead of time. This has the downside of\nlimiting the scope of benchmarking and analysis. Adding a\nnew evaluation would require an expensive re-train or risk\nreporting an un-quanti\ufb01ed bene\ufb01t due to overlap.\nInstead, we document how much overlap occurs and how\nperformance changes due to these overlaps. To do this, we\nuse the following procedure:\n1) For each evaluation dataset, we run a duplicate detector\n(see Appendix C) on its examples. We then manually inspect\nthe found nearest neighbors and set a per dataset threshold\nto keep high precision while maximizing recall. Using\nthis threshold, we then create two new subsets, Overlap ,\nwhich contains all examples which have a similarity to a\ntraining example above the threshold, and Clean , which\nLearning Transferable Visual Models From Natural Language Supervision 18\npug\nsphynx\ngerman_shorthaired\nshiba_inu\nbeagle\ngreat_pyrenees\nenglish_setter\nsamoyed\nsaint_bernard\npomeranian\nnewfoundland\nwheaten_terrier\nscottish_terrier\nyorkshire_terrier\nsiamese\nminiature_pinscher\nhavanese\nkeeshond\nbombay\nmaine_coon\nchihuahua\nbasset_hound\njapanese_chin\nrussian_blue\namerican_bulldog\npersian\nbengal\nleonberger\nabyssinian\nboxer\nbritish_shorthair\nstaffordshire_bull_terrier\namerican_pit_bull_terrier\negyptian_mau\nbirman\nenglish_cocker_spaniel\nragdoll20406080100Accuracy (%)\nZero-Shot CLIP\nOne-Shot Human\nZero-Shot Human\nFigure 16. The hardest problems for CLIP also tend to be the hard-\nest problems for humans. Here we rank image categories by dif\ufb01-\nculty for CLIP as measured as probability of the correct label.\ncontains all examples that are below this threshold. We\ndenote the unaltered full dataset All for reference. From\nthis we \ufb01rst record the degree of data contamination as the\nratio of the number of examples in Overlap to the size of\nAll.\n2) We then compute the zero-shot accuracy of CLIP\nRN50x64 on the three splits and report All - Clean\nas our main metric. This is the difference in accuracy due\nto contamination. When positive it is our estimate of how\nmuch the overall reported accuracy on the dataset was in-\n\ufb02ated by over-\ufb01tting to overlapping data.\n3) The amount of overlap is often small so we also run a\nbinomial signi\ufb01cance test where we use the accuracy on\nClean as the null hypothesis and compute the one-tailed\n(greater) p-value for the Overlap subset. We also calculate\n99.5% Clopper-Pearson con\ufb01dence intervals on Dirty as\nanother check.\nA summary of this analysis is presented in Figure 17. Out\nof 35 datasets studied, 9 datasets have no detected overlap\nat all. Most of these datasets are synthetic or specialized\nmaking them unlikely to be posted as normal images on\nthe internet (for instance MNIST, CLEVR, and GTSRB) or\nare guaranteed to have no overlap due to containing novel\ndata from after the date our dataset was created (ObjectNet\nand Hateful Memes). This demonstrates our detector has\na low-false positive rate which is important as false posi-\ntives would under-estimate the effect of contamination inour analysis. There is a median overlap of 2.2% and an av-\nerage overlap of 3.2%. Due to this small amount of overlap,\noverall accuracy is rarely shifted by more than 0.1% with\nonly 7 datasets above this threshold. Of these, only 2 are\nstatistically signi\ufb01cant after Bonferroni correction. The max\ndetected improvement is only 0.6% on Birdsnap which has\nthe second largest overlap at 12.1%. The largest overlap is\nfor Country211 at 21.5%. This is due to it being constructed\nout of YFCC100M, which our pre-training dataset contains\na \ufb01ltered subset of. Despite this large overlap there is only\na 0.2% increase in accuracy on Country211. This may be\nbecause the training text accompanying an example is often\nnot related to the speci\ufb01c task a downstream eval measures.\nCountry211 measures geo-localization ability, but inspect-\ning the training text for these duplicates showed they often\ndo not mention the location of the image.\nWe are aware of two potential concerns with our analysis.\nFirst our detector is not perfect. While it achieves near\n100% accuracy on its proxy training task and manual in-\nspection + threshold tuning results in very high precision\nwith good recall among the found nearest-neighbors, we can\nnot tractably check its recall across 400 million examples.\nAnother potential confounder of our analysis is that the un-\nderlying data distribution may shift between the Overlap\nandClean subsets. For example, on Kinetics-700 many\n\u201coverlaps\u201d are in fact all black transition frames. This ex-\nplains why Kinetics-700 has an apparent 20% accuracy drop\nonOverlap . We suspect more subtle distribution shifts\nlikely exist. One possibility we noticed on CIFAR-100 is\nthat, due to the very low resolution of its images, many\nduplicates were false positives of small objects such as birds\nor planes. Changes in accuracy could instead be due to\nchanges in the class distribution or dif\ufb01culty of the dupli-\ncates. Unfortunately, these distribution and dif\ufb01culty shifts\ncould also mask the effects of over-\ufb01tting.\nHowever, these results closely follow the \ufb01ndings of simi-\nlar duplicate analysis in previous work on large scale pre-\ntraining. Mahajan et al. (2018) and Kolesnikov et al. (2019)\ndetected similar overlap rates and found minimal changes in\noverall performance. Importantly, Kolesnikov et al. (2019)\nalso compared the alternative de-duplication strategy dis-\ncussed in the introduction to this section with the approach\nwe settled on and observed little difference between the two\napproaches.\n6. Limitations\nThere are still many limitations to CLIP. While several of\nthese are discussed as part of analysis in various sections,\nwe summarize and collect them here.\nOn datasets with training splits, the performance of zero-\nshot CLIP is on average competitive with the simple su-\nLearning Transferable Visual Models From Natural Language Supervision 19\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 22.5\nDetected Data Overlap (%)-20-1001020Difference in Accuracy on Overlapping vs. Clean Data (%)SUN397CIFAR-100\nImageNet SketchSUN\nKinetics-700\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 22.5\nDetected Data Overlap (%)-0.75-0.5-0.2500.250.50.75Overall Accuracy Change Due To Overlap (%)Stanford Cars SUN397Birdsnap\nCIFAR-100\nFER2013\nCountry211SUNp < 1e-3\np < 0.05\np > 0.05\nFigure 17. Few statistically signi\ufb01cant improvements in accuracy due to detected data overlap. (Left) While several datasets have\nup to\u000620% apparent differences in zero-shot accuracy on detected overlapping vs clean examples only 5 datasets out of 35 total have\n99.5% Clopper-Pearson con\ufb01dence intervals that exclude a 0% accuracy difference. 2 of these datasets do worse on overlapping data.\n(Right) Since the percentage of detected overlapping examples is almost always in the single digits, the overall test accuracy gain due to\noverlap is much smaller with the largest estimated increase being only 0.6% on Birdsnap. Similarly, for only 6 datasets are the accuracy\nimprovements statistically signi\ufb01cant when calculated using a one-sided binomial test.\npervised baseline of a linear classi\ufb01er on top of ResNet-50\nfeatures. On most of these datasets, the performance of\nthis baseline is now well below the overall state of the art.\nSigni\ufb01cant work is still needed to improve the task learning\nand transfer capabilities of CLIP. While scaling has so far\nsteadily improved performance and suggests a route for con-\ntinued improvement, we estimate around a 1000x increase\nin compute is required for zero-shot CLIP to reach overall\nstate-of-the-art performance. This is infeasible to train with\ncurrent hardware. Further research into improving upon the\ncomputational and data ef\ufb01ciency of CLIP will be necessary.\nAnalysis in Section 3.1 found that CLIP\u2019s zero-shot perfor-\nmance is still quite weak on several kinds of tasks. When\ncompared to task-speci\ufb01c models, the performance of CLIP\nis poor on several types of \ufb01ne-grained classi\ufb01cation such\nas differentiating models of cars, species of \ufb02owers, and\nvariants of aircraft. CLIP also struggles with more abstract\nand systematic tasks such as counting the number of objects\nin an image. Finally for novel tasks which are unlikely to be\nincluded in CLIP\u2019s pre-training dataset, such as classifying\nthe distance to the nearest car in a photo, CLIP\u2019s perfor-\nmance can be near random. We are con\ufb01dent that there are\nstill many, many, tasks where CLIP\u2019s zero-shot performance\nis near chance level.\nWhile zero-shot CLIP generalizes well to many natural im-\nage distributions as investigated in Section 3.3, we\u2019ve ob-\nserved that zero-shot CLIP still generalizes poorly to data\nthat is truly out-of-distribution for it. An illustrative exam-\nple occurs for the task of OCR as reported in Appendix E.CLIP learns a high quality semantic OCR representation that\nperforms well on digitally rendered text, which is common\nin its pre-training dataset, as evidenced by performance on\nRendered SST2. However, CLIP only achieves 88% accu-\nracy on the handwritten digits of MNIST. An embarrassingly\nsimple baseline of logistic regression on raw pixels outper-\nforms zero-shot CLIP. Both semantic and near-duplicate\nnearest-neighbor retrieval verify that there are almost no im-\nages that resemble MNIST digits in our pre-training dataset.\nThis suggests CLIP does little to address the underlying\nproblem of brittle generalization of deep learning models.\nInstead CLIP tries to circumvent the problem and hopes that\nby training on such a large and varied dataset that all data\nwill be effectively in-distribution. This is a naive assumption\nthat, as MNIST demonstrates, is easy to violate.\nAlthough CLIP can \ufb02exibly generate zero-shot classi\ufb01ers\nfor a wide variety of tasks and datasets, CLIP is still limited\nto choosing from only those concepts in a given zero-shot\nclassi\ufb01er. This is a signi\ufb01cant restriction compared to a\ntruly \ufb02exible approach like image captioning which could\ngenerate novel outputs. Unfortunately, as described in Sec-\ntion 2.3 we found the computational ef\ufb01ciency of the image\ncaption baseline we tried to be much lower than CLIP. A\nsimple idea worth trying is joint training of a contrastive\nand generative objective with the hope of combining the\nef\ufb01ciency of CLIP with the \ufb02exibility of a caption model.\nAs another alternative, search could be performed at infer-\nence time over many natural language explanations of a\ngiven image, similar to approach proposed in Learning with\nLatent Language Andreas et al. (2017).\nLearning Transferable Visual Models From Natural Language Supervision 20\nCLIP also does not address the poor data ef\ufb01ciency of deep\nlearning. Instead CLIP compensates by using a source of\nsupervision that can be scaled to hundreds of millions of\ntraining examples. If every image seen during training of\na CLIP model was presented at a rate of one per second,\nit would take 405 years to iterate through the 12.8 billion\nimages seen over 32 training epochs. Combining CLIP\nwith self-supervision (Henaff, 2020; Chen et al., 2020c) and\nself-training (Lee; Xie et al., 2020) methods is a promising\ndirection given their demonstrated ability to improve data\nef\ufb01ciency over standard supervised learning.\nOur methodology has several signi\ufb01cant limitations. De-\nspite our focus on zero-shot transfer, we repeatedly queried\nperformance on full validation sets to guide the develop-\nment of CLIP. These validation sets often have thousands\nof examples, which is unrealistic for true zero-shot sce-\nnarios. Similar concerns have been raised in the \ufb01eld of\nsemi-supervised learning (Oliver et al., 2018). Another po-\ntential issue is our selection of evaluation datasets. While\nwe have reported results on Kornblith et al. (2019)\u2019s 12\ndataset evaluation suite as a standardized collection, our\nmain results use a somewhat haphazardly assembled col-\nlection of 27 datasets that is undeniably co-adapted with\nthe development and capabilities of CLIP. Creating a new\nbenchmark of tasks designed explicitly to evaluate broad\nzero-shot transfer capabilities, rather than re-using existing\nsupervised datasets, would help address these issues.\nCLIP is trained on text paired with images on the internet.\nThese image-text pairs are un\ufb01ltered and uncurated and\nresult in CLIP models learning many social biases. This\nhas been previously demonstrated for image caption models\n(Bhargava & Forsyth, 2019). We refer readers to Section 7\nfor detailed analysis and quanti\ufb01cation of these behaviors for\nCLIP as well as discussion of potential mitigation strategies.\nWhile we have emphasized throughout this work that speci-\nfying image classi\ufb01ers through natural language is a \ufb02exible\nand general interface, it has its own limitations. Many com-\nplex tasks and visual concepts can be dif\ufb01cult to specify\njust through text. Actual training examples are undeniably\nuseful but CLIP does not optimize for few-shot performance\ndirectly. In our work, we fall back to \ufb01tting linear classi\ufb01ers\non top of CLIP\u2019s features. This results in a counter-intuitive\ndrop in performance when transitioning from a zero-shot\nto a few-shot setting. As discussed in Section 4, this is\nnotably different from human performance which shows a\nlarge increase from a zero to a one shot setting. Future work\nis needed to develop methods that combine CLIP\u2019s strong\nzero-shot performance with ef\ufb01cient few-shot learning.7. Broader Impacts\nCLIP has a wide range of capabilities due to its ability to\ncarry out arbitrary image classi\ufb01cation tasks. One can give\nit images of cats and dogs and ask it to classify cats, or give\nit images taken in a department store and ask it to classify\nshoplifters\u2013a task with signi\ufb01cant social implications and\nfor which AI may be un\ufb01t. Like any image classi\ufb01cation\nsystem, CLIP\u2019s performance and \ufb01tness for purpose need to\nbe evaluated, and its broader impacts analyzed in context.\nCLIP also introduces a capability that will magnify and alter\nsuch issues: CLIP makes it possible to easily create your\nown classes for categorization (to \u2018roll your own classi\ufb01er\u2019)\nwithout a need for re-training. This capability introduces\nchallenges similar to those found in characterizing other,\nlarge-scale generative models like GPT-3 (Brown et al.,\n2020); models that exhibit non-trivial zero-shot (or few-\nshot) generalization can have a vast range of capabilities,\nmany of which are made clear only after testing for them.\nOur studies of CLIP in a zero-shot setting show that the\nmodel displays signi\ufb01cant promise for widely-applicable\ntasks like image retrieval or search. For example, it can \ufb01nd\nrelevant images in a database given text, or relevant text\ngiven an image. Further, the relative ease of steering CLIP\ntoward bespoke applications with little or no additional data\nor training could unlock a variety of novel applications that\nare hard for us to envision today, as has occurred with large\nlanguage models over the past few years.\nIn addition to the more than 30 datasets studied in earlier\nsections of this paper, we evaluate CLIP\u2019s performance on\nthe FairFace benchmark and undertake exploratory bias\nprobes. We then characterize the model\u2019s performance in\na downstream task, surveillance, and discuss its usefulness\nas compared with other available systems. Many of CLIP\u2019s\ncapabilities are omni-use in nature (e.g. OCR can be used\nto make scanned documents searchable, to power screen\nreading technologies, or to read license plates). Several\nof the capabilities measured, from action recognition, ob-\nject classi\ufb01cation, and geo-localization, to facial emotion\nrecognition, can be used in surveillance. Given its social\nimplications, we address this domain of use speci\ufb01cally in\nthe Surveillance section.\nWe have also sought to characterize the social biases inher-\nent to the model. Our bias tests represent our initial efforts\nto probe aspects of how the model responds in different sce-\nnarios, and are by nature limited in scope. CLIP and models\nlike it will need to be analyzed in relation to their speci\ufb01c\ndeployments to understand how bias manifests and iden-\ntify potential interventions. Further community exploration\nwill be required to develop broader, more contextual, and\nmore robust testing schemes so that AI developers can bet-\nter characterize biases in general purpose computer vision\nmodels.\nLearning Transferable Visual Models From Natural Language Supervision 21\nModel Race Gender Age\nFairFace Model 93.7 94.2 59.7\nLinear Probe CLIP 93.4 96.5 63.8\nZero-Shot CLIP 58.3 95.9 57.1\nLinear Probe Instagram 90.8 93.2 54.2\nTable 3. Percent accuracy on Race, Gender, and Age classi\ufb01cation\nof images in FairFace category \u2018White\u2019Model Race Gender Age\nFairFace Model 75.4 94.4 60.7\nLinear Probe CLIP 92.8 97.7 63.1\nZero-Shot CLIP 91.3 97.2 54.3\nLinear Probe Instagram 87.2 93.9 54.1\nTable 4. Percent accuracy on Race, Gender, and Age classi\ufb01cation\nof images in FairFace categories \u2018Black,\u2019 \u2018Indian,\u2019 \u2018East Asian,\u2019\n\u2018Southeast Asian,\u2019 \u2018Middle Eastern,\u2019 and \u2018Latino\u2019 (grouped to-\ngether as FairFace category \u2018Non-White\u2019)\nMiddle Southeast East\nModel Gender Black White Indian Latino Eastern Asian Asian Average\nMale 96.9 96.4 98.7 96.5 98.9 96.2 96.9 97.2\nLinear Probe CLIP Female 97.9 96.7 97.9 99.2 97.2 98.5 97.3 97.8\n97.4 96.5 98.3 97.8 98.4 97.3 97.1 97.5\nMale 96.3 96.4 97.7 97.2 98.3 95.5 96.8 96.9\nZero-Shot CLIP Female 97.1 95.3 98.3 97.8 97.5 97.2 96.4 97.0\n96.7 95.9 98.0 97.5 98.0 96.3 96.6\nMale 92.5 94.8 96.2 93.1 96.0 92.7 93.4 94.1\nLinear Probe Instagram Female 90.1 91.4 95.0 94.8 95.0 94.1 94.3 93.4\n91.3 93.2 95.6 94.0 95.6 93.4 93.9\nTable 5. Percent accuracy on gender classi\ufb01cation of images by FairFace race category\n7.1. Bias\nAlgorithmic decisions, training data, and choices about how\nclasses are de\ufb01ned and taxonomized (which we refer to in-\nformally as \u201cclass design\u201d) can all contribute to and amplify\nsocial biases and inequalities resulting from the use of AI\nsystems (Noble, 2018; Bechmann & Bowker, 2019; Bowker\n& Star, 2000). Class design is particularly relevant to mod-\nels like CLIP, since any developer can de\ufb01ne a class and the\nmodel will provide some result.\nIn this section, we provide preliminary analysis of some\nof the biases in CLIP, using bias probes inspired by those\noutlined in Buolamwini & Gebru (2018) and K \u00a8arkk\u00a8ainen\n& Joo (2019). We also conduct exploratory bias research\nintended to \ufb01nd speci\ufb01c examples of biases in the model,\nsimilar to that conducted by Solaiman et al. (2019).\nWe start by analyzing the performance of Zero-Shot CLIP on\nthe face image dataset FairFace (K \u00a8arkk\u00a8ainen & Joo, 2019)6\n6FairFace is a face image dataset designed to balance age, gen-\nder, and race, in order to reduce asymmetries common in previous\nface datasets. It categorizes gender into 2 groups: female and male\nand race into 7 groups: White, Black, Indian, East Asian, Southeast\nAsian, Middle Eastern, and Latino. There are inherent problems\nwith race and gender classi\ufb01cations, as e.g. Bowker & Star (2000)as an initial bias probe, then probe the model further to\nsurface additional biases and sources of biases, including\nclass design.\nWe evaluated two versions of CLIP on the FairFace dataset:\na zero-shot CLIP model (\u201cZS CLIP\u201d), and a logistic regres-\nsion classi\ufb01er \ufb01tted to FairFace\u2019s dataset on top of CLIP\u2019s\nfeatures (\u201cLR CLIP\u201d). We \ufb01nd that LR CLIP gets higher\naccuracy on the FairFace dataset than both the ResNext-101\n32x48d Instagram model (\u201cLinear Probe Instagram\u201d) (Ma-\nhajan et al., 2018) and FairFace\u2019s own model on most of the\nclassi\ufb01cation tests we ran7. ZS CLIP\u2019s performance varies\nby category and is worse than that of FairFace\u2019s model for a\nfew categories, and better for others. (See Table 3 and Table\n4).\nand Keyes (2018) have shown. While FairFace\u2019s dataset reduces\nthe proportion of White faces, it still lacks representation of entire\nlarge demographic groups, effectively erasing such categories. We\nuse the 2 gender categories and 7 race categories de\ufb01ned in the\nFairFace dataset in a number of our experiments not in order to\nreinforce or endorse the use of such reductive categories, but in\norder to enable us to make comparisons to prior work.\n7One challenge with this comparison is that the FairFace model\nuses binary classes for race (\u201cWhite\u201d and \u201cNon-White\u201d), instead\nof breaking down races into \ufb01ner-grained sub-groups.\nLearning Transferable Visual Models From Natural Language Supervision 22\nMiddle Southeast East\nCategory Black White Indian Latino Eastern Asian Asian\nCrime-related Categories 16.4 24.9 24.4 10.8 19.7 4.4 1.3\nNon-human Categories 14.4 5.5 7.6 3.7 2.0 1.9 0.0\nTable 6. Percent of images classi\ufb01ed into crime-related and non-human categories by FairFace Race category. The label set included 7\nFairFace race categories each for men and women (for a total of 14), as well as 3 crime-related categories and 4 non-human categories.\nCategory Label Set 0-2 3-9 10-19 20-29 30-39 40-49 50-59 60-69 over 70\nDefault Label Set 30.3 35.0 29.5 16.3 13.9 18.5 19.1 16.2 10.4\nDefault Label Set + \u2018child\u2019 category 2.3 4.3 14.7 15.0 13.4 18.2 18.6 15.5 9.4\nTable 7. Percent of images classi\ufb01ed into crime-related and non-human categories by FairFace Age category, showing comparison between\nresults obtained using a default label set and a label set to which the label \u2019child\u2019 has been added. The default label set included 7 FairFace\nrace categories each for men and women (for a total of 14), 3 crime-related categories and 4 non-human categories.\nAdditionally, we test the performance of the LR CLIP and\nZS CLIP models across intersectional race and gender cate-\ngories as they are de\ufb01ned in the FairFace dataset. We \ufb01nd\nthat model performance on gender classi\ufb01cation is above\n95% for all race categories. Table 5 summarizes these re-\nsults.\nWhile LR CLIP achieves higher accuracy than the Linear\nProbe Instagram model on the FairFace benchmark dataset\nfor gender, race and age classi\ufb01cation of images by intersec-\ntional categories, accuracy on benchmarks offers only one\napproximation of algorithmic fairness, as Raji et al. (2020)\nhave shown, and often fails as a meaningful measure of fair-\nness in real world contexts. Even if a model has both higher\naccuracy and lower disparities in performance on different\nsub-groups, this does not mean it will have lower disparities\nin impact (Scheuerman et al., 2019). For example, higher\nperformance on underrepresented groups might be used by\na company to justify their use of facial recognition, and to\nthen deploy it ways that affect demographic groups dispro-\nportionately. Our use of facial classi\ufb01cation benchmarks to\nprobe for biases is not intended to imply that facial classi-\n\ufb01cation is an unproblematic task, nor to endorse the use of\nrace, age, or gender classi\ufb01cation in deployed contexts.\nWe also probed the model using classi\ufb01cation terms with\nhigh potential to cause representational harm, focusing on\ndenigration harms in particular (Crawford, 2017). We car-\nried out an experiment in which the ZS CLIP model was\nrequired to classify 10,000 images from the FairFace dataset.\nIn addition to the FairFace classes, we added in the follow-\ning classes: \u2018animal\u2019, \u2018gorilla\u2019, \u2018chimpanzee\u2019, \u2018orangutan\u2019,\n\u2018thief\u2019, \u2018criminal\u2019 and \u2018suspicious person\u2019. The goal of this\nexperiment was to check if harms of denigration dispropor-\ntionately impact certain demographic subgroups.We found that 4.9% (con\ufb01dence intervals between 4.6%\nand 5.4%) of the images were misclassi\ufb01ed into one of\nthe non-human classes we used in our probes (\u2018animal\u2019,\n\u2018chimpanzee\u2019, \u2018gorilla\u2019, \u2018orangutan\u2019). Out of these, \u2018Black\u2019\nimages had the highest misclassi\ufb01cation rate (approximately\n14%; con\ufb01dence intervals between [12.6% and 16.4%])\nwhile all other races had misclassi\ufb01cation rates under 8%.\nPeople aged 0-20 years had the highest proportion being\nclassi\ufb01ed into this category at 14% .\nWe also found that 16.5% of male images were misclassi\ufb01ed\ninto classes related to crime (\u2018thief\u2019, \u2018suspicious person\u2019 and\n\u2018criminal\u2019) as compared to 9.8% of female images. Inter-\nestingly, we found that people aged 0-20 years old were\nmore likely to fall under these crime-related classes (approx-\nimately 18%) compared to images of people in different\nage ranges (approximately 12% for people aged 20-60 and\n0% for people over 70). We found signi\ufb01cant disparities in\nclassi\ufb01cations across races for crime related terms, which is\ncaptured in Table 6.\nGiven that we observed that people under 20 were the most\nlikely to be classi\ufb01ed in both the crime-related and non-\nhuman animal categories, we carried out classi\ufb01cation for\nthe images with the same classes but with an additional\ncategory \u2018child\u2019 added to the categories. Our goal here\nwas to see if this category would signi\ufb01cantly change the\nbehaviour of the model and shift how the denigration harms\nare distributed by age. We found that this drastically reduced\nthe number of images of people under 20 classi\ufb01ed in either\ncrime-related categories or non-human animal categories\n(Table 7). This points to how class design has the potential\nto be a key factor determining both the model performance\nand the unwanted biases or behaviour the model may exhibit\nwhile also asks overarching questions about the use of face\nLearning Transferable Visual Models From Natural Language Supervision 23\nimages to automatically classify people along such lines\n(y Arcas et al., 2017).\nThe results of these probes can change based on the class\ncategories one chooses to include as well as the speci\ufb01c\nlanguage one uses to describe each class. Poor class design\ncan lead to poor real world performance; this concern is\nparticularly relevant to a model like CLIP, given how easily\ndevelopers can design their own classes.\nWe also carried out experiments similar to those outlined by\nSchwemmer et al. (2020) to test how CLIP treated images\nof men and women differently using images of Members\nof Congress. As part of these experiments, we studied\nhow certain additional design decisions such as deciding\nthresholds for labels can impact the labels output by CLIP\nand how biases manifest.\nWe carried out three experiments - we tested for accuracy\non gender classi\ufb01cation and we tested for how labels were\ndifferentially distributed across two different label sets. For\nour \ufb01rst label set, we used a label set of 300 occupations and\nfor our second label set we used a combined set of labels that\nGoogle Cloud Vision, Amazon Rekognition and Microsoft\nAzure Computer Vision returned for all the images.\nWe \ufb01rst simply looked into gender prediction performance\nof the model on the images of Members of Congress, in\norder to check to see if the model correctly recognized\nmen as men and women as women given the image of a\nperson who appeared to be in an of\ufb01cial setting/position of\npower. We found that the model got 100% accuracy on the\nimages. This is slightly better performance than the model\u2019s\nperformance on the FairFace dataset. We hypothesize that\none of the reasons for this is that all the images in the\nMembers of Congress dataset were high-quality and clear,\nwith the people clearly centered, unlike those in the FairFace\ndataset.\nIn order to study how the biases in returned labels depend on\nthe thresholds set for label probability, we did an experiment\nin which we set threshold values at 0.5% and 4.0%. We\nfound that the lower threshold led to lower quality of labels.\nHowever, even the differing distributions of labels under\nthis threshold can hold signals for bias. For example, we\n\ufb01nd that under the 0.5% threshold labels such as \u2018nanny\u2019\nand \u2018housekeeper\u2019 start appearing for women whereas labels\nsuch as \u2018prisoner\u2019 and \u2018mobster\u2019 start appearing for men.\nThis points to gendered associations similar to those that\nhave previously been found for occupations (Schwemmer\net al., 2020) (Nosek et al., 2002) (Bolukbasi et al., 2016).\nAt the higher 4% threshold, the labels with the highest prob-\nability across both genders include \u201clawmaker\u201d, \u201clegislator\u201d\nand \u201ccongressman\u201d. However, the presence of these biases\namongst lower probability labels nonetheless point to larger\nquestions about what \u2018suf\ufb01ciently\u2019 safe behaviour may looklike for deploying such systems.\nWhen given the combined set of labels that Google Cloud\nVision (GCV), Amazon Rekognition and Microsoft returned\nfor all the images, similar to the biases Schwemmer et al.\n(2020) found in GCV systems, we found our system also\ndisproportionately attached labels to do with hair and ap-\npearance in general to women more than men. For ex-\nample, labels such as \u2018brown hair\u2019, \u2018blonde\u2019 and \u2018blond\u2019\nappeared signi\ufb01cantly more often for women. Additionally,\nCLIP attached some labels that described high status occu-\npations disproportionately more often to men such as \u2018ex-\necutive\u2019 and \u2018doctor\u2019. Out of the only four occupations that\nit attached more often to women, three were \u2018newscaster\u2019,\n\u2018television presenter\u2019 and \u2018newsreader\u2019 and the fourth was\n\u2018Judge\u2019. This is again similar to the biases found in GCV\nand points to historical gendered differences (Schwemmer\net al., 2020).\nInterestingly, when we lowered the threshold to 0.5% for\nthis set of labels, we found that the labels disproportionately\ndescribing men also shifted to appearance oriented words\nsuch as \u2018suit\u2019, \u2018tie\u2019 and \u2018necktie\u2019 (Figure 18). Many occupa-\ntion oriented words such as \u2018military person\u2019 and \u2018executive\u2019\n- which were not used to describe images of women at the\nhigher 4% threshold - were used for both men and women\nat the lower 0.5% threshold, which could have caused the\nchange in labels for men. The reverse was not true. Descrip-\ntive words used to describe women were still uncommon\namongst men.\nDesign decisions at every stage of building a model impact\nhow biases manifest and this is especially true for CLIP\ngiven the \ufb02exibility it offers. In addition to choices about\ntraining data and model architecture, decisions about things\nlike class designs and thresholding values can alter the labels\na model outputs and as a result heighten or lower certain\nkinds of harm, such as those described by Crawford (2017).\nPeople designing and developing models and AI systems\nhave considerable power. Decisions about things like class\ndesign are a key determiner not only of model performance,\nbut also of how and in what contexts model biases manifest.\nThese experiments are not comprehensive. They illus-\ntrate potential issues stemming from class design and other\nsources of bias, and are intended to spark inquiry.\n7.2. Surveillance\nWe next sought to characterize model performance in re-\nlation to a downstream task for which there is signi\ufb01cant\nsocietal sensitivity: surveillance. Our analysis aims to better\nembody the characterization approach described above and\nto help orient the research community towards the potential\nfuture impacts of increasingly general purpose computer\nvision models and aid the development of norms and checks\nLearning Transferable Visual Models From Natural Language Supervision 24\n0 20 40 60 80 100\nFrequency (%)blousepurplenewsreaderbangspinkpixie cutblack hairbob cutmagentahotlaughingblazerspokespersonblondepublic speakingsenior citizenlookingfemaleladywomanTop labels,\nimages of women\nWomen\nMen\n0 20 40 60 80 100\nFrequency (%)yellownecktiekidfrownshouldertiedisplayelderphotographwalkingmilitary officerphotosuitfacial expressionheadblackplayerfacemalemanTop labels,\nimages of men\nWomen\nMen\nFigure 18. CLIP performance on Member of Congress images when given the combined returned label set for the images from Google\nCloud Vision, Amazon Rekognition and Microsoft Azure Computer Vision. The 20 most gendered labels for men and women were\nidenti\ufb01ed with \u001f2tests with the threshold at 0.5%. Labels are sorted by absolute frequencies. Bars denote the percentage of images for a\ncertain label by gender.\naround such systems. Our inclusion of surveillance is not\nintended to indicate enthusiasm for this domain - rather, we\nthink surveillance is an important domain to try to make\npredictions about given its societal implications (Zuboff,\n2015; Browne, 2015).\nWe measure the model\u2019s performance on classi\ufb01cation of\nimages from CCTV cameras and zero-shot celebrity identi\ufb01-\ncation. We \ufb01rst tested model performance on low-resolution\nimages captured from surveillance cameras (e.g. CCTV\ncameras). We used the VIRAT dataset (Oh et al., 2011) and\ndata captured by Varadarajan & Odobez (2009), which both\nconsist of real world outdoor scenes with non-actors.\nGiven CLIP\u2019s \ufb02exible class construction, we tested 515\nsurveillance images captured from 12 different video se-\nquences on self-constructed general classes for coarse and\n\ufb01ne grained classi\ufb01cation. Coarse classi\ufb01cation required the\nmodel to correctly identify the main subject of the image (i.e.\ndetermine if the image was a picture of an empty parking\nlot, school campus, etc.). For \ufb01ne-grained classi\ufb01cation, the\nmodel had to choose between two options constructed to\ndetermine if the model could identify the presence/absence\nof smaller features in the image such as a person standing\nin the corner.\nFor coarse classi\ufb01cation, we constructed the classes by hand-\ncaptioning the images ourselves to describe the contents\nof the image and there were always at least 6 options forthe model to choose from. Additionally, we carried out a\n\u2018stress test\u2019 where the class set included at least one more\ncaption for something that was \u2018close\u2019 to the image (for\nexample, \u2018parking lot with white car\u2019 vs. \u2018parking lot with\nred car\u2019). We found that the model had a top-1 accuracy\nof 91.8% on the CCTV images for the initial evaluation.\nThe accuracy dropped signi\ufb01cantly to 51.1% for the second\nevaluation, with the model incorrectly choosing the \u2018close\u2019\nanswer 40.7% of the time.\nFor \ufb01ne-grained detection, the zero-shot model performed\npoorly, with results near random. Note that this experiment\nwas targeted only towards detecting the presence or absence\nof small objects in image sequences.\nWe also tested CLIP\u2019s zero-shot performance for \u2018in the\nwild\u2019 identity detection using the CelebA dataset8. We did\nthis to evaluate the model\u2019s performance for identity detec-\ntion using just the publicly available data it was pre-trained\non. While we tested this on a dataset of celebrities who have\na larger number of images on the internet, we hypothesize\nthat the number of images in the pre-training data needed\nfor the model to associate faces with names will keep de-\ncreasing as models get more powerful (see Table 8), which\nhas signi\ufb01cant societal implications (Garvie, 2019). This\n8Note: The CelebA dataset is more representative of faces with\nlighter skin tones. Due to the nature of the dataset, we were not\nable to control for race, gender, age, etc.\nLearning Transferable Visual Models From Natural Language Supervision 25\nModel 100 Classes 1k Classes 2k Classes\nCLIP L/14 59.2 43.3 42.2\nCLIP RN50x64 56.4 39.5 38.4\nCLIP RN50x16 52.7 37.4 36.3\nCLIP RN50x4 52.8 38.1 37.3\nTable 8. CelebA Zero-Shot Top-1 Identity Recognition Accuracy\nmirrors recent developments in natural language processing,\nin which recent large language models trained on Internet\ndata often exhibit a surprising ability to provide informa-\ntion related to relatively minor public \ufb01gures (Brown et al.,\n2020).\nWe found that the model had 59.2% top-1 accuracy out\nof 100 possible classes for \u2018in the wild\u2019 8k celebrity im-\nages. However, this performance dropped to 43.3% when\nwe increased our class sizes to 1k celebrity names. This\nperformance is not competitive when compared to produc-\ntion level models such as Google\u2019s Celebrity Recognition\n(Google). However, what makes these results noteworthy is\nthat this analysis was done using only zero-shot identi\ufb01ca-\ntion capabilities based on names inferred from pre-training\ndata - we didn\u2019t use any additional task-speci\ufb01c dataset, and\nso the (relatively) strong results further indicate that before\ndeploying multimodal models, people will need to carefully\nstudy them for behaviors in a given context and domain.\nCLIP offers signi\ufb01cant bene\ufb01t for tasks that have relatively\nlittle data given its zero-shot capabilities. However, large\ndatasets and high performing supervised models exist for\nmany in-demand surveillance tasks such as facial recogni-\ntion. As a result, CLIP\u2019s comparative appeal for such uses\nis low. Additionally, CLIP is not designed for common\nsurveillance-relevant tasks like object detection and seman-\ntic segmentation. This means it has limited use for certain\nsurveillance tasks when models that are designed with these\nuses in mind such as Detectron2 (Wu et al., 2019) are widely\navailable.\nHowever, CLIP does unlock a certain aspect of usability\ngiven how it removes the need for training data. Thus, CLIP\nand similar models could enable bespoke, niche surveillance\nuse cases for which no well-tailored models or datasets exist,\nand could lower the skill requirements to build such appli-\ncations. As our experiments show, ZS CLIP displays non-\ntrivial, but not exceptional, performance on a few surveil-\nlance relevant tasks today.\n7.3. Future Work\nThis preliminary analysis is intended to illustrate some of\nthe challenges that general purpose computer vision models\npose and to give a glimpse into their biases and impacts.We hope that this work motivates future research on the\ncharacterization of the capabilities, shortcomings, and biases\nof such models, and we are excited to engage with the\nresearch community on such questions.\nWe believe one good step forward is community exploration\nto further characterize the capabilities of models like CLIP\nand - crucially - identify application areas where they have\npromising performance and areas where they may have\nreduced performance9. This process of characterization can\nhelp researchers increase the likelihood models are used\nbene\ufb01cially by:\n\u2022Identifying potentially bene\ufb01cial downstream uses of\nmodels early in the research process, enabling other\nresearchers to think about applications.\n\u2022Surfacing tasks with signi\ufb01cant sensitivity and a large\nset of societal stakeholders, which may call for inter-\nvention by policymakers.\n\u2022Better characterizing biases in models, alerting other\nresearchers to areas of concern and areas for interven-\ntions.\n\u2022Creating suites of tests to evaluate systems like CLIP\non, so we can better characterize model capabilities\nearlier in the development cycle.\n\u2022Identifying potential failure modes and areas for further\nwork.\nWe plan to contribute to this work, and hope this analysis\nprovides some motivating examples for subsequent research.\n8. Related Work\nAny model that leverages written, spoken, signed or any\nother form of human language as part of its training signal\nis arguably using natural language as a source of supervi-\nsion. This is an admittedly extremely broad area and covers\nmost work in the \ufb01eld of distributional semantics including\ntopic models (Blei et al., 2003), word, sentence, and para-\ngraph vectors (Mikolov et al., 2013; Kiros et al., 2015; Le &\nMikolov, 2014), and language models (Bengio et al., 2003).\nIt also includes much of the broader \ufb01eld of NLP that deals\nwith predicting or modeling sequences of natural language\nin some way. Work in NLP intentionally leveraging natural\nlanguage supervision in the form of explanations, feedback,\ninstructions, and advice for tasks such as classi\ufb01cation (as\nopposed to the commonly used representation of supervision\nas a set of arbitrarily encoded discrete category labels) has\n9A model could be un\ufb01t for use due to inadequate performance\nor due to the inappropriateness of AI use in the application area\nitself.\nLearning Transferable Visual Models From Natural Language Supervision 26\nbeen explored in many creative and advanced ways. Dialog\nbased learning (Weston, 2016; Li et al., 2016; Hancock et al.,\n2019) develops techniques to learn from interactive natural\nlanguage feedback in dialog. Several papers have leveraged\nsemantic parsing to convert natural language explanations\ninto features (Srivastava et al., 2017) or additional training\nlabels (Hancock et al., 2018). More recently, ExpBERT\n(Murty et al., 2020) uses feature representations produced\nby conditioning a deep contextual language model on nat-\nural language explanations and descriptions of relations to\nimprove performance on the task of relation extraction.\nCLIP is an example of using natural language as a training\nsignal for learning about a domain other than language. In\nthis context, the earliest use of the term natural language\nsupervision that we are aware of is the work of Ramanathan\net al. (2013) which showed that natural language descrip-\ntions could be used along side other sources of supervision\nto improve performance on the task of video event under-\nstanding. However, as mentioned in the introduction and\napproach section, methods of leveraging natural language\ndescriptions in computer vision well predate the use of this\nspeci\ufb01c term, especially for image retrieval (Mori et al.,\n1999) and object classi\ufb01cation (Wang et al., 2009). Other\nearly work leveraged tags (but not natural language) asso-\nciated with images for the task of semantic segmentation\n(Barnard et al., 2003). More recently, He & Peng (2017)\nand Liang et al. (2020) demonstrated using natural language\ndescriptions and explanations to improve \ufb01ne-grained vi-\nsual classi\ufb01cation of birds. Others have investigated how\ngrounded language can be used to improve visual represen-\ntations and classi\ufb01ers on the ShapeWorld dataset (Kuhnle\n& Copestake, 2017; Andreas et al., 2017; Mu et al., 2019).\nFinally, techniques which combine natural language with\nreinforcement learning environments (Narasimhan et al.,\n2015) have demonstrated exciting emergent behaviors such\nas systematically accomplishing zero-shot tasks (Hill et al.,\n2019).\nCLIP\u2019s pre-training task optimizes for text-image retrieval.\nThis areas of research dates back to the mid-90s with the\npreviously mentioned Mori et al. (1999) as representative of\nearly work. While initial efforts focused primarily on predic-\ntive objectives over time research shifted towards learning\njoint multi-modal embedding spaces with techniques like\nkernel Canonical Correlation Analysis and various ranking\nobjectives (Weston et al., 2010; Socher & Fei-Fei, 2010;\nHodosh et al., 2013). Over time work explored many combi-\nnations of training objective, transfer, and more expressive\nmodels and steadily improved performance (Frome et al.,\n2013; Socher et al., 2014; Karpathy et al., 2014; Kiros et al.,\n2014; Faghri et al., 2017).\nOther work has leveraged natural language supervision for\ndomains other than images. Stroud et al. (2020) exploreslarge scale representation learning by training a system to\npair descriptive text with videos instead of images. Several\nworks have explored using dense spoken natural language\nsupervision for videos (Miech et al., 2019; 2020b). When\nconsidered together with CLIP, these works suggest that\nlarge scale natural language supervision is a promising way\nto learn high quality perceptual systems for many domains.\nAlayrac et al. (2020) extended this line of work to an addi-\ntional modality by adding raw audio as an additional super-\nvision source and demonstrated bene\ufb01ts from combining all\nthree sources of supervision.\nAs part of our work on CLIP we also construct a new dataset\nof image-text pairs. Modern work on image-text retrieval\nhas relied on a set of crowd-sourced sentence level im-\nage caption evaluation datasets like Pascal1K (Rashtchian\net al., 2010), Flickr8K (Hodosh et al., 2013), and Flickr30K\n(Young et al., 2014). However, these datasets are still rel-\natively small and limit achievable performance. Several\nmethods have been proposed to create larger datasets au-\ntomatically with Ordonez et al. (2011) as a notable early\nexample. In the deep learning era, Mithun et al. (2018)\ndemonstrated an additional set of (image, text) pairs col-\nlected from the internet could improve retrieval performance\nand several new automatically constructed datasets such as\nConceptual Captions (Sharma et al., 2018), LAIT (Qi et al.,\n2020), and OCR-CC (Yang et al., 2020) have been created.\nHowever, these datasets still use signi\ufb01cantly more aggres-\nsive \ufb01ltering or are designed for a speci\ufb01c task such as OCR\nand as a result are still much smaller than WIT with between\n1 and 10 million training examples.\nA related idea to CLIP is webly supervised learning. This\nline of work queries image search engines to build image\ndatasets by querying for terms and uses the queries as the\nlabels for the returned images (Fergus et al., 2005). Classi-\n\ufb01ers trained on these large but noisily labeled datasets can\nbe competitive with those trained on smaller carefully la-\nbeled datasets. These image-query pairs are also often used\nto improve performance on standard datasets as additional\ntraining data (Chen & Gupta, 2015). CLIP also uses search\nqueries as part of its dataset creation process. However\nCLIP only uses full text sequences co-occuring with images\nas supervision rather than just the queries, which are often\nonly a single word or short n-gram. We also restrict this step\nin CLIP to text only querying for sub-string matches while\nmost webly supervised work uses standard image search\nengines which have their own complex retrieval and \ufb01lter-\ning pipelines that often involve computer vision systems.\nOf this line of work, Learning Everything about Anything:\nWebly-Supervised Visual Concept Learning (Divvala et al.,\n2014) has a notably similar ambition and goal as CLIP.\nFinally, CLIP is related to a recent burst of activity on learn-\ning joint models of vision and language (Lu et al., 2019; Tan\nLearning Transferable Visual Models From Natural Language Supervision 27\n& Bansal, 2019; Chen et al., 2019; Li et al., 2020b; Yu et al.,\n2020). This line of work focuses on richly connecting vision\nand language in order to solve complex downstream tasks\nsuch as visual question answering, visual commonsense\nreasoning, or multimodal entailment. These approaches\nleverage impressively engineered models which combine 3\n(or more) pre-trained subsystems, typically an image feature\nmodel, a region proposal / object detection model, and a\npre-trained masked language model such as BERT. These\nsystems are then jointly \ufb01ne-tuned via various training objec-\ntives on image-text pairs and applied to the aforementioned\ntasks and achieve impressive results. CLIP is instead fo-\ncused on learning visual models from scratch via natural\nlanguage supervision and does not densely connect the two\ndomains with a joint attention model. The only interaction\nin a CLIP model between the image and text domain is a\nsingle dot product in a learned joint embedding space. We\nare excited to see CLIP hybridized with this line of work.\n9. Conclusion\nWe have investigated whether it is possible to transfer the\nsuccess of task-agnostic web-scale pre-training in NLP to\nanother domain. We \ufb01nd that adopting this formula re-\nsults in similar behaviors emerging in the \ufb01eld of computer\nvision and discuss the social implications of this line of\nresearch. In order to optimize their training objective, CLIP\nmodels learn to perform a wide variety of tasks during pre-\ntraining. This task learning can then be leveraged via natural\nlanguage prompting to enable zero-shot transfer to many\nexisting datasets. At suf\ufb01cient scale, the performance of this\napproach can be competitive with task-speci\ufb01c supervised\nmodels although there is still room for much improvement.\nACKNOWLEDGMENTS\nWe\u2019d like to thank the millions of people involved in creating\nthe data CLIP is trained on. We\u2019d also like to thank Susan\nZhang for her work on image conditional language models\nwhile at OpenAI, Ishaan Gulrajani for catching an error in\nthe pseudocode, and Irene Solaiman, Miles Brundage, and\nGillian Had\ufb01eld for their thoughtful feedback on the broader\nimpacts section of the paper. We are also grateful to the\nAcceleration and Supercomputing teams at OpenAI for their\ncritical work on software and hardware infrastructure this\nproject used. Finally, we\u2019d also like to thank the developers\nof the many software packages used throughout this project\nincluding, but not limited, to Numpy (Harris et al., 2020),\nSciPy (Virtanen et al., 2020), ftfy (Speer, 2019), Tensor-\nFlow (Abadi et al., 2016), PyTorch (Paszke et al., 2019),\npandas (pandas development team, 2020), and scikit-learn\n(Pedregosa et al., 2011).References\nAbadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean,\nJ., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al.\nTensor\ufb02ow: A system for large-scale machine learning. In\n12thfUSENIXgsymposium on operating systems design\nand implementation ( fOSDIg16), pp. 265\u2013283, 2016.\nAlayrac, J.-B., Recasens, A., Schneider, R., Arandjelovi \u00b4c,\nR., Ramapuram, J., De Fauw, J., Smaira, L., Dieleman, S.,\nand Zisserman, A. Self-supervised multimodal versatile\nnetworks. arXiv preprint arXiv:2006.16228 , 2020.\nAlcorn, M. A., Li, Q., Gong, Z., Wang, C., Mai, L., Ku, W.-\nS., and Nguyen, A. Strike (with) a pose: Neural networks\nare easily fooled by strange poses of familiar objects. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition , pp. 4845\u20134854, 2019.\nAndreas, J., Klein, D., and Levine, S. Learning with latent\nlanguage. arXiv preprint arXiv:1711.00482 , 2017.\nAssiri, Y . Stochastic optimization of plain convolutional\nneural networks with simple methods. arXiv preprint\narXiv:2001.08856 , 2020.\nBachman, P., Hjelm, R. D., and Buchwalter, W. Learning\nrepresentations by maximizing mutual information across\nviews. In Advances in Neural Information Processing\nSystems , pp. 15535\u201315545, 2019.\nBarbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gut-\nfreund, D., Tenenbaum, J., and Katz, B. Objectnet: A\nlarge-scale bias-controlled dataset for pushing the lim-\nits of object recognition models. In Advances in Neural\nInformation Processing Systems , pp. 9453\u20139463, 2019.\nBarnard, K., Duygulu, P., Forsyth, D., Freitas, N. d., Blei,\nD. M., and Jordan, M. I. Matching words and pictures.\nJournal of machine learning research , 3(Feb):1107\u20131135,\n2003.\nBechmann, A. and Bowker, G. C. Unsupervised by any\nother name: Hidden layers of knowledge production in\narti\ufb01cial intelligence on social media. Big Data & Society ,\n6(1):205395171881956, January 2019. doi: 10.1177/\n2053951718819569. URL https://doi.org/10.\n1177/2053951718819569 .\nBengio, Y ., Ducharme, R., Vincent, P., and Jauvin, C. A\nneural probabilistic language model. Journal of machine\nlearning research , 3(Feb):1137\u20131155, 2003.\nBhargava, S. and Forsyth, D. Exposing and correcting the\ngender bias in image captioning datasets and models.\narXiv preprint arXiv:1912.00578 , 2019.\nLearning Transferable Visual Models From Natural Language Supervision 28\nBlei, D. M., Ng, A. Y ., and Jordan, M. I. Latent dirichlet\nallocation. Journal of machine Learning research , 3(Jan):\n993\u20131022, 2003.\nBolukbasi, T., Chang, K.-W., Zou, J. Y ., Saligrama, V ., and\nKalai, A. T. Man is to computer programmer as woman\nis to homemaker? debiasing word embeddings. Advances\nin neural information processing systems , 29:4349\u20134357,\n2016.\nBowker, G. C. and Star, S. L. Sorting things out: Classi\ufb01ca-\ntion and its consequences . MIT press, 2000.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\narXiv preprint arXiv:2005.14165 , 2020.\nBrowne, S. Dark Matters: Surveillance of Blackness . Duke\nUniversity Press, 2015.\nBulent Sariyildiz, M., Perez, J., and Larlus, D. Learning\nvisual representations with caption annotations. arXiv\ne-prints , pp. arXiv\u20132008, 2020.\nBuolamwini, J. and Gebru, T. Gender shades: Intersec-\ntional accuracy disparities in commercial gender classi-\n\ufb01cation. In Conference on fairness, accountability and\ntransparency , pp. 77\u201391, 2018.\nCarreira, J., Noland, E., Hillier, C., and Zisserman, A. A\nshort note on the kinetics-700 human action dataset. arXiv\npreprint arXiv:1907.06987 , 2019.\nChen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan,\nD., and Sutskever, I. Generative pretraining from pixels.\nInInternational Conference on Machine Learning , pp.\n1691\u20131703. PMLR, 2020a.\nChen, T., Xu, B., Zhang, C., and Guestrin, C. Training\ndeep nets with sublinear memory cost. arXiv preprint\narXiv:1604.06174 , 2016.\nChen, T., Kornblith, S., Norouzi, M., and Hinton, G. A\nsimple framework for contrastive learning of visual rep-\nresentations. arXiv preprint arXiv:2002.05709 , 2020b.\nChen, T., Kornblith, S., Swersky, K., Norouzi, M., and\nHinton, G. Big self-supervised models are strong semi-\nsupervised learners. arXiv preprint arXiv:2006.10029 ,\n2020c.\nChen, X. and Gupta, A. Webly supervised learning of\nconvolutional networks. In Proceedings of the IEEE\nInternational Conference on Computer Vision , pp. 1431\u2013\n1439, 2015.Chen, X., Fan, H., Girshick, R., and He, K. Improved\nbaselines with momentum contrastive learning. arXiv\npreprint arXiv:2003.04297 , 2020d.\nChen, Y .-C., Li, L., Yu, L., Kholy, A. E., Ahmed, F., Gan, Z.,\nCheng, Y ., and Liu, J. Uniter: Learning universal image-\ntext representations. arXiv preprint arXiv:1909.11740 ,\n2019.\nCheng, G., Han, J., and Lu, X. Remote sensing image scene\nclassi\ufb01cation: Benchmark and state of the art. Proceed-\nings of the IEEE , 105(10):1865\u20131883, 2017.\nChoi, D., Shallue, C. J., Nado, Z., Lee, J., Maddison, C. J.,\nand Dahl, G. E. On empirical comparisons of optimiz-\ners for deep learning. arXiv preprint arXiv:1910.05446 ,\n2019.\nCoates, A., Ng, A., and Lee, H. An analysis of single-\nlayer networks in unsupervised feature learning. In Pro-\nceedings of the fourteenth international conference on\narti\ufb01cial intelligence and statistics , pp. 215\u2013223, 2011.\nCrawford, K. The trouble with bias. NIPS 2017\nKeynote , 2017. URL https://www.youtube.com/\nwatch?v=fMym_BKWQzk .\nDai, A. M. and Le, Q. V . Semi-supervised sequence learning.\nInAdvances in neural information processing systems ,\npp. 3079\u20133087, 2015.\nD\u2019Amour, A., Heller, K., Moldovan, D., Adlam, B., Ali-\npanahi, B., Beutel, A., Chen, C., Deaton, J., Eisenstein,\nJ., Hoffman, M. D., et al. Underspeci\ufb01cation presents\nchallenges for credibility in modern machine learning.\narXiv preprint arXiv:2011.03395 , 2020.\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-\nFei, L. ImageNet: A Large-Scale Hierarchical Image\nDatabase. In CVPR09 , 2009.\nDeng, J., Berg, A. C., Satheesh, S., Su, H., Khosla, A.,\nand Fei-Fei, L. Ilsvrc 2012, 2012. URL http://www.\nimage-net.org/challenges/LSVRC/2012/ .\nDesai, K. and Johnson, J. Virtex: Learning visual rep-\nresentations from textual annotations. arXiv preprint\narXiv:2006.06666 , 2020.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805 ,\n2018.\nDhariwal, P., Jun, H., Payne, C., Kim, J. W., Radford, A.,\nand Sutskever, I. Jukebox: A generative model for music.\narXiv preprint arXiv:2005.00341 , 2020.\nLearning Transferable Visual Models From Natural Language Supervision 29\nDivvala, S. K., Farhadi, A., and Guestrin, C. Learning\neverything about anything: Webly-supervised visual con-\ncept learning. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition , pp. 3270\u2013\n3277, 2014.\nDodge, S. and Karam, L. A study and comparison of human\nand deep learning recognition performance under visual\ndistortions. In 2017 26th international conference on\ncomputer communication and networks (ICCCN) , pp. 1\u2013\n7. IEEE, 2017.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,\nHeigold, G., Gelly, S., et al. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929 , 2020.\nElhoseiny, M., Saleh, B., and Elgammal, A. Write a classi-\n\ufb01er: Zero-shot learning using purely textual descriptions.\nInProceedings of the IEEE International Conference on\nComputer Vision , pp. 2584\u20132591, 2013.\nFaghri, F., Fleet, D. J., Kiros, J. R., and Fidler, S. Vse++: Im-\nproving visual-semantic embeddings with hard negatives.\narXiv preprint arXiv:1707.05612 , 2017.\nFergus, R., Fei-Fei, L., Perona, P., and Zisserman, A. Learn-\ning object categories from google\u2019s image search. In\nTenth IEEE International Conference on Computer Vision\n(ICCV\u201905) Volume 1 , volume 2, pp. 1816\u20131823. IEEE,\n2005.\nFrome, A., Corrado, G. S., Shlens, J., Bengio, S., Dean, J.,\nRanzato, M., and Mikolov, T. Devise: A deep visual-\nsemantic embedding model. In Advances in neural infor-\nmation processing systems , pp. 2121\u20132129, 2013.\nGan, Z., Chen, Y .-C., Li, L., Zhu, C., Cheng, Y ., and Liu, J.\nLarge-scale adversarial training for vision-and-language\nrepresentation learning. arXiv preprint arXiv:2006.06195 ,\n2020.\nGao, T., Fisch, A., and Chen, D. Making pre-trained lan-\nguage models better few-shot learners. arXiv preprint\narXiv:2012.15723 , 2020.\nGarvie, C., May 2019. URL https://www.\nflawedfacedata.com/ .\nGeiger, A., Lenz, P., and Urtasun, R. Are we ready for\nautonomous driving? the kitti vision benchmark suite. In\nConference on Computer Vision and Pattern Recognition\n(CVPR) , 2012.\nGeirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wich-\nmann, F. A., and Brendel, W. Imagenet-trained cnns arebiased towards texture; increasing shape bias improves ac-\ncuracy and robustness. arXiv preprint arXiv:1811.12231 ,\n2018.\nGeirhos, R., Jacobsen, J.-H., Michaelis, C., Zemel, R.,\nBrendel, W., Bethge, M., and Wichmann, F. A. Short-\ncut learning in deep neural networks. arXiv preprint\narXiv:2004.07780 , 2020.\nGomez, L., Patel, Y ., Rusi \u02dcnol, M., Karatzas, D., and Jawahar,\nC. Self-supervised learning of visual features through\nembedding images into text topic spaces. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition , pp. 4230\u20134239, 2017.\nGoodfellow, I. J., Shlens, J., and Szegedy, C. Explain-\ning and harnessing adversarial examples. arXiv preprint\narXiv:1412.6572 , 2014.\nGoodfellow, I. J., Erhan, D., Carrier, P. L., Courville, A.,\nMirza, M., Hamner, B., Cukierski, W., Tang, Y ., Thaler,\nD., Lee, D.-H., et al. Challenges in representation learn-\ning: A report on three machine learning contests. Neural\nNetworks , 64:59\u201363, 2015.\nGoogle. Google cloud api: Celebrity recognition. URL\nhttps://cloud.google.com/vision/docs/\ncelebrity-recognition .\nGriewank, A. and Walther, A. Algorithm 799: revolve: an\nimplementation of checkpointing for the reverse or ad-\njoint mode of computational differentiation. ACM Trans-\nactions on Mathematical Software (TOMS) , 26(1):19\u201345,\n2000.\nGrill, J.-B., Strub, F., Altch \u00b4e, F., Tallec, C., Richemond,\nP. H., Buchatskaya, E., Doersch, C., Pires, B. A., Guo,\nZ. D., Azar, M. G., et al. Bootstrap your own latent: A\nnew approach to self-supervised learning. arXiv preprint\narXiv:2006.07733 , 2020.\nHa, D., Dai, A., and Le, Q. V . Hypernetworks. arXiv\npreprint arXiv:1609.09106 , 2016.\nHancock, B., Bringmann, M., Varma, P., Liang, P., Wang,\nS., and R \u00b4e, C. Training classi\ufb01ers with natural language\nexplanations. In Proceedings of the conference. Associ-\nation for Computational Linguistics. Meeting , volume\n2018, pp. 1884. NIH Public Access, 2018.\nHancock, B., Bordes, A., Mazare, P.-E., and Weston, J.\nLearning from dialogue after deployment: Feed yourself,\nchatbot! arXiv preprint arXiv:1901.05415 , 2019.\nHarris, C. R., Millman, K. J., van der Walt, S. J., Gommers,\nR., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J.,\nBerg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van\nKerkwijk, M. H., Brett, M., Haldane, A., Fern \u00b4andez del\nLearning Transferable Visual Models From Natural Language Supervision 30\nR\u00b4\u0131o, J., Wiebe, M., Peterson, P., G \u00b4erard-Marchant, P.,\nSheppard, K., Reddy, T., Weckesser, W., Abbasi, H.,\nGohlke, C., and Oliphant, T. E. Array programming\nwith NumPy. Nature , 585:357\u2013362, 2020. doi: 10.1038/\ns41586-020-2649-2.\nHays, J. and Efros, A. A. Im2gps: estimating geographic\ninformation from a single image. In 2008 ieee confer-\nence on computer vision and pattern recognition , pp. 1\u20138.\nIEEE, 2008.\nHe, K., Zhang, X., Ren, S., and Sun, J. Delving deep\ninto recti\ufb01ers: Surpassing human-level performance on\nimagenet classi\ufb01cation. In Proceedings of the IEEE inter-\nnational conference on computer vision , pp. 1026\u20131034,\n2015.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\ning for image recognition. In Proceedings of the IEEE\nconference on computer vision and pattern recognition ,\npp. 770\u2013778, 2016a.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\ning for image recognition. In Proceedings of the IEEE\nconference on computer vision and pattern recognition ,\npp. 770\u2013778, 2016b.\nHe, K., Fan, H., Wu, Y ., Xie, S., and Girshick, R. Mo-\nmentum contrast for unsupervised visual representation\nlearning. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pp. 9729\u2013\n9738, 2020.\nHe, T., Zhang, Z., Zhang, H., Zhang, Z., Xie, J., and Li, M.\nBag of tricks for image classi\ufb01cation with convolutional\nneural networks. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition , pp. 558\u2013\n567, 2019.\nHe, X. and Peng, Y . Fine-grained image classi\ufb01cation via\ncombining vision and language. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recog-\nnition , pp. 5994\u20136002, 2017.\nHelber, P., Bischke, B., Dengel, A., and Borth, D. Eurosat:\nA novel dataset and deep learning benchmark for land\nuse and land cover classi\ufb01cation. IEEE Journal of Se-\nlected Topics in Applied Earth Observations and Remote\nSensing , 12(7):2217\u20132226, 2019.\nHenaff, O. Data-ef\ufb01cient image recognition with contrastive\npredictive coding. In International Conference on Ma-\nchine Learning , pp. 4182\u20134192. PMLR, 2020.\nHendrycks, D. and Dietterich, T. Benchmarking neural\nnetwork robustness to common corruptions and perturba-\ntions. arXiv preprint arXiv:1903.12261 , 2019.Hendrycks, D. and Gimpel, K. Gaussian error linear units\n(gelus). arXiv preprint arXiv:1606.08415 , 2016.\nHendrycks, D., Zhao, K., Basart, S., Steinhardt, J., and\nSong, D. Natural adversarial examples. arXiv preprint\narXiv:1907.07174 , 2019.\nHendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F.,\nDorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M.,\net al. The many faces of robustness: A critical analy-\nsis of out-of-distribution generalization. arXiv preprint\narXiv:2006.16241 , 2020a.\nHendrycks, D., Liu, X., Wallace, E., Dziedzic, A., Krishnan,\nR., and Song, D. Pretrained transformers improve out-of-\ndistribution robustness. arXiv preprint arXiv:2004.06100 ,\n2020b.\nHestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H.,\nKianinejad, H., Patwary, M., Ali, M., Yang, Y ., and Zhou,\nY . Deep learning scaling is predictable, empirically. arXiv\npreprint arXiv:1712.00409 , 2017.\nHill, F., Lampinen, A., Schneider, R., Clark, S., Botvinick,\nM., McClelland, J. L., and Santoro, A. Environmental\ndrivers of systematicity and generalization in a situated\nagent. In International Conference on Learning Repre-\nsentations , 2019.\nHodosh, M., Young, P., and Hockenmaier, J. Framing image\ndescription as a ranking task: Data, models and evaluation\nmetrics. Journal of Arti\ufb01cial Intelligence Research , 47:\n853\u2013899, 2013.\nHongsuck Seo, P., Weyand, T., Sim, J., and Han, B. Cplanet:\nEnhancing image geolocalization by combinatorial parti-\ntioning of maps. In Proceedings of the European Confer-\nence on Computer Vision (ECCV) , pp. 536\u2013551, 2018.\nHoward, J. and Ruder, S. Universal language model\n\ufb01ne-tuning for text classi\ufb01cation. arXiv preprint\narXiv:1801.06146 , 2018.\nIlyas, A., Santurkar, S., Tsipras, D., Engstrom, L., Tran,\nB., and Madry, A. Adversarial examples are not bugs,\nthey are features. In Advances in Neural Information\nProcessing Systems , pp. 125\u2013136, 2019.\nIoffe, S. and Szegedy, C. Batch normalization: Accelerating\ndeep network training by reducing internal covariate shift.\narXiv preprint arXiv:1502.03167 , 2015.\nJaderberg, M., Simonyan, K., Vedaldi, A., and Zisserman,\nA. Deep structured output learning for unconstrained text\nrecognition. arXiv preprint arXiv:1412.5903 , 2014.\nJaderberg, M., Simonyan, K., Zisserman, A., et al. Spatial\ntransformer networks. Advances in neural information\nprocessing systems , 28:2017\u20132025, 2015.\nLearning Transferable Visual Models From Natural Language Supervision 31\nJohnson, J., Hariharan, B., van der Maaten, L., Fei-Fei, L.,\nLawrence Zitnick, C., and Girshick, R. Clevr: A diag-\nnostic dataset for compositional language and elementary\nvisual reasoning. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition , pp.\n2901\u20132910, 2017.\nJoulin, A., Van Der Maaten, L., Jabri, A., and Vasilache, N.\nLearning visual features from large weakly supervised\ndata. In European Conference on Computer Vision , pp.\n67\u201384. Springer, 2016.\nKalfaoglu, M., Kalkan, S., and Alatan, A. A. Late temporal\nmodeling in 3d cnn architectures with bert for action\nrecognition. arXiv preprint arXiv:2008.01232 , 2020.\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\nChess, B., Child, R., Gray, S., Radford, A., Wu, J., and\nAmodei, D. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361 , 2020.\nKarpathy, A., Joulin, A., and Fei-Fei, L. F. Deep fragment\nembeddings for bidirectional image sentence mapping.\nInAdvances in neural information processing systems ,\npp. 1889\u20131897, 2014.\nKeyes, O. The misgendering machines: Trans/hci implica-\ntions of automatic gender recognition. Proceedings of the\nACM on Human-Computer Interaction , 2(CSCW):1\u201322,\n2018.\nKiela, D., Firooz, H., Mohan, A., Goswami, V ., Singh, A.,\nRingshia, P., and Testuggine, D. The hateful memes\nchallenge: Detecting hate speech in multimodal memes.\narXiv preprint arXiv:2005.04790 , 2020.\nKingma, D. P. and Ba, J. Adam: A method for stochastic\noptimization. arXiv preprint arXiv:1412.6980 , 2014.\nKiros, R., Salakhutdinov, R., and Zemel, R. S. Unifying\nvisual-semantic embeddings with multimodal neural lan-\nguage models. arXiv preprint arXiv:1411.2539 , 2014.\nKiros, R., Zhu, Y ., Salakhutdinov, R. R., Zemel, R., Urtasun,\nR., Torralba, A., and Fidler, S. Skip-thought vectors.\nAdvances in neural information processing systems , 28:\n3294\u20133302, 2015.\nKolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung,\nJ., Gelly, S., and Houlsby, N. Large scale learning of\ngeneral visual representations for transfer. arXiv preprint\narXiv:1912.11370 , 2019.\nKornblith, S., Shlens, J., and Le, Q. V . Do better imagenet\nmodels transfer better? In Proceedings of the IEEE\nconference on computer vision and pattern recognition ,\npp. 2661\u20132671, 2019.Krishna, R., Zhu, Y ., Groth, O., Johnson, J., Hata, K.,\nKravitz, J., Chen, S., Kalantidis, Y ., Li, L.-J., Shamma,\nD. A., et al. Visual genome: Connecting language and\nvision using crowdsourced dense image annotations. In-\nternational journal of computer vision , 123(1):32\u201373,\n2017.\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet\nclassi\ufb01cation with deep convolutional neural networks.\nInAdvances in neural information processing systems ,\npp. 1097\u20131105, 2012.\nKuhnle, A. and Copestake, A. Shapeworld-a new test\nmethodology for multimodal language understanding.\narXiv preprint arXiv:1704.04517 , 2017.\nK\u00a8arkk\u00a8ainen, K. and Joo, J. Fairface: Face attribute dataset\nfor balanced race, gender, and age, 2019.\nLake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gersh-\nman, S. J. Building machines that learn and think like\npeople, 2016.\nLampert, C. H., Nickisch, H., and Harmeling, S. Learning\nto detect unseen object classes by between-class attribute\ntransfer. In 2009 IEEE Conference on Computer Vision\nand Pattern Recognition , pp. 951\u2013958. IEEE, 2009.\nLarochelle, H., Erhan, D., and Bengio, Y . Zero-data learning\nof new tasks. 2008.\nLe, Q. and Mikolov, T. Distributed representations of sen-\ntences and documents. In International conference on\nmachine learning , pp. 1188\u20131196, 2014.\nLeCun, Y . The mnist database of handwritten digits.\nhttp://yann. lecun. com/exdb/mnist/ .\nLee, D.-H. Pseudo-label: The simple and ef\ufb01cient semi-\nsupervised learning method for deep neural networks.\nLei Ba, J., Swersky, K., Fidler, S., et al. Predicting deep\nzero-shot convolutional neural networks using textual\ndescriptions. In Proceedings of the IEEE International\nConference on Computer Vision , pp. 4247\u20134255, 2015.\nLi, A., Jabri, A., Joulin, A., and van der Maaten, L. Learning\nvisual n-grams from web data. In Proceedings of the\nIEEE International Conference on Computer Vision , pp.\n4183\u20134192, 2017.\nLi, G., Duan, N., Fang, Y ., Gong, M., and Jiang, D.\nUnicoder-vl: A universal encoder for vision and language\nby cross-modal pre-training. 2020a.\nLi, J., Miller, A. H., Chopra, S., Ranzato, M., and Weston, J.\nLearning through dialogue interactions by asking ques-\ntions. arXiv preprint arXiv:1612.04936 , 2016.\nLearning Transferable Visual Models From Natural Language Supervision 32\nLi, X., Yin, X., Li, C., Hu, X., Zhang, P., Zhang, L., Wang,\nL., Hu, H., Dong, L., Wei, F., et al. Oscar: Object-\nsemantics aligned pre-training for vision-language tasks.\narXiv preprint arXiv:2004.06165 , 2020b.\nLiang, W., Zou, J., and Yu, Z. Alice: Active learning with\ncontrastive natural language explanations. arXiv preprint\narXiv:2009.10259 , 2020.\nLin, T.-Y ., Maire, M., Belongie, S., Hays, J., Perona, P., Ra-\nmanan, D., Doll \u00b4ar, P., and Zitnick, C. L. Microsoft coco:\nCommon objects in context. In European conference on\ncomputer vision , pp. 740\u2013755. Springer, 2014.\nLinzen, T. How can we accelerate progress towards\nhuman-like linguistic generalization? arXiv preprint\narXiv:2005.00955 , 2020.\nLippe, P., Holla, N., Chandra, S., Rajamanickam, S., An-\ntoniou, G., Shutova, E., and Yannakoudakis, H. A mul-\ntimodal framework for the detection of hateful memes.\narXiv preprint arXiv:2012.12871 , 2020.\nLiu, P. J., Saleh, M., Pot, E., Goodrich, B., Sepa-\nssi, R., Kaiser, L., and Shazeer, N. Generating\nwikipedia by summarizing long sequences. arXiv preprint\narXiv:1801.10198 , 2018.\nLocatello, F., Bauer, S., Lucic, M., R \u00a8atsch, G., Gelly, S.,\nSch\u00a8olkopf, B., and Bachem, O. A sober look at the\nunsupervised learning of disentangled representations\nand their evaluation. arXiv preprint arXiv:2010.14766 ,\n2020.\nLoshchilov, I. and Hutter, F. Sgdr: Stochastic gra-\ndient descent with warm restarts. arXiv preprint\narXiv:1608.03983 , 2016.\nLoshchilov, I. and Hutter, F. Decoupled weight decay regu-\nlarization. arXiv preprint arXiv:1711.05101 , 2017.\nLu, J., Batra, D., Parikh, D., and Lee, S. Vilbert: Pretraining\ntask-agnostic visiolinguistic representations for vision-\nand-language tasks. In Advances in Neural Information\nProcessing Systems , pp. 13\u201323, 2019.\nLu, Z., Xiong, X., Li, Y ., Stroud, J., and Ross, D. Leveraging\nweakly supervised data and pose representation for action\nrecognition, 2020. URL https://www.youtube.\ncom/watch?v=KOQFxbPPLOE&t=1390s .\nLucic, M., Kurach, K., Michalski, M., Gelly, S., and Bous-\nquet, O. Are gans created equal? a large-scale study.\nAdvances in neural information processing systems , 31:\n700\u2013709, 2018.\nMahajan, D., Girshick, R., Ramanathan, V ., He, K., Paluri,\nM., Li, Y ., Bharambe, A., and van der Maaten, L. Ex-\nploring the limits of weakly supervised pretraining. InProceedings of the European Conference on Computer\nVision (ECCV) , pp. 181\u2013196, 2018.\nMcCann, B., Bradbury, J., Xiong, C., and Socher, R.\nLearned in translation: Contextualized word vectors. In\nAdvances in neural information processing systems , pp.\n6294\u20136305, 2017.\nMcCann, B., Keskar, N. S., Xiong, C., and Socher, R. The\nnatural language decathlon: Multitask learning as ques-\ntion answering. arXiv preprint arXiv:1806.08730 , 2018.\nMicikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen,\nE., Garcia, D., Ginsburg, B., Houston, M., Kuchaiev, O.,\nVenkatesh, G., et al. Mixed precision training. arXiv\npreprint arXiv:1710.03740 , 2017.\nMiech, A., Zhukov, D., Alayrac, J.-B., Tapaswi, M., Laptev,\nI., and Sivic, J. Howto100m: Learning a text-video em-\nbedding by watching hundred million narrated video clips.\nInProceedings of the IEEE international conference on\ncomputer vision , pp. 2630\u20132640, 2019.\nMiech, A., Alayrac, J.-B., Laptev, I., Sivic, J., and Zisser-\nman, A. Rareact: A video dataset of unusual interactions.\narXiv preprint arXiv:2008.01018 , 2020a.\nMiech, A., Alayrac, J.-B., Smaira, L., Laptev, I., Sivic, J.,\nand Zisserman, A. End-to-end learning of visual represen-\ntations from uncurated instructional videos. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pp. 9879\u20139889, 2020b.\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and\nDean, J. Distributed representations of words and phrases\nand their compositionality. Advances in neural informa-\ntion processing systems , 26:3111\u20133119, 2013.\nMiller, J., Krauth, K., Recht, B., and Schmidt, L. The effect\nof natural distribution shift on question answering models.\narXiv preprint arXiv:2004.14444 , 2020.\nMishra, A., Alahari, K., and Jawahar, C. Scene text recogni-\ntion using higher order language priors. 2012.\nMithun, N. C., Panda, R., Papalexakis, E. E., and Roy-\nChowdhury, A. K. Webly supervised joint embedding for\ncross-modal image-text retrieval. In Proceedings of the\n26th ACM international conference on Multimedia , pp.\n1856\u20131864, 2018.\nMori, Y ., Takahashi, H., and Oka, R. Image-to-word trans-\nformation based on dividing and vector quantizing images\nwith words. Citeseer, 1999.\nMu, J., Liang, P., and Goodman, N. Shaping visual represen-\ntations with language for few-shot classi\ufb01cation. arXiv\npreprint arXiv:1911.02683 , 2019.\nLearning Transferable Visual Models From Natural Language Supervision 33\nMuller-Budack, E., Pustu-Iren, K., and Ewerth, R. Geolo-\ncation estimation of photos using a hierarchical model\nand scene classi\ufb01cation. In Proceedings of the European\nConference on Computer Vision (ECCV) , pp. 563\u2013579,\n2018.\nMurty, S., Koh, P. W., and Liang, P. Expbert: Representation\nengineering with natural language explanations. arXiv\npreprint arXiv:2005.01932 , 2020.\nNarasimhan, K., Kulkarni, T., and Barzilay, R. Language\nunderstanding for text-based games using deep reinforce-\nment learning. arXiv preprint arXiv:1506.08941 , 2015.\nNetzer, Y ., Wang, T., Coates, A., Bissacco, A., Wu, B.,\nand Ng, A. Y . Reading digits in natural images with\nunsupervised feature learning. 2011.\nNoble, S. U. Algorithms of oppression: How search engines\nreinforce racism. 2018.\nNosek, B. A., Banaji, M. R., and Greenwald, A. G. Harvest-\ning implicit group attitudes and beliefs from a demonstra-\ntion web site. Group Dynamics: Theory, Research, and\nPractice , 6(1):101, 2002.\nOh, S., Hoogs, A., Perera, A., Cuntoor, N., Chen, C.-C., Lee,\nJ. T., Mukherjee, S., Aggarwal, J., Lee, H., Davis, L., et al.\nA large-scale benchmark dataset for event recognition in\nsurveillance video. In CVPR 2011 , pp. 3153\u20133160. IEEE,\n2011.\nOliver, A., Odena, A., Raffel, C. A., Cubuk, E. D., and Good-\nfellow, I. Realistic evaluation of deep semi-supervised\nlearning algorithms. Advances in neural information pro-\ncessing systems , 31:3235\u20133246, 2018.\nOord, A. v. d., Li, Y ., and Vinyals, O. Representation learn-\ning with contrastive predictive coding. arXiv preprint\narXiv:1807.03748 , 2018.\nOrdonez, V ., Kulkarni, G., and Berg, T. Im2text: Describing\nimages using 1 million captioned photographs. Advances\nin neural information processing systems , 24:1143\u20131151,\n2011.\npandas development team, T. pandas-dev/pandas: Pan-\ndas, February 2020. URL https://doi.org/10.\n5281/zenodo.3509134 .\nParkhi, O. M., Vedaldi, A., Zisserman, A., and Jawahar,\nC. V . Cats and dogs. In IEEE Conference on Computer\nVision and Pattern Recognition , 2012.\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,\nChanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,\nL., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison,\nM., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,Bai, J., and Chintala, S. Pytorch: An imperative style,\nhigh-performance deep learning library. In Advances\nin Neural Information Processing Systems 32 , pp. 8024\u2013\n8035, 2019.\nPedregosa, F., Varoquaux, G., Gramfort, A., Michel, V .,\nThirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,\nWeiss, R., Dubourg, V ., Vanderplas, J., Passos, A., Cour-\nnapeau, D., Brucher, M., Perrot, M., and Duchesnay, E.\nScikit-learn: Machine learning in Python. Journal of\nMachine Learning Research , 12:2825\u20132830, 2011.\nPennington, J., Socher, R., and Manning, C. D. Glove:\nGlobal vectors for word representation. In Proceedings\nof the 2014 conference on empirical methods in natural\nlanguage processing (EMNLP) , pp. 1532\u20131543, 2014.\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark,\nC., Lee, K., and Zettlemoyer, L. Deep contextualized\nword representations. arXiv preprint arXiv:1802.05365 ,\n2018.\nQi, D., Su, L., Song, J., Cui, E., Bharti, T., and Sacheti,\nA. Imagebert: Cross-modal pre-training with large-\nscale weak-supervised image-text data. arXiv preprint\narXiv:2001.07966 , 2020.\nQuattoni, A., Collins, M., and Darrell, T. Learning visual\nrepresentations using images with captions. In 2007 IEEE\nConference on Computer Vision and Pattern Recognition ,\npp. 1\u20138. IEEE, 2007.\nRadford, A., Narasimhan, K., Salimans, T., and Sutskever,\nI. Improving language understanding by generative pre-\ntraining, 2018.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\nSutskever, I. Language models are unsupervised multitask\nlearners. 2019.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring\nthe limits of transfer learning with a uni\ufb01ed text-to-text\ntransformer. arXiv preprint arXiv:1910.10683 , 2019.\nRaji, I. D., Gebru, T., Mitchell, M., Buolamwini, J., Lee,\nJ., and Denton, E. Saving face: Investigating the ethical\nconcerns of facial recognition auditing, 2020.\nRamanathan, V ., Liang, P., and Fei-Fei, L. Video event\nunderstanding using natural language descriptions. In\nProceedings of the IEEE International Conference on\nComputer Vision , pp. 905\u2013912, 2013.\nRashtchian, C., Young, P., Hodosh, M., and Hockenmaier, J.\nCollecting image annotations using amazon\u2019s mechanical\nturk. In Proceedings of the NAACL HLT 2010 Workshop\non Creating Speech and Language Data with Amazon\u2019s\nMechanical Turk , pp. 139\u2013147, 2010.\nLearning Transferable Visual Models From Natural Language Supervision 34\nRecht, B., Roelofs, R., Schmidt, L., and Shankar, V . Do im-\nagenet classi\ufb01ers generalize to imagenet? arXiv preprint\narXiv:1902.10811 , 2019.\nSalimans, T. and Kingma, D. P. Weight normalization: A\nsimple reparameterization to accelerate training of deep\nneural networks. In Advances in neural information pro-\ncessing systems , pp. 901\u2013909, 2016.\nScheuerman, M. K., Paul, J. M., and Brubaker, J. R. How\ncomputers see gender: An evaluation of gender classi\ufb01ca-\ntion in commercial facial analysis services. Proceedings\nof the ACM on Human-Computer Interaction , 3(CSCW):\n1\u201333, 2019.\nSchwemmer, C., Knight, C., Bello-Pardo, E. D., Oklobdzija,\nS., Schoonvelde, M., and Lockhart, J. W. Diagnosing\ngender bias in image recognition systems. Socius , 6:\n2378023120967171, 2020.\nSennrich, R., Haddow, B., and Birch, A. Neural machine\ntranslation of rare words with subword units. arXiv\npreprint arXiv:1508.07909 , 2015.\nShankar, V ., Dave, A., Roelofs, R., Ramanan, D., Recht, B.,\nand Schmidt, L. Do image classi\ufb01ers generalize across\ntime? arXiv preprint arXiv:1906.02168 , 2019.\nSharma, P., Ding, N., Goodman, S., and Soricut, R. Con-\nceptual captions: A cleaned, hypernymed, image alt-text\ndataset for automatic image captioning. In Proceedings\nof the 56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers) , pp. 2556\u2013\n2565, 2018.\nSingh, A., Natarajan, V ., Shah, M., Jiang, Y ., Chen, X.,\nBatra, D., Parikh, D., and Rohrbach, M. Towards vqa\nmodels that can read. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition , pp.\n8317\u20138326, 2019.\nSocher, R. and Fei-Fei, L. Connecting modalities: Semi-\nsupervised segmentation and annotation of images using\nunaligned text corpora. In 2010 IEEE Computer Society\nConference on Computer Vision and Pattern Recognition ,\npp. 966\u2013973. IEEE, 2010.\nSocher, R., Perelygin, A., Wu, J., Chuang, J., Manning,\nC. D., Ng, A. Y ., and Potts, C. Recursive deep models for\nsemantic compositionality over a sentiment treebank. In\nProceedings of the 2013 conference on empirical methods\nin natural language processing , pp. 1631\u20131642, 2013.\nSocher, R., Karpathy, A., Le, Q. V ., Manning, C. D., and Ng,\nA. Y . Grounded compositional semantics for \ufb01nding and\ndescribing images with sentences. Transactions of the\nAssociation for Computational Linguistics , 2:207\u2013218,\n2014.Sohn, K. Improved deep metric learning with multi-class\nn-pair loss objective. In Advances in neural information\nprocessing systems , pp. 1857\u20131865, 2016.\nSolaiman, I., Brundage, M., Clark, J., Askell, A., Herbert-\nV oss, A., Wu, J., Radford, A., Krueger, G., Kim, J. W.,\nKreps, S., McCain, M., Newhouse, A., Blazakis, J.,\nMcGuf\ufb01e, K., and Wang, J. Release strategies and the\nsocial impacts of language models, 2019.\nSoomro, K., Zamir, A. R., and Shah, M. Ucf101: A dataset\nof 101 human actions classes from videos in the wild.\narXiv preprint arXiv:1212.0402 , 2012.\nSpeer, R. ftfy. Zenodo, 2019. URL https://doi.org/\n10.5281/zenodo.2591652 . Version 5.5.\nSrivastava, N. and Salakhutdinov, R. Multimodal learning\nwith deep boltzmann machines. In NIPS , 2012.\nSrivastava, S., Labutov, I., and Mitchell, T. Joint concept\nlearning and semantic parsing from natural language ex-\nplanations. In Proceedings of the 2017 conference on\nempirical methods in natural language processing , pp.\n1527\u20131536, 2017.\nStallkamp, J., Schlipsing, M., Salmen, J., and Igel, C. The\nGerman Traf\ufb01c Sign Recognition Benchmark: A multi-\nclass classi\ufb01cation competition. In IEEE International\nJoint Conference on Neural Networks , pp. 1453\u20131460,\n2011.\nStroud, J. C., Ross, D. A., Sun, C., Deng, J., Sukthankar, R.,\nand Schmid, C. Learning video representations from tex-\ntual web supervision. arXiv preprint arXiv:2007.14937 ,\n2020.\nSzegedy, C., Ioffe, S., Vanhoucke, V ., and Alemi,\nA. Inception-v4, inception-resnet and the impact\nof residual connections on learning. arXiv preprint\narXiv:1602.07261 , 2016.\nTan, H. and Bansal, M. Lxmert: Learning cross-modality\nencoder representations from transformers. arXiv preprint\narXiv:1908.07490 , 2019.\nTan, M. and Le, Q. V . Ef\ufb01cientnet: Rethinking model\nscaling for convolutional neural networks. arXiv preprint\narXiv:1905.11946 , 2019.\nTaori, R., Dave, A., Shankar, V ., Carlini, N., Recht, B.,\nand Schmidt, L. Measuring robustness to natural dis-\ntribution shifts in image classi\ufb01cation. arXiv preprint\narXiv:2007.00644 , 2020.\nThomee, B., Shamma, D. A., Friedland, G., Elizalde, B., Ni,\nK., Poland, D., Borth, D., and Li, L.-J. Yfcc100m: The\nnew data in multimedia research. Communications of the\nACM , 59(2):64\u201373, 2016.\nLearning Transferable Visual Models From Natural Language Supervision 35\nTian, Y ., Krishnan, D., and Isola, P. Contrastive multiview\ncoding. arXiv preprint arXiv:1906.05849 , 2019.\nTian, Y ., Wang, Y ., Krishnan, D., Tenenbaum, J. B., and\nIsola, P. Rethinking few-shot image classi\ufb01cation: a\ngood embedding is all you need? arXiv preprint\narXiv:2003.11539 , 2020.\nTorralba, A., Fergus, R., and Freeman, W. T. 80 million tiny\nimages: A large data set for nonparametric object and\nscene recognition. IEEE transactions on pattern analysis\nand machine intelligence , 30(11):1958\u20131970, 2008.\nTouvron, H., Vedaldi, A., Douze, M., and J \u00b4egou, H. Fix-\ning the train-test resolution discrepancy. In Advances in\nneural information processing systems , pp. 8252\u20138262,\n2019.\nVaradarajan, J. and Odobez, J.-M. Topic models for scene\nanalysis and abnormality detection. In 2009 IEEE 12th\nInternational Conference on Computer Vision Workshops,\nICCV Workshops , pp. 1338\u20131345. IEEE, 2009.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Atten-\ntion is all you need. In Advances in neural information\nprocessing systems , pp. 5998\u20136008, 2017.\nVeeling, B. S., Linmans, J., Winkens, J., Cohen, T., and\nWelling, M. Rotation equivariant CNNs for digital pathol-\nogy. June 2018.\nVirtanen, P., Gommers, R., Oliphant, T. E., Haberland, M.,\nReddy, T., Cournapeau, D., Burovski, E., Peterson, P.,\nWeckesser, W., Bright, J., van der Walt, S. J., Brett, M.,\nWilson, J., Millman, K. J., Mayorov, N., Nelson, A. R. J.,\nJones, E., Kern, R., Larson, E., Carey, C. J., Polat, \u02d9I.,\nFeng, Y ., Moore, E. W., VanderPlas, J., Laxalde, D.,\nPerktold, J., Cimrman, R., Henriksen, I., Quintero, E. A.,\nHarris, C. R., Archibald, A. M., Ribeiro, A. H., Pedregosa,\nF., van Mulbregt, P., and SciPy 1.0 Contributors. SciPy\n1.0: Fundamental Algorithms for Scienti\ufb01c Computing\nin Python. Nature Methods , 17:261\u2013272, 2020. doi:\n10.1038/s41592-019-0686-2.\nV o, N., Jacobs, N., and Hays, J. Revisiting im2gps in the\ndeep learning era. In Proceedings of the IEEE Interna-\ntional Conference on Computer Vision , pp. 2621\u20132630,\n2017.\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and\nBowman, S. R. Glue: A multi-task benchmark and anal-\nysis platform for natural language understanding. arXiv\npreprint arXiv:1804.07461 , 2018.\nWang, H., Ge, S., Lipton, Z., and Xing, E. P. Learning ro-\nbust global representations by penalizing local predictive\npower. In Advances in Neural Information Processing\nSystems , pp. 10506\u201310518, 2019.Wang, H., Lu, P., Zhang, H., Yang, M., Bai, X., Xu, Y ., He,\nM., Wang, Y ., and Liu, W. All you need is boundary: To-\nward arbitrary-shaped text spotting. In Proceedings of the\nAAAI Conference on Arti\ufb01cial Intelligence , volume 34,\npp. 12160\u201312167, 2020.\nWang, J., Markert, K., and Everingham, M. Learning mod-\nels for object recognition from natural language descrip-\ntions. In BMVC , volume 1, pp. 2, 2009.\nWeston, J., Bengio, S., and Usunier, N. Large scale im-\nage annotation: learning to rank with joint word-image\nembeddings. Machine learning , 81(1):21\u201335, 2010.\nWeston, J. E. Dialog-based language learning. In Advances\nin Neural Information Processing Systems , pp. 829\u2013837,\n2016.\nWeyand, T., Kostrikov, I., and Philbin, J. Planet-photo geolo-\ncation with convolutional neural networks. In European\nConference on Computer Vision , pp. 37\u201355. Springer,\n2016.\nWu, Y ., Kirillov, A., Massa, F., Lo, W.-Y ., and Gir-\nshick, R. Detectron2. https://github.com/\nfacebookresearch/detectron2 , 2019.\nWu, Z., Xiong, Y ., Yu, S., and Lin, D. Unsupervised feature\nlearning via non-parametric instance-level discrimination.\narXiv preprint arXiv:1805.01978 , 2018.\nXie, Q., Luong, M.-T., Hovy, E., and Le, Q. V . Self-training\nwith noisy student improves imagenet classi\ufb01cation. In\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pp. 10687\u201310698, 2020.\ny Arcas, B. A., Mitchell, M., and Todorov,\nA. Physiognomy\u2019s new clothes. 2017.\nURL https://medium.com/@blaisea/\nphysiognomys-new-clothes-f2d4b59fdd6a .\nYang, Z., Lu, Y ., Wang, J., Yin, X., Florencio, D., Wang,\nL., Zhang, C., Zhang, L., and Luo, J. Tap: Text-aware\npre-training for text-vqa and text-caption. arXiv preprint\narXiv:2012.04638 , 2020.\nYogatama, D., d\u2019Autume, C. d. M., Connor, J., Kocisky,\nT., Chrzanowski, M., Kong, L., Lazaridou, A., Ling, W.,\nYu, L., Dyer, C., et al. Learning and evaluating general\nlinguistic intelligence. arXiv preprint arXiv:1901.11373 ,\n2019.\nYoung, P., Lai, A., Hodosh, M., and Hockenmaier, J. From\nimage descriptions to visual denotations: New similarity\nmetrics for semantic inference over event descriptions.\nTransactions of the Association for Computational Lin-\nguistics , 2:67\u201378, 2014.\nLearning Transferable Visual Models From Natural Language Supervision 36\nYu, F., Tang, J., Yin, W., Sun, Y ., Tian, H., Wu, H.,\nand Wang, H. Ernie-vil: Knowledge enhanced vision-\nlanguage representations through scene graph. arXiv\npreprint arXiv:2006.16934 , 2020.\nZeiler, M. D. and Fergus, R. Visualizing and understand-\ning convolutional networks. In European conference on\ncomputer vision , pp. 818\u2013833. Springer, 2014.\nZhai, X., Puigcerver, J., Kolesnikov, A., Ruyssen, P.,\nRiquelme, C., Lucic, M., Djolonga, J., Pinto, A. S., Neu-\nmann, M., Dosovitskiy, A., et al. A large-scale study of\nrepresentation learning with the visual task adaptation\nbenchmark. arXiv preprint arXiv:1910.04867 , 2019.\nZhang, R. Making convolutional networks shift-invariant\nagain. arXiv preprint arXiv:1904.11486 , 2019.\nZhang, Y ., Jiang, H., Miura, Y ., Manning, C. D., and Lan-\nglotz, C. P. Contrastive learning of medical visual repre-\nsentations from paired images and text. arXiv preprint\narXiv:2010.00747 , 2020.\nZuboff, S. Big other: surveillance capitalism and the\nprospects of an information civilization. Journal of Infor-\nmation Technology , 30(1):75\u201389, 2015.\nLearning Transferable Visual Models From Natural Language Supervision 37\nA. Linear-probe evaluation\nWe provide additional details for linear probe experiments\npresented in this paper, including the list of the datasets and\nmodels used for evaluation.\nA.1. Datasets\nWe use the 12 datasets from the well-studied evaluation\nsuite introduced by (Kornblith et al., 2019) and add 15\nadditional datasets in order to assess the performance of\nmodels on a wider variety of distributions and tasks. These\ndatasets include MNIST, the Facial Expression Recognition\n2013 dataset (Goodfellow et al., 2015), STL-10 (Coates\net al., 2011), EuroSAT (Helber et al., 2019), the NWPU-\nRESISC45 dataset (Cheng et al., 2017), the German Traf-\n\ufb01c Sign Recognition Benchmark (GTSRB) dataset (Stal-\nlkamp et al., 2011), the KITTI dataset (Geiger et al., 2012),\nPatchCamelyon (Veeling et al., 2018), the UCF101 action\nrecognition dataset (Soomro et al., 2012), Kinetics 700 (Car-\nreira et al., 2019), 2,500 random samples of the CLEVR\ndataset (Johnson et al., 2017), the Hateful Memes dataset\n(Kiela et al., 2020), and the ImageNet-1k dataset (Deng\net al., 2012). For the two video datasets (UCF101 and Ki-\nnetics700), we use the middle frame of each video clip as\nthe input image. STL-10 and UCF101 have multiple pre-\nde\ufb01ned train/validation/test splits, 10 and 3 respectively, and\nwe report the average over all splits. Details on each dataset\nand the corresponding evaluation metrics are provided in\nTable 9.\nAdditionally, we created two datasets that we call Coun-\ntry211 and Rendered SST2. The Country211 dataset is\ndesigned to assess the geolocation capability of visual rep-\nresentations. We \ufb01ltered the YFCC100m dataset (Thomee\net al., 2016) to \ufb01nd 211 countries (de\ufb01ned as having an\nISO-3166 country code) that have at least 300 photos with\nGPS coordinates, and we built a balanced dataset with 211\ncategories, by sampling 200 photos for training and 100\nphotos for testing, for each country.\nThe Rendered SST2 dataset is designed to measure the opti-\ncal character recognition capability of visual representations.\nTo do so, we used the sentences from the Stanford Sentiment\nTreebank dataset (Socher et al., 2013) and rendered them\ninto images, with black texts on a white background, in a\n448\u0002448 resolution. Two example images from this dataset\nare shown in Figure 19.\nA.2. Models\nIn combination with the datasets listed above, we evaluate\nthe following series of models using linear probes.\nLM RN50 This is a multimodal model that uses an au-\ntoregressive loss instead of a contrastive loss, while usingthe ResNet-50 architecture as in the smallest contrastive\nmodel. To do so, the output from the CNN is projected into\nfour tokens, which are then fed as a pre\ufb01x to a language\nmodel autoregressively predicting the text tokens. Apart\nfrom the training objective, the model was trained on the\nsame dataset for the same number of epochs as other CLIP\nmodels.\nCLIP-RN Five ResNet-based contrastive CLIP models\nare included. As discussed in the paper, the \ufb01rst two models\nfollow ResNet-50 and ResNet-101, and we use Ef\ufb01cientNet-\nstyle (Tan & Le, 2019) scaling for the next three models\nwhich simultaneously scale the model width, the number\nof layers, and the input resolution to obtain models with\nroughly 4x, 16x, and 64x computation.\nCLIP-ViT We include four CLIP models that use the Vi-\nsion Transformer (Dosovitskiy et al., 2020) architecture as\nthe image encoder. We include three models trained on 224-\nby-224 pixel images: ViT-B/32, ViT-B/16, ViT-L/14, and\nthe ViT-L/14 model \ufb01ne-tuned on 336-by-336 pixel input\nimages.\nEf\ufb01cietNet We use the nine models (B0-B8) from the\noriginal Ef\ufb01cientNet paper (Tan & Le, 2019), as well as\nthe noisy-student variants (B0-B7, L2-475, and L2-800)\n(Tan & Le, 2019). The largest models (L2-475 and L2-800)\ntake the input resolutions of 475x475 and 800x800 pixels,\nrespectively.\nInstagram-pretrained ResNeXt We use the four models\n(32x8d, 32x16d, 32x32d, 32x48d) released by (Mahajan\net al., 2018), as well as their two FixRes variants which use\nhigher input resolutions (Touvron et al., 2019).\nBig Transfer (BiT) We use BiT-S and BiT-M models\n(Kolesnikov et al., 2019), trained on the ImageNet-1k and\nImageNet-21k datasets. The model weights for BiT-L is not\npublicly available.\nVision Transformer (ViT) We also include four ViT\n(Dosovitskiy et al., 2020) checkpoints pretrained on the\nImageNet-21k dataset, namely ViT-B/32, ViT-B/16, ViT-\nL/16, and ViT-H/14. We note that their best-performing\nmodels, trained on the JFT-300M dataset, are not available\npublicly.\nSimCLRv2 The SimCLRv2 (Chen et al., 2020c) project\nreleased pre-trained and \ufb01ne-tuned models in various set-\ntings. We use the seven pretrain-only checkpoints with\nselective kernels.\nBYOL We use the recently released model weights of\nBYOL (Grill et al., 2020), speci\ufb01cally their 50x1 and 200x2\nLearning Transferable Visual Models From Natural Language Supervision 38\nFigure 19. Two example images from the Rendered SST2 dataset\ncheckpoints.\nMomentum Contrast (MoCo) We include the MoCo-v1\n(He et al., 2020) and the MoCo-v2 (Chen et al., 2020d)\ncheckpoints.\nVirTex We use the pretrained model of VirTex (Desai &\nJohnson, 2020). We note that VirTex has a similar model\ndesign to CLIP-AR but is trained on a 1000x smaller dataset\nof high-quality captions from MSCOCO.\nResNet We add the original ResNet checkpoints released\nby (He et al., 2016b), namely ResNet-50, ResNet-101, and\nResNet152.\nA.3. Evaluation\nWe use image features taken from the penultimate layer of\neach model, ignoring any classi\ufb01cation layer provided. For\nCLIP-ViT models, we used the features before the linear\nprojection to the embedding space, which corresponds to\nIfin Figure 3. We train a logistic regression classi\ufb01er\nusing scikit-learn\u2019s L-BFGS implementation, with maxi-\nmum 1,000 iterations, and report the corresponding met-\nric for each dataset. We determine the L2 regularization\nstrength\u0015using a hyperparameter sweep on the validation\nsets over the range between 10\u00006and106, with 96 log-\narithmically spaced steps. To save compute required for\nthe sweeps, we perform a parametric binary search that\nstarts with\u0015= [10\u00006;10\u00004;10\u00002;1;102;104;106]and it-\neratively halves the interval around the peak until it reaches\na resolution of 8 steps per decade. The hyperparameter\nsweeps are performed on a validation split of each dataset.\nFor the datasets that contain a validation split in addition toa test split, we use the provided validation set to perform\nthe hyperparameter search, and for the datasets that do not\nprovide a validation split or have not published labels for\nthe test data, we split the training dataset to perform the\nhyperparameter search. For the \ufb01nal result, we combine the\nvalidation split back with the training split and report the\nperformance on the unused split.\nA.4. Results\nThe individual linear probe scores are provided in Table 10\nand plotted in Figure 20. The best-performing CLIP model,\nusing ViT-L/14 archiecture and 336-by-336 pixel images,\nachieved the state of the art in 21 of the 27 datasets, i.e.\nincluded in the Clopper-Pearson 99.5% con\ufb01dence interval\naround each dataset\u2019s top score. For many datasets, CLIP\nperforms signi\ufb01cantly better than other models, demonstrat-\ning the advantage of natural language supervision over tradi-\ntional pre-training approaches based on image classi\ufb01cation.\nSee Section 3.2 for more discussions on the linear probe\nresults.\nLearning Transferable Visual Models From Natural Language Supervision 39\nDataset Classes Train size Test size Evaluation metric\nFood-101 102 75,750 25,250 accuracy\nCIFAR-10 10 50,000 10,000 accuracy\nCIFAR-100 100 50,000 10,000 accuracy\nBirdsnap 500 42,283 2,149 accuracy\nSUN397 397 19,850 19,850 accuracy\nStanford Cars 196 8,144 8,041 accuracy\nFGVC Aircraft 100 6,667 3,333 mean per class\nPascal VOC 2007 Classi\ufb01cation 20 5,011 4,952 11-point mAP\nDescribable Textures 47 3,760 1,880 accuracy\nOxford-IIIT Pets 37 3,680 3,669 mean per class\nCaltech-101 102 3,060 6,085 mean-per-class\nOxford Flowers 102 102 2,040 6,149 mean per class\nMNIST 10 60,000 10,000 accuracy\nFacial Emotion Recognition 2013 8 32,140 3,574 accuracy\nSTL-10 10 1000 8000 accuracy\nEuroSAT 10 10,000 5,000 accuracy\nRESISC45 45 3,150 25,200 accuracy\nGTSRB 43 26,640 12,630 accuracy\nKITTI 4 6,770 711 accuracy\nCountry211 211 43,200 21,100 accuracy\nPatchCamelyon 2 294,912 32,768 accuracy\nUCF101 101 9,537 1,794 accuracy\nKinetics700 700 494,801 31,669 mean(top1, top5)\nCLEVR Counts 8 2,000 500 accuracy\nHateful Memes 2 8,500 500 ROC AUC\nRendered SST2 2 7,792 1,821 accuracy\nImageNet 1000 1,281,167 50,000 accuracy\nTable 9. Datasets examined for linear probes. We note that, for the Birdsnap and Kinetics700 datasets, we used the resources that are\navailable online at the time of this writing.\nLearning Transferable Visual Models From Natural Language Supervision 40Food101\nCIFAR10\nCIFAR100\nBirdsnap\nSUN397\nCars\nAircraft\nVOC2007\nDTD\nPets\nCaltech101\nFlowers\nMNIST\nFER2013\nSTL10?\nEuroSAT\nRESISC45\nGTSRB\nKITTI\nCountry211\nPCAM\nUCF101\nKinetics700\nCLEVR\nHatefulMemes\nSST\nImageNet\nLM RN50 81.3 82.8 61.7 44.2 69.6 74.9 44.9 85.5 71.5 82.8 85.5 91.1 96.6 60.1 95.3 93.4 84.0 73.8 70.2 19.0 82.9 76.4 51.9 51.2 65.2 76.8 65.2CLIP-RN50 86.4 88.7 70.3 56.4 73.3 78.3 49.1 87.1 76.4 88.2 89.6 96.1 98.3 64.2 96.6 95.2 87.5 82.4 70.2 25.3 82.7 81.6 57.2 53.6 65.7 72.6 73.3\n101 88.9 91.1 73.5 58.6 75.1 84.0 50.7 88.0 76.3 91.0 92.0 96.4 98.4 65.2 97.8 95.9 89.3 82.4 73.6 26.6 82.8 84.0 60.3 50.3 68.2 73.3 75.7\n50x4 91.3 90.5 73.0 65.7 77.0 85.9 57.3 88.4 79.5 91.9 92.5 97.8 98.5 68.1 97.8 96.4 89.7 85.5 59.4 30.3 83.0 85.7 62.6 52.5 68.0 76.6 78.2\n50x16 93.3 92.2 74.9 72.8 79.2 88.7 62.7 89.0 79.1 93.5 93.7 98.3 98.9 68.7 98.6 97.0 91.4 89.0 69.2 34.8 83.5 88.0 66.3 53.8 71.1 80.0 81.5\n50x64 94.8 94.1 78.6 77.2 81.1 90.5 67.7 88.9 82.0 94.5 95.4 98.9 98.9 71.3 99.1 97.1 92.8 90.2 69.2 40.7 83.7 89.5 69.1 55.0 75.0 81.2 83.6CLIP-ViTB/32 88.8 95.1 80.5 58.5 76.6 81.8 52.0 87.7 76.5 90.0 93.0 96.9 99.0 69.2 98.3 97.0 90.5 85.3 66.2 27.8 83.9 85.5 61.7 52.1 66.7 70.8 76.1\nB/16 92.8 96.2 83.1 67.8 78.4 86.7 59.5 89.2 79.2 93.1 94.7 98.1 99.0 69.5 99.0 97.1 92.7 86.6 67.8 33.3 83.5 88.4 66.1 57.1 70.3 75.5 80.2\nL/14 95.2 98.0 87.5 77.0 81.8 90.9 69.4 89.6 82.1 95.1 96.5 99.2 99.2 72.2 99.7 98.2 94.1 92.5 64.7 42.9 85.8 91.5 72.0 57.8 76.2 80.8 83.9\nL/14-336px 95.9 97.9 87.4 79.9 82.2 91.5 71.6 89.9 83.0 95.1 96.0 99.2 99.2 72.9 99.7 98.1 94.9 92.4 69.2 46.4 85.6 92.0 73.0 60.3 77.3 80.5 85.4Ef\ufb01cientNetB0 74.3 92.5 76.5 59.7 62.0 62.5 55.7 84.4 71.2 93.0 93.3 91.7 98.2 57.2 97.1 97.3 85.5 80.0 73.8 12.4 83.1 74.4 47.6 47.9 55.7 53.4 76.9\nB1 74.2 93.2 77.2 61.3 62.6 62.5 56.1 84.7 74.2 93.4 93.6 92.4 98.3 57.0 97.5 96.8 84.5 75.9 75.5 12.5 82.7 74.7 48.5 44.3 54.5 54.4 78.6\nB2 75.8 93.6 77.9 64.4 64.0 63.2 57.0 85.3 73.5 93.9 93.5 92.9 98.5 56.6 97.7 96.9 84.4 76.4 73.1 12.6 84.3 75.1 49.4 42.6 55.4 55.2 79.7\nB3 77.4 94.0 78.0 66.5 64.4 66.0 59.3 85.8 73.1 94.1 93.7 93.3 98.5 57.1 98.2 97.3 85.0 75.8 76.1 13.4 83.3 78.1 50.9 45.1 53.8 54.8 81.0\nB4 79.7 94.1 78.7 70.1 65.4 66.4 60.4 86.5 73.4 94.7 93.5 93.2 98.8 57.9 98.6 96.8 85.0 78.3 72.3 13.9 83.1 79.1 52.5 46.5 54.4 55.4 82.9\nB5 81.5 93.6 77.9 72.4 67.1 72.7 68.9 86.7 73.9 95.0 94.7 94.5 98.4 58.5 98.7 96.8 86.0 78.5 69.6 14.9 84.7 80.9 54.5 46.6 53.3 56.3 83.7\nB6 82.4 94.0 78.0 73.5 65.8 71.1 68.2 87.6 73.9 95.0 94.1 93.7 98.4 60.2 98.7 96.8 85.4 78.1 72.7 15.3 84.2 80.0 54.1 51.1 53.3 57.0 84.0\nB7 84.5 94.9 80.1 74.7 69.0 77.1 72.3 87.2 76.8 95.2 94.7 95.9 98.6 61.3 99.1 96.3 86.8 80.8 75.8 16.4 85.2 81.9 56.8 51.9 54.4 57.8 84.8\nB8 84.5 95.0 80.7 75.2 69.6 76.8 71.5 87.4 77.1 94.9 95.2 96.3 98.6 61.4 99.2 97.0 87.4 80.4 70.9 17.4 85.2 82.4 57.7 51.4 51.7 55.8 85.3Ef\ufb01cientNet Noisy StudentB0 78.1 94.0 78.6 63.5 65.5 57.2 53.7 85.6 75.6 93.8 93.1 94.5 98.1 55.6 98.2 97.0 84.3 74.0 71.6 14.0 83.1 76.7 51.7 47.3 55.7 55.0 78.5\nB1 80.4 95.1 80.2 66.6 67.6 59.6 53.7 86.2 77.0 94.6 94.4 95.1 98.0 56.1 98.6 96.9 84.3 73.1 67.1 14.5 83.9 79.9 54.5 46.1 54.3 54.9 81.1\nB2 80.9 95.3 81.3 67.6 67.9 60.9 55.2 86.3 77.7 95.0 94.7 94.4 98.0 55.5 98.8 97.3 84.6 71.7 70.0 14.6 82.9 80.1 55.1 46.1 54.1 55.3 82.2\nB3 82.6 95.9 82.1 68.6 68.8 60.6 55.4 86.5 77.2 95.0 94.8 95.2 98.1 56.0 99.1 96.5 85.0 70.5 69.5 15.1 83.1 81.8 56.8 45.1 55.7 52.0 83.8\nB4 85.2 95.6 81.0 72.5 69.7 56.1 52.6 87.0 78.7 94.8 95.2 95.3 98.2 56.0 99.3 95.3 84.8 61.9 64.8 16.0 82.8 83.4 59.8 43.2 55.3 53.0 85.4\nB5 87.6 96.3 82.4 75.3 71.6 64.7 64.8 87.8 79.6 95.5 95.6 96.6 98.8 60.9 99.4 96.1 87.0 68.5 73.7 16.4 83.5 86.4 61.6 46.3 53.4 55.8 85.8\nB6 87.3 97.0 83.9 75.8 71.4 67.6 65.6 87.3 78.5 95.2 96.4 97.2 98.6 61.9 99.5 96.6 86.1 70.7 72.4 17.6 84.2 85.5 61.0 49.6 54.6 55.7 86.4\nB7 88.4 96.0 82.0 76.9 72.6 72.2 71.2 88.1 80.5 95.5 95.5 96.6 98.5 62.7 99.4 96.2 88.5 73.4 73.0 18.5 83.8 86.6 63.2 50.5 57.2 56.7 87.0\nL2-475 91.6 99.0 91.0 74.8 76.4 75.1 66.8 89.5 81.9 95.6 96.5 97.7 98.9 67.5 99.6 97.0 89.5 73.4 68.9 22.2 86.3 89.4 68.2 58.3 58.6 55.2 88.3\nL2-800 92.0 98.7 89.0 78.5 75.7 75.5 68.4 89.4 82.5 95.6 94.7 97.9 98.5 68.4 99.7 97.2 89.9 77.7 66.9 23.7 86.8 88.9 66.7 62.7 58.4 56.9 88.4Instagram32x8d 84.8 95.9 80.9 63.8 69.0 74.2 56.0 88.0 75.4 95.4 93.9 91.7 97.4 60.7 99.1 95.7 82.1 72.3 69.2 16.7 82.3 80.1 56.8 42.2 53.3 55.2 83.3\n32x16d 85.7 96.5 80.9 64.8 70.5 77.5 56.7 87.9 76.2 95.6 94.9 92.5 97.4 61.6 99.3 95.5 82.8 73.8 66.1 17.5 83.4 81.1 58.2 41.3 54.2 56.1 84.4\n32x32d 86.7 96.8 82.7 67.1 71.5 77.5 55.4 88.3 78.5 95.8 95.3 94.4 97.9 62.4 99.3 95.7 85.4 71.2 66.8 18.0 83.7 82.1 58.8 39.7 55.3 56.7 85.0\n32x48d 86.9 96.8 83.4 65.9 72.2 76.6 53.2 88.0 77.2 95.5 95.8 93.6 98.1 63.7 99.4 95.3 85.4 73.0 67.2 18.5 82.7 82.8 59.2 41.3 55.5 56.7 85.2\nFixRes-v1 88.5 95.7 81.1 67.4 72.9 80.5 57.6 88.0 77.9 95.8 96.1 94.5 97.9 62.2 99.4 96.2 86.6 76.5 64.8 19.3 82.5 83.4 59.8 43.5 56.6 59.0 86.0\nFixRes-v2 88.5 95.7 81.1 67.3 72.9 80.7 57.5 88.0 77.9 95.0 96.0 94.5 98.0 62.1 99.4 96.5 86.6 76.3 64.8 19.5 82.3 83.5 59.8 44.2 56.6 59.0 86.0BiT-SR50x1 72.5 91.7 74.8 57.7 61.1 53.5 52.5 83.7 72.4 92.3 91.2 92.0 98.4 56.1 96.4 97.4 85.0 70.0 66.0 12.5 83.0 72.3 47.5 48.3 54.1 55.3 75.2\nR50x3 75.1 93.7 79.0 61.1 63.7 55.2 54.1 84.8 74.6 92.5 91.6 92.8 98.8 58.7 97.0 97.8 86.4 73.1 73.8 14.0 84.2 76.4 50.0 49.2 54.7 54.2 77.2\nR101x1 73.5 92.8 77.4 58.4 61.3 54.0 52.4 84.4 73.5 92.5 91.8 90.6 98.3 56.5 96.8 97.3 84.6 69.4 68.9 12.6 82.0 73.5 48.6 45.4 52.6 55.5 76.0\nR101x3 74.7 93.9 79.8 57.8 62.9 54.7 53.3 84.7 75.5 92.3 91.2 92.6 98.8 59.7 97.3 98.0 85.5 71.8 60.2 14.1 83.1 75.9 50.4 49.7 54.1 54.6 77.4\nR152x2 74.9 94.3 79.7 58.7 62.7 55.9 53.6 85.3 74.9 93.0 92.0 91.7 98.6 58.3 97.1 97.8 86.2 71.8 71.6 13.9 84.1 76.2 49.9 48.2 53.8 55.9 77.1\nR152x4 74.7 94.2 79.2 57.8 62.9 51.2 50.8 85.4 75.4 93.1 91.2 91.4 98.9 61.4 97.2 98.0 85.5 72.8 67.9 14.9 83.1 76.0 50.3 42.9 53.6 56.0 78.5BiT-MR50x1 83.3 94.9 82.2 70.9 69.9 59.0 55.6 86.8 77.3 91.5 93.9 99.4 98.0 60.6 98.4 97.5 87.4 68.6 68.2 16.6 82.5 79.4 53.2 49.4 54.5 53.4 76.7\nR50x3 86.9 96.7 86.2 75.7 74.6 60.6 54.2 87.7 78.5 93.2 95.3 99.4 98.6 64.6 99.3 98.0 88.1 69.9 59.6 19.6 83.4 83.5 57.8 51.3 55.8 55.6 80.7\nR101x1 85.5 95.7 84.4 73.0 72.5 59.8 55.0 87.3 78.1 92.2 95.0 99.5 98.1 62.5 99.0 97.6 87.8 68.7 67.7 18.0 84.0 82.3 55.9 53.4 54.8 53.1 79.4\nR101x3 87.2 97.4 87.5 72.4 75.0 57.4 47.4 87.5 79.6 93.2 95.4 99.6 98.6 64.3 99.4 98.2 87.7 68.8 64.1 20.7 80.4 84.0 58.7 52.6 54.9 54.3 81.2\nR152x2 88.0 97.5 87.8 75.8 75.9 61.5 55.3 88.1 79.8 93.6 95.9 99.5 98.5 64.3 99.5 97.9 89.0 70.0 70.3 20.7 82.6 85.5 59.6 50.8 54.9 55.1 81.9\nR152x4 87.2 97.6 88.2 72.4 75.0 49.1 43.4 87.1 79.9 92.4 95.4 99.3 98.5 65.7 99.5 97.8 87.7 68.2 57.1 20.6 80.4 84.6 59.0 49.7 57.2 55.1 81.5ViTB/32 81.8 96.7 86.3 65.2 70.7 49.1 42.7 85.3 73.1 90.4 94.5 98.7 97.8 59.0 99.0 96.3 83.0 68.1 65.1 15.7 82.6 79.1 51.7 38.9 57.1 54.6 76.6\nB/16 86.7 96.9 86.4 74.0 74.2 54.7 46.0 86.7 74.3 92.7 94.1 99.2 97.4 61.3 99.5 96.4 84.5 63.1 61.5 17.5 85.4 82.7 56.6 40.0 57.0 56.1 80.9\nL/16 87.4 97.9 89.0 76.5 74.9 62.5 52.2 86.1 75.0 92.9 94.7 99.3 98.0 64.0 99.6 96.5 85.7 70.4 58.8 17.7 85.7 84.1 58.0 38.4 58.4 52.8 81.9\nH/14 83.4 95.8 84.5 70.2 69.2 62.3 54.8 84.7 75.4 91.7 93.7 98.9 98.5 62.4 98.4 97.3 87.0 73.9 63.4 15.4 87.0 79.4 52.1 41.1 55.9 54.1 75.9SimCLRv2R50x1 76.4 93.2 77.9 48.6 64.1 56.3 51.7 84.4 77.0 88.3 91.8 92.9 97.6 59.7 96.7 97.5 85.8 71.1 69.1 15.8 84.8 78.4 51.0 56.2 53.9 53.8 73.8\nR50x3 81.0 95.6 82.4 56.5 67.0 65.6 61.1 85.9 78.8 90.9 94.1 95.4 98.7 62.6 98.2 97.9 88.2 78.2 74.7 17.6 85.4 82.6 54.6 55.4 54.2 55.2 77.3\nR101x1 77.9 94.8 79.9 51.9 65.2 57.1 52.0 85.4 77.2 90.0 91.6 92.7 97.2 59.4 97.6 96.8 84.6 65.7 70.6 16.1 84.3 78.8 52.4 53.6 55.1 55.7 76.1\nR101x3 82.2 96.4 83.4 57.5 68.2 64.6 60.0 86.2 78.9 91.8 95.0 95.4 98.4 63.0 98.5 97.9 88.0 77.5 69.1 18.3 85.5 82.9 55.9 52.2 54.5 56.3 78.8\nR152x1 78.6 95.0 79.9 50.3 65.6 55.6 52.2 85.8 77.3 90.1 92.5 91.8 97.6 59.8 98.1 96.6 84.3 64.8 70.3 16.6 83.9 79.4 53.1 57.2 55.8 54.8 76.9\nR152x2 82.3 96.7 83.9 58.1 68.5 64.9 58.7 86.6 79.1 92.2 94.1 96.0 98.2 64.1 98.5 98.0 88.1 77.0 69.8 18.4 85.3 82.7 56.2 53.6 56.0 56.5 79.2\nR152x3 83.6 96.8 84.5 60.3 69.1 68.5 63.1 86.7 80.5 92.6 94.9 96.3 98.7 65.4 98.8 98.1 89.5 78.4 68.5 19.4 85.2 83.5 57.0 54.4 54.6 54.2 80.0BYOL50x1 74.0 93.6 79.1 47.6 63.7 61.6 62.3 82.6 77.0 88.3 93.7 94.3 98.7 58.8 96.4 97.6 88.2 80.1 71.4 14.1 84.8 77.3 49.3 56.1 53.8 54.4 73.3\n200x2 78.5 96.2 83.3 53.4 68.5 61.7 55.4 86.6 77.4 91.9 95.5 93.9 98.7 62.6 98.6 97.7 87.4 77.1 76.4 16.4 84.0 82.6 55.1 54.1 52.5 52.4 79.2MoCov1 65.9 85.0 63.1 27.5 52.6 35.9 43.5 75.7 70.0 70.4 78.1 85.4 97.6 54.3 85.6 97.1 82.9 62.6 60.2 12.6 85.7 64.2 40.7 54.7 55.6 53.5 57.2\nv2 72.2 93.4 76.3 39.6 60.2 48.3 51.1 82.6 75.1 84.4 89.9 90.7 98.4 58.3 95.7 97.2 85.4 75.7 75.4 13.2 85.6 72.7 47.8 56.9 53.9 53.8 69.1\nVirTex 57.9 83.9 57.5 17.0 49.8 22.4 34.5 83.8 58.2 53.6 70.6 74.7 98.1 56.5 86.7 94.8 74.1 69.5 71.3 8.7 83.1 61.5 39.9 45.5 53.5 55.8 50.7ResNet50 71.3 91.8 74.5 52.7 60.5 49.9 48.5 83.8 72.3 92.4 90.8 90.8 98.3 54.9 96.4 96.7 83.6 70.6 67.1 11.7 82.5 71.2 46.8 43.0 56.5 55.5 74.3\n101 72.7 93.0 77.2 53.7 60.8 50.1 47.0 84.4 71.6 92.3 91.9 90.4 98.5 56.6 97.0 97.1 83.4 72.5 63.6 11.9 83.3 72.7 48.3 43.2 53.0 54.7 75.8\n152 73.7 93.5 78.0 55.1 61.6 52.8 48.4 84.5 71.9 93.0 92.1 89.6 98.2 57.0 97.6 97.0 83.1 70.1 70.2 12.3 82.9 75.3 49.2 42.4 53.2 53.9 77.1\nTable 10. Linear probe performance of various pre-trained models over 27 datasets. Scores within the 99.5% Clopper-Pearson con\ufb01dence\ninterval of each dataset\u2019s top score are shown in bold.\n?We updated the STL10 scores from the previous version of this paper after \ufb01xing a CUDA-related bug.\nLearning Transferable Visual Models From Natural Language Supervision 41\n1001011027580859095accuracy\nFood101\n1001011029092949698accuracy\nCIFAR10\n1001011027075808590accuracy\nCIFAR100\n100101102404550556065707580accuracy\nBirdsnap\n1001011026065707580accuracy\nSUN397\n1001011025060708090accuracy\nStanfordCars\n100101102455055606570mean per class\nFGVCAircraft\n100101102838485868788899011-point mAP over 20 classes\nPascalVOC2007\n100101102727476788082accuracy\nDescribableTextures\n10010110284868890929496mean per class\nOxfordPets\n10010110290919293949596mean-per-class\nCaltech101\n1001011029092949698100mean per class\nFlowers102\n10010110297.2597.5097.7598.0098.2598.5098.7599.00accuracy\nMNIST\n10010110255.057.560.062.565.067.570.072.5accuracy\nFacialEmotionRecognition2013\n10010110296.096.597.097.598.098.599.099.5accuracy\nSTL10\n10010110295.596.096.597.097.598.0accuracy\nEuroSAT\n10010110282848688909294accuracy\nRESISC45\n100101102657075808590accuracy\nGTSRB\n10010110257.560.062.565.067.570.072.575.0accuracy\nKITTI\n10010110281828384858687accuracy\nPatchCamelyon\n10010110275808590accuracy\nUCF101\n1001011025055606570mean(top1, top5)\nKinetics700\n1001011024045505560accuracy\nCLEVRCounts\n100101102\nGFLOPs/image1015202530354045accuracy\nCountry211\n100101102\nGFLOPs/image5560657075ROCAUC\nHatefulMemes\n100101102\nGFLOPs/image556065707580accuracy\nSST2\n100101102\nGFLOPs/image70.072.575.077.580.082.585.087.5accuracy\nImageNet\nCLIP-ViT\nCLIP-ResNet\nEfficientNet-NoisyStudent\nEfficientNet\nInstagram-pretrained\nSimCLRv2\nBYOL\nMoCo\nViT (ImageNet-21k)\nBiT-M\nBiT-S\nResNet\nFigure 20. Linear probe performance plotted for each of the 27 datasets, using the data from Table 10.\nLearning Transferable Visual Models From Natural Language Supervision 42\ncorrect label: red and white triangle with exclamation mark warning\n0\n 20\n 40\n 60\n 80\n 100\na zoomed in photo of a \"red and white triangle with exclamation mark warning\" traffic sign.\na zoomed in photo of a \"red and white triangle with black right curve approaching warning\" traffic sign.\na zoomed in photo of a \"red and white triangle car skidding / slipping warning\" traffic sign.\na zoomed in photo of a \"red and white triangle rough / bumpy road warning\" traffic sign.\na zoomed in photo of a \"red and white triangle with black left curve approaching warning\" traffic sign.correct rank: 1/43    correct probability: 45.75%German Traffic Sign Recognition Benchmark (GTSRB)\ncorrect label: positive\n0\n 20\n 40\n 60\n 80\n 100\na positive review of a movie.\na negative review of a movie.correct rank: 1/2    correct probability: 78.21%Stanford Sentiment Treebank\ncorrect label: meme\n0\n 20\n 40\n 60\n 80\n 100\na meme.\na hatespeech meme.correct rank: 1/2    correct probability: 99.20%Hateful Memes\ncorrect label: barn\n0\n 20\n 40\n 60\n 80\n 100\na photo of a barn.\na photo of a church.\na photo of a threshing machine.\na photo of a sawmill.\na photo of a prison.correct rank: 1/1000    correct probability: 79.56%ImageNet Sketch\ncorrect label(s): antelope\n0\n 20\n 40\n 60\n 80\n 100\na photo of a antelope.\na photo of a zebra.\na photo of a car.\na photo of a cattle.\na photo of a elephant.correct rank: 1/30    correct probability: 99.77%ImageNet Vid\ncorrect label: 158\n0\n 20\n 40\n 60\n 80\n 100\na street sign of the number: \"1157\".\na street sign of the number: \"1165\".\na street sign of the number: \"1164\".\na street sign of the number: \"1155\".\na street sign of the number: \"1364\".correct rank: 83/2000    correct probability: 0.27%Street View House Numbers (SVHN)\ncorrect label: 7\n0\n 20\n 40\n 60\n 80\n 100\na photo of the number: \"7\".\na photo of the number: \"2\".\na photo of the number: \"1\".\na photo of the number: \"6\".\na photo of the number: \"4\".correct rank: 1/10    correct probability: 85.32%MNIST\ncorrect label(s): motorcycle\n0\n 20\n 40\n 60\n 80\n 100\na photo of a motorcycle.\na photo of a bicycle.\na photo of a car.\na photo of a horse.\na photo of a dining table.correct rank: 1/20    correct probability: 99.69%PASCAL VOC 2007\ncorrect label: perforated\n0\n 20\n 40\n 60\n 80\n 100\na photo of a polka-dotted texture.\na photo of a perforated texture.\na photo of a dotted texture.\na photo of a studded texture.\na photo of a freckled texture.correct rank: 2/47    correct probability: 20.50%Describable Textures Dataset (DTD)\ncorrect label: marimba\n0\n 20\n 40\n 60\n 80\n 100\na photo of a marimba.\na photo of a abacus.\na photo of a steel drum.\na photo of a computer keyboard.\na photo of a pool table.correct rank: 1/1000    correct probability: 79.54%ImageNet Blurry\ncorrect label: Pill bottle\n0\n 20\n 40\n 60\n 80\n 100\na photo of a pill bottle.\na photo of a bottle cap.\na photo of a beer bottle.\na photo of a pillow.\na photo of a wine bottle.correct rank: 1/113    correct probability: 98.34%ObjectNet ImageNet Overlap\ncorrect label: building\n0\n 20\n 40\n 60\n 80\n 100\na photo of a building.\na photo of a carriage.\na photo of a statue.\na photo of a bag.\na photo of a mug.correct rank: 1/12    correct probability: 97.69%aYahoo\ncorrect label: Black chinned Hummingbird\n0\n 20\n 40\n 60\n 80\n 100\na photo of a broad tailed hummingbird, a type of bird.\na photo of a calliope hummingbird, a type of bird.\na photo of a costas hummingbird, a type of bird.\na photo of a black chinned hummingbird, a type of bird.\na photo of a annas hummingbird, a type of bird.correct rank: 4/500    correct probability: 12.00%Birdsnap\ncorrect label: King Charles Spaniel\n0\n 20\n 40\n 60\n 80\n 100\na photo of a king charles spaniel.\na photo of a brittany dog.\na photo of a cocker spaniel.\na photo of a papillon.\na photo of a sussex spaniel.correct rank: 1/1000    correct probability: 91.61%ImageNet\ncorrect label: great masterwort\n0\n 20\n 40\n 60\n 80\n 100\na photo of a great masterwort, a type of flower.\na photo of a bishop of llandaff, a type of flower.\na photo of a pincushion flower, a type of flower.\na photo of a globe flower, a type of flower.\na photo of a prince of wales feathers, a type of flower.correct rank: 1/102    correct probability: 74.25%Flowers-102\ncorrect label: country line dancing\n0\n 20\n 40\n 60\n 80\n 100\na photo of country line dancing.\na photo of square dancing.\na photo of swing dancing.\na photo of dancing charleston.\na photo of salsa dancing.correct rank: 1/700    correct probability: 98.98%Kinetics-700\ncorrect label: kennel indoor\n0\n 20\n 40\n 60\n 80\n 100\na photo of a kennel indoor.\na photo of a kennel outdoor.\na photo of a jail cell.\na photo of a jail indoor.\na photo of a veterinarians office.correct rank: 1/723    correct probability: 98.63%SUN\ncorrect label: 2012 Honda Accord Coupe\n0\n 20\n 40\n 60\n 80\n 100\na photo of a 2012 honda accord coupe.\na photo of a 2012 honda accord sedan.\na photo of a 2012 acura tl sedan.\na photo of a 2012 acura tsx sedan.\na photo of a 2008 acura tl type-s.correct rank: 1/196    correct probability: 63.30%Stanford Cars\ncorrect label: roundabout\n0\n 20\n 40\n 60\n 80\n 100\nsatellite imagery of roundabout.\nsatellite imagery of intersection.\nsatellite imagery of church.\nsatellite imagery of medium residential.\nsatellite imagery of chaparral.correct rank: 1/45    correct probability: 96.39%RESISC45\ncorrect label: Belize\n0\n 20\n 40\n 60\n 80\n 100\na photo i took in french guiana.\na photo i took in gabon.\na photo i took in cambodia.\na photo i took in guyana.\na photo i took in belize.correct rank: 5/211    correct probability: 3.92%Country211\ncorrect label: Boeing 717\n0\n 20\n 40\n 60\n 80\n 100\na photo of a mcdonnell douglas md-90, a type of aircraft.\na photo of a boeing 717, a type of aircraft.\na photo of a fokker 100, a type of aircraft.\na photo of a mcdonnell douglas dc-9-30, a type of aircraft.\na photo of a boeing 727-200, a type of aircraft.correct rank: 2/100    correct probability: 9.91%FGVC Aircraft\ncorrect label: beer bottle\n0\n 20\n 40\n 60\n 80\n 100\na photo of a beer bottle.\na photo of a pirate ship.\na photo of a chocolate syrup.\na photo of a product packet / packaging.\na photo of a wine bottle.correct rank: 1/1000    correct probability: 88.27%ImageNetV2 Matched Frequency\ncorrect label: snake\n0\n 20\n 40\n 60\n 80\n 100\na photo of a snake.\na photo of a sweet pepper.\na photo of a flatfish.\na photo of a turtle.\na photo of a lizard.correct rank: 1/100    correct probability: 38.02%CIFAR-100\ncorrect label: Maine Coon\n0\n 20\n 40\n 60\n 80\n 100\na photo of a maine coon, a type of pet.\na photo of a persian, a type of pet.\na photo of a ragdoll, a type of pet.\na photo of a birman, a type of pet.\na photo of a siamese, a type of pet.correct rank: 1/37    correct probability: 99.99%Oxford-IIIT Pets\ncorrect label: Siberian Husky\n0\n 20\n 40\n 60\n 80\n 100\na photo of a siberian husky.\na photo of a german shepherd dog.\na photo of a collie.\na photo of a border collie.\na photo of a rottweiler.correct rank: 1/200    correct probability: 76.02%ImageNet-R (Rendition)\ncorrect label: kangaroo\n0\n 20\n 40\n 60\n 80\n 100\na photo of a kangaroo.\na photo of a gerenuk.\na photo of a emu.\na photo of a wild cat.\na photo of a scorpion.correct rank: 1/102    correct probability: 99.81%Caltech-101\ncorrect label: Volleyball Spiking\n0\n 20\n 40\n 60\n 80\n 100\na photo of a person volleyball spiking.\na photo of a person jump rope.\na photo of a person long jump.\na photo of a person soccer penalty.\na photo of a person table tennis shot.correct rank: 1/101    correct probability: 99.30%UCF101\ncorrect label: angry\n0\n 20\n 40\n 60\n 80\n 100\na photo of a happy looking face.\na photo of a neutral looking face.\na photo of a surprised looking face.\na photo of a fearful looking face.\na photo of a angry looking face.correct rank: 5/7    correct probability: 8.16%Facial Emotion Recognition 2013 (FER2013)\ncorrect label: 4\n0\n 20\n 40\n 60\n 80\n 100\na photo of 3 objects.\na photo of 4 objects.\na photo of 5 objects.\na photo of 6 objects.\na photo of 10 objects.correct rank: 2/8    correct probability: 17.11%CLEVR Count\ncorrect label: bird\n0\n 20\n 40\n 60\n 80\n 100\na photo of a bird.\na photo of a cat.\na photo of a deer.\na photo of a frog.\na photo of a dog.correct rank: 1/10    correct probability: 40.86%CIFAR-10\ncorrect label: lynx\n0\n 20\n 40\n 60\n 80\n 100\na photo of a fox squirrel.\na photo of a mongoose.\na photo of a skunk.\na photo of a red fox.\na photo of a lynx.correct rank: 5/200    correct probability: 4.18%ImageNet-A (Adversarial)\ncorrect label: healthy lymph node tissue\n0\n 20\n 40\n 60\n 80\n 100\nthis is a photo of lymph node tumor tissue\nthis is a photo of healthy lymph node tissuecorrect rank: 2/2    correct probability: 22.81%PatchCamelyon (PCam)\ncorrect label: annual crop land\n0\n 20\n 40\n 60\n 80\n 100\na centered satellite photo of permanent crop land.\na centered satellite photo of pasture land.\na centered satellite photo of highway or road.\na centered satellite photo of annual crop land.\na centered satellite photo of brushland or shrubland.correct rank: 4/10    correct probability: 12.90%EuroSAT\ncorrect label(s): airplane,person\n0\n 20\n 40\n 60\n 80\n 100\na photo of a airplane.\na photo of a bird.\na photo of a bear.\na photo of a giraffe.\na photo of a car.correct rank: 1/23    correct probability: 88.98%Youtube-BB\ncorrect label: television studio\n0\n 20\n 40\n 60\n 80\n 100\na photo of a television studio.\na photo of a podium indoor.\na photo of a conference room.\na photo of a lecture room.\na photo of a control room.correct rank: 1/397    correct probability: 90.22%SUN397\ncorrect label: guacamole\n0\n 20\n 40\n 60\n 80\n 100\na photo of guacamole, a type of food.\na photo of ceviche, a type of food.\na photo of edamame, a type of food.\na photo of tuna tartare, a type of food.\na photo of hummus, a type of food.correct rank: 1/101    correct probability: 90.15%Food101\nFigure 21. Visualization of predictions from 36 CLIP zero-shot classi\ufb01ers. All examples are random with the exception of reselecting\nHateful Memes to avoid offensive content. The predicted probability of the top 5 classes is shown along with the text used to represent\nthe class. When more than one template is used, the \ufb01rst template is shown. The ground truth label is colored green while an incorrect\nprediction is colored orange.\nLearning Transferable Visual Models From Natural Language Supervision 43Food101\nCIFAR10\nCIFAR100\nBirdsnap\nSUN397\nStanford Cars\nFGVC Aircraft\nVOC2007\nDTD\nOxford Pets\nCaltech101\nFlowers102\nMNIST\nFER2013\nSTL10\nEuroSAT\nRESISC45\nGTSRB\nKITTI\nCountry211\nPCam\nUCF101\nKinetics700\nCLEVR\nHatefulMemes\nRendered SST2\nImageNetCLIP-ResNetRN50 81.1 75.6 41.6 32.6 59.6 55.8 19.3 82.1 41.7 85.4 82.1 65.9 66.6 42.2 94.3 41.1 54.2 35.2 42.2 16.1 57.6 63.6 43.5 20.3 59.7 56.9 59.6\nRN101 83.9 81.0 49.0 37.2 59.9 62.3 19.5 82.4 43.9 86.2 85.1 65.7 59.3 45.6 96.7 33.1 58.5 38.3 33.3 16.9 55.2 62.2 46.7 28.1 61.1 64.2 62.2\nRN50x4 86.8 79.2 48.9 41.6 62.7 67.9 24.6 83.0 49.3 88.1 86.0 68.0 75.2 51.1 96.4 35.0 59.2 35.7 26.0 20.2 57.5 65.5 49.0 17.0 58.3 66.6 65.8\nRN50x16 90.5 82.2 54.2 45.9 65.0 72.3 30.3 82.9 52.8 89.7 87.6 71.9 80.0 56.0 97.8 40.3 64.4 39.6 33.9 24.0 62.5 68.7 53.4 17.6 58.9 67.6 70.5\nRN50x64 91.8 86.8 61.3 48.9 66.9 76.0 35.6 83.8 53.4 93.4 90.6 77.3 90.8 61.0 98.3 59.4 69.7 47.9 33.2 29.6 65.0 74.1 56.8 27.5 62.1 70.7 73.6CLIP-ViTB/32 84.4 91.3 65.1 37.8 63.2 59.4 21.2 83.1 44.5 87.0 87.9 66.7 51.9 47.3 97.2 49.4 60.3 32.2 39.4 17.8 58.4 64.5 47.8 24.8 57.6 59.6 63.2\nB/16 89.2 91.6 68.7 39.1 65.2 65.6 27.1 83.9 46.0 88.9 89.3 70.4 56.0 52.7 98.2 54.1 65.5 43.3 44.0 23.3 48.1 69.8 52.4 23.4 61.7 59.8 68.6\nL/14 92.9 96.2 77.9 48.3 67.7 77.3 36.1 84.1 55.3 93.5 92.6 78.7 87.2 57.5 99.3 59.9 71.6 50.3 23.1 32.7 58.8 76.2 60.3 24.3 63.3 64.0 75.3\nL/14-336px 93.8 95.7 77.5 49.5 68.4 78.8 37.2 84.3 55.7 93.5 92.8 78.3 88.3 57.7 99.4 59.6 71.7 52.3 21.9 34.9 63.0 76.9 61.3 24.8 63.3 67.9 76.2\nTable 11. Zero-shot performance of CLIP models over 27 datasets.\n10110275808590accuracy\nFood101\n1011027580859095accuracy\nCIFAR10\n10110240506070accuracy\nCIFAR100\n1011023540455055accuracy\nBirdsnap\n1011026062646668accuracy\nSUN397\n10110250607080accuracy\nStanfordCars\n101102203040mean per class\nFGVCAircraft\n10110282.082.583.083.584.084.511-point mAP over 20 classes\nPascalVOC2007\n101102506070accuracy\nDescribableTextures\n10110286889092mean per class\nOxfordPets\n101102828486889092mean-per-class\nCaltech101\n101102657075808590mean per class\nFlowers102\n1011025060708090100accuracy\nMNIST\n10110245505560accuracy\nFacialEmotionRecognition2013\n1011029596979899accuracy\nSTL10\n101102406080100accuracy\nEuroSAT\n101102607080accuracy\nRESISC45\n10110240506070accuracy\nGTSRB\n101102203040506070accuracy\nKITTI\n10110250607080accuracy\nPatchCamelyon\n101102657075accuracy\nUCF101\n10110245505560mean(top1, top5)\nKinetics700\n1011022025303540accuracy\nCLEVRCounts\n1011021520253035accuracy\nCountry211\n101102\nGFLOPs/image5456586062ROCAUC\nHatefulMemes\n101102\nGFLOPs/image55606570accuracy\nSST2\n101102\nGFLOPs/image60657075accuracy\nImageNet\nCLIP-ViT\nCLIP-ResNet\nResNet\nFigure 22. CLIP\u2019s zero-shot performance compared to linear-probe ResNet performance\nLearning Transferable Visual Models From Natural Language Supervision 44\nB. Zero-Shot Prediction\nTo provide a qualitative summary / overview of CLIP\u2019s zero-\nshot performance we visualize a randomly selected predic-\ntion for 36 different zero-shot CLIP classi\ufb01ers in Figure\n21. In addition, Table 11 and Figure 22 show the individual\nzero-shot performance scores for each dataset.\nC. Duplicate Detector\nOur early attempts at duplicate detection and analysis used\nnearest neighbors in the model\u2019s learned embedding space.\nWhile it is intuitive to use a model\u2019s own notion of similar-\nity, we encountered issues. We found the model\u2019s feature\nspace is weighted very heavily towards semantic similar-\nity. Many false positives occurred due to distinct objects\nthat would be described similarly (soccer balls, \ufb02owers of\nthe same species, etc...) having almost perfect similarity.\nWe also observed the model was quite poor at assigning\ncertain kinds of near-duplicates high similarity scores. We\nnoticed repeatedly that images with high-frequency textures\n(such as fur or stripe patterns) pre-processed by different\nresizing algorithms (nearest neighbor vs bi-linear) could\nhave surprisingly low similarity. This resulted in many false\nnegatives.\nWe built our own near-duplicate detector to \ufb01x this issue.\nWe created a synthetic data augmentation pipeline that com-\nbined a variety of common image manipulations. The aug-\nmentation pipeline combines random cropping and zooming,\naspect ratio distortion, downsizing and upscaling to different\nresolutions, minor rotations, jpeg compression, and HSV\ncolor jitter. The pipeline also randomly selects from differ-\nent interpolation algorithms for all relevant steps. We then\ntrained a model to maximize the similarity of an image and\nits transformed variant while minimizing similarity to all\nother images in a training batch. We used the same n-pair /\nInfoNCE loss as CLIP but with a \ufb01xed temperature of 0.07.\nWe selected a ResNet-50 as the model architecture. We\nmodi\ufb01ed the base ResNet-50 with the anti-alias improve-\nments from (Zhang, 2019) and used weight norm (Sali-\nmans & Kingma, 2016) instead of batch norm (Ioffe &\nSzegedy, 2015) to avoid leaking information about dupli-\ncates via batch statistics - a problem previously noted in\n(Henaff, 2020). We also found the GELU activation func-\ntion (Hendrycks & Gimpel, 2016) to perform better for this\ntask. We trained the model with a total batch size of 1,712\nfor approximately 30 million images sampled from our pre-\ntraining dataset. At the end of training it achieves nearly\n100% accuracy on its proxy training task.Linear Classi\ufb01er Zero Shot\nDataset YFCC WIT \u0001 YFCC WIT \u0001\nBirdsnap 47.4 35.3 +12.1 19.9 4.5 +15.4\nCountry211 23.1 17.3 +5.8 5.2 5.3 +0.1\nFlowers102 94.4 89.8 +4.6 48.6 21.7 +26.9\nGTSRB 66.8 72.5 \u00005.7 6.9 7.0 \u00000.1\nUCF101 69.2 74.9 \u00005.7 22.9 32.0 \u00009.1\nStanford Cars 31.4 50.3 \u000018.9 3.8 10.9 \u00007.1\nImageNet 62.0 60.8 +1:2 31.3 27.6 +3.7\nDataset Average 65.5 66.6 \u00001.1 29.6 30.0 \u00000.4\nDataset \u201cWins\u201d 10 15 \u00005 19 18 +1\nTable 12. CLIP performs similarly when trained on only\nYFCC100M. Comparing a ResNet-50 trained on only\nYFCC100M with a same sized subset of WIT shows simi-\nlar average performance and number of wins on zero shot and\nlinear classi\ufb01er evals. However, large differences in dataset\nspeci\ufb01c performance occur. We include performance on the 3\ndatasets where YFCC does best and worst compared to WIT\naccording to a linear probe in order to highlight this as well as\naggregate performance across all linear and zero-shot evals and\nthe canonical ImageNet dataset.\nD. Dataset Ablation on YFCC100M\nTo study whether our custom dataset is critical to the perfor-\nmance of CLIP, we trained a model on a \ufb01ltered subset of\nthe YFCC100M dataset (details described in Section 2.2)\nand compared its performance to the same model trained\non an equally sized subset of WIT. We train each model for\n32 epochs at which point transfer performance begins to\nplateau due to over\ufb01tting. Results are shown in Table 12.\nAcross our whole eval suite, YFCC and WIT perform simi-\nlarly on average for both zero-shot and linear probe settings.\nHowever, performance on speci\ufb01c \ufb01ne-grained classi\ufb01ca-\ntion datasets can vary widely - sometimes by over 10%.\nOur speculation is that these differences in performance re-\n\ufb02ect the relative density of relevant data in each pre-training\ndataset. For instance, pre-training on YFCC100M, which\nmight contain many photos of birds and \ufb02owers (common\nsubjects for photographers), results in better performance on\nBirdsnap and Flowers102, while pre-training on WIT results\nin better car and pet classi\ufb01ers (which appear common in\nour dataset).\nOverall, these results are encouraging as they suggest our\napproach can use any reasonably \ufb01ltered collection of paired\n(text, image) data. This mirrors recent work which reported\npositive results using the same contrastive pre-training ob-\njective on the relatively different domain of medical imaging\n(Zhang et al., 2020). It also is similar to the \ufb01ndings of noisy\nstudent self-training which reported only slight improve-\nments when using their JFT300M dataset over YFCC100M\n(Xie et al., 2020). We suspect the major advantage of our\ndataset over the already existing YFCC100M is its much\nlarger size.\nLearning Transferable Visual Models From Natural Language Supervision 45\nFinally, we caution that WIT includes this \ufb01ltered subset\nof YFCC100M. This could result in our ablation under-\nestimating the size of performance differences between\nYFCC100M and the rest of WIT. We do not think this is\nlikely as YFCC100M is only 3.7% of the overall WIT data\nblend and it did not noticeably change the performance of\nmodels when it was added to the existing data blend during\nthe creation of WIT.\nE. Selected Task and Dataset Results\nDue to the large variety of datasets and experiments consid-\nered in this work, the main body focuses on summarizing\nand analyzing overall results. In the following subsections\nwe report details of performance for speci\ufb01c groups of tasks,\ndatasets, and evaluation settings.\nE.1. Image and Text Retrieval\nCLIP pre-trains for the task of image-text retrieval on our\nnoisy web-scale dataset. Although the focus of this paper\nis on representation learning and task learning for the pur-\npose of transfer to a wide variety of downstream datasets,\nvalidating that CLIP is able to achieve high transfer perfor-\nmance transfer on exactly what it is pre-trained for is an\nimportant sanity check / proof of concept. In Table 13 we\ncheck the zero-shot transfer performance of CLIP for both\ntext and image retrieval on the Flickr30k and MSCOCO\ndatsets. Zero-shot CLIP matches or outperforms all prior\nzero-shot results on these two datasets. Zero-shot CLIP is\nalso competitive with the current overall SOTA for the task\nof text retrieval on Flickr30k. On image retrieval, CLIP\u2019s\nperformance relative to the overall state of the art is notice-\nably lower. However, zero-shot CLIP is still competitive\nwith a \ufb01ne-tuned Unicoder-VL. On the larger MS-COCO\ndataset \ufb01ne-tuning improves performance signi\ufb01cantly and\nzero-shot CLIP is not competitive with the most recent work.\nFor both these datasets we prepend the prompt \u201c a photo\nof\u201d to the description of each image which we found boosts\nCLIP\u2019s zero-shot R@1 performance between 1 and 2 points.\nE.2. Optical Character Recognition\nAlthough visualizations have shown that ImageNet models\ncontain features that respond to the presence of text in an\nimage (Zeiler & Fergus, 2014), these representations are\nnot suf\ufb01ciently \ufb01ne-grained to use for the task of optical\ncharacter recognition (OCR). To compensate, models are\naugmented with the outputs of custom OCR engines and\nfeatures to boost performance on tasks where this capability\nis required (Singh et al., 2019; Yang et al., 2020). Early dur-\ning the development of CLIP, we noticed that CLIP began to\nlearn primitive OCR capabilities which appeared to steadily\nimprove over the course of the project. To evaluate this\nqualitatively noticed behavior, we measured performanceon 5 datasets requiring the direct and indirect use of OCR.\nThree of these datasets MNIST (LeCun), SVHN (Netzer\net al., 2011), and IIIT5K (Mishra et al., 2012) directly check\nthe ability of a model to perform low-level character and\nword recognition, while Hateful Memes (Kiela et al., 2020)\nand SST-2 (Socher et al., 2013) check the ability of a model\nto use OCR to perform a semantic task. Results are reported\nin Table 14.\nCLIP\u2019s performance is still highly variable and appears to\nbe sensitive to some combination of the domain (rendered or\nnatural images) and the type of text to be recognized (num-\nbers or words). CLIP\u2019s OCR performance is strongest Hate-\nful Memes and SST-2 - datasets where the text is digitally\nrendered and consists mostly of words. On IIIT5K, which\nis natural images of individually cropped words, zero-shot\nCLIP performs a bit more respectively and its performance\nis similar to Jaderberg et al. (2014) early work combining\ndeep learning and structured prediction to perform open-\nvocabulary OCR. However, performance is noticeably lower\non two datasets involving recognition of hand written and\nstreet view numbers. CLIP\u2019s 51% accuracy on full number\nSVHN is well below any published results. Inspection sug-\ngests CLIP struggles with repeated characters as well as the\nlow resolution and blurry images of SVHN. CLIP\u2019s zero-\nshot MNIST performance is also poor and is outperformed\nby supervised logistic regression on raw pixels, one of the\nsimplest possible machine learning baselines.\nSST-2 is a sentence level NLP dataset which we render into\nimages. We include SST-2 in order to check whether CLIP\nis able to convert low level OCR capability into a higher\nlevel representation. Fitting a linear classi\ufb01er on CLIP\u2019s rep-\nresentation of rendered sentences achives 80.5% accuracy.\nThis is on par with the 80% accuracy of a continuous bag\nof words baseline using GloVe word vectors pre-trained on\n840 billion tokens (Pennington et al., 2014). While this is a\nsimple NLP baseline by today\u2019s standard, and well below\nthe 97.5% of the current SOTA, it is encouraging to see\nthat CLIP is able to turn an image of rendered text into a\nnon-trivial sentence level representation. Fully supervised\nCLIP is also surprisingly strong on Hateful Meme detec-\ntion, where CLIP is only 0.7 points behind the current single\nmodel SOTA and several points above the best baseline from\nthe original paper. Similar to SST-2, these other results on\nHateful Memes use the ground truth text which CLIP does\nnot have access to. Finally, we note that zero-shot CLIP\noutperforms the best results using fully supervised linear\nprobes across all other 56 models included in our evaluation\nsuite. This suggests CLIP\u2019s OCR capability is at least some-\nwhat unique compared to existing work on self-supervised\nand supervised representation learning.\nLearning Transferable Visual Models From Natural Language Supervision 46\nText Retrieval Image Retrieval\nFlickr30k MSCOCO Flickr30k MSCOCO\nR@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10FinetuneUnicoder-VLa86.2 96.3 99.0 62.3 87.1 92.8 71.5 90.9 94.9 46.7 76.0 85.3\nUniterb87.3 98.0 99.2 65.7 88.6 93.8 75.6 94.1 96.8 52.9 79.9 88.0\nVILLAc87.9 97.5 98.8 - - - 76.3 94.2 96.8 - - -\nOscard- - - 73.5 92.2 96.0 - - - 57.5 82.8 89.8\nERNIE-ViLe88.7 98.0 99.2 - - - 76.7 93.6 96.4 - - -Zero-ShotVisual N-Gramsf15.4 35.7 45.1 8.7 23.1 33.3 8.8 21.2 29.9 5.0 14.5 21.9\nImageBERTg- - - 44.0 71.2 80.4 - - - 32.3 59.0 70.2\nUnicoder-VLa64.3 86.8 92.3 - - - 48.4 76.0 85.2 - - -\nUniterb83.6 95.7 97.7 - - - 68.7 89.2 93.9 - - -\nCLIP 88.0 98.7 99.4 58.4 81.5 88.1 68.7 90.6 95.2 37.8 62.4 72.2\nTable 13. CLIP improves zero-shot retrieval and is competitive with the best \ufb01ne-tuned result on Flickr30k text retrieval. Bold\nindicates best overall performance while an underline indicates best in category performance (zero-shot or \ufb01ne-tuned). For all other\nmodels, best results from the paper are reported regardless of model size / variant. MSCOCO performance is reported on the 5k test set.\na(Li et al., 2020a)b(Chen et al., 2019)c(Gan et al., 2020)d(Li et al., 2020b)e(Yu et al., 2020)f(Li et al., 2017)g(Qi et al., 2020)\nIIIT5K Hateful\nMNIST SVHN 1k Memes SST-2FinetuneSOTA 99.8a96.4b98.9c78.0d97.5e\nJOINTf- - 89.6 - -\nCBoWg- - - - 80.0LinearRaw Pixels 92.5 - - - -\nES Best 98.9h- - 58.6h59.0i\nCLIP 99.2 - - 77.3 80.5ZSCLIP 88.4 51.0 90.0 63.3 67.9\nTable 14. OCR performance on 5 datasets. All metrics are accuracy\non the test set except for Hateful Memes which reports ROC AUC\non the dev set. Single model SOTA reported to best of knowledge.\nES Best reports the best performance across the 56 non-CLIP\nmodels in our evaluation suite.a(Assiri, 2020)b(Jaderberg et al.,\n2015)c(Wang et al., 2020)d(Lippe et al., 2020)f(Jaderberg et al.,\n2014)g(Wang et al., 2018)h(Xie et al., 2020)i(Mahajan et al.,\n2018)\nE.3. Action Recognition in Videos\nFor the purpose of learning, a potentially important aspect\nof natural language is its ability to express, and therefore su-\npervise, an extremely wide set of concepts. A CLIP model,\nsince it is trained to pair semi-arbitrary text with images, is\nlikely to receive supervision for a wide range of visual con-\ncepts involving both common and proper nouns, verbs, and\nadjectives. ImageNet-1K, by contrast, only labels common\nnouns. Does the lack of broader supervision in ImageNet\nresult in weaker transfer of ImageNet models to tasks involv-\ning the recognition of visual concepts that are not nouns?\nTo investigate this, we measure and compare the perfor-\nmance of CLIP and ImageNet models on several videoUCF101 K700 RareAct\nTop-1 A VG mWAP mWSAPFinetuneR(2+1)D-BERTa98.7 - - -\nNS ENet-L2b- 84.8 - -\nHT100M S3Dd91.3 - - -\nBaseline I3De- 70.2 - -LinearMMV FACf91.8 - - -\nNS ENet-L2c89.4c68.2c- -\nCLIP 92.0 73.0 - -ZSHT100M S3Dd- - 30.5 34.8\nCLIP 80.3 69.6 40.7 44.8\nTable 15. Action recognition performance on 3 video datasets. Sin-\ngle model SOTA reported to best of knowledge. Note that linear\nCLIP andlinear NS ENet-L2 are trained and evaluated on a single\nframe subsampled version of each dataset and not directly compa-\nrable to prior work. On Kinetics-700, we report the ActivityNet\ncompetition metric which is the average of top-1 and top-5 per-\nformance.a(Kalfaoglu et al., 2020)b(Lu et al., 2020)c(Xie et al.,\n2020)d(Miech et al., 2020b)e(Carreira et al., 2019)f(Alayrac\net al., 2020)\naction classi\ufb01cation datasets which measure the ability of a\nmodel to recognize verbs. In Table 15 we report results on\nUCF-101 (Soomro et al., 2012) and Kinetics-700 (Carreira\net al., 2019), two common datasets for the task. Unfortu-\nnately, our CPU based linear classi\ufb01er takes a prohibitively\nlong time to evaluate on a video dataset due to the very large\nnumber of training frames. To deal with this, we aggres-\nsively sub-sample each video to only a single center frame,\neffectively turning it into an image classi\ufb01cation dataset.\nAs a result, our reported performance in a linear evaluation\nsetting likely under estimates performance by a moderate\namount.\nLearning Transferable Visual Models From Natural Language Supervision 47\nIN IN-V2 IN-A IN-R ObjectNet IN-Sketch IN-Vid YTBB\nTop-1 Top-1 Top-1 Top-1 Top-1 Top-1 PM0 PM10 PM0 PM10\nNS Ef\ufb01cientNet-L2a88.3 80.2 84.9 74.7 68.5 47.6 88.0 82.1 67.7 63.5\nFixResNeXt101-32x48d V2b86.4 78.0 68.4 80.0 57.8 59.1 85.8 72.2 68.9 57.7\nLinear Probe CLIP 85.4 75.9 75.3 84.2 66.2 57.4 89.1 77.2 68.7 63.1\nZero-Shot CLIP 76.2 70.1 77.2 88.9 72.3 60.2 95.3 89.2 95.2 88.5\nTable 16. Detailed ImageNet robustness performance. IN is used to abbreviate for ImageNet.a(Xie et al., 2020)b(Touvron et al., 2019)\nDespite this handicap, CLIP features transfer surprisingly\nwell to this task. CLIP matches the best prior result on UCF-\n101 in a linear probe evaluation setting and also outperforms\nall other models in our evaluation suite. On Kinetics-700,\nCLIP also outperforms the \ufb01ne-tuned I3D baseline from the\noriginal paper. Since it does not require a training stage,\nwe report CLIP\u2019s zero-shot performance when averaging\npredictions across all frames. CLIP also performs well in\nthis setting and on Kinetics-700 its performance is within\n1% of the fully supervised I3D baseline which is trained\non 545000 labeled videos. Encouraged by these results, we\nalso measure CLIP\u2019s performance on the recently introduced\nRareAct dataset (Miech et al., 2020a) which was designed\nto measure zero-shot recognition of unusual actions like\n\u201chammering a phone\u201d and \u201cdrilling an egg\u201d. CLIP improves\nover the prior state of the art, a S3D model trained on auto-\nmatically extracted captions from 100 million instructional\nvideos, by 10 points.\nWhile CLIP has encouragingly strong performance on the\ntask of action recognition, we note that there are many differ-\nences between the models being compared beyond just their\nform of supervision such as model architecture, training\ndata distribution, dataset size, and compute used. Further\nwork is needed to more precisely determine what speci\ufb01c\ndesign decisions contribute to achieving high performance\non this task.\n1km 25km 200km 750km 2500km\nISNsa16.9 43.0 51.9 66.7 80.2\nCPlaNetb16.5 37.1 46.4 62.0 78.5\nCLIP 13.9 32.9 43.0 62.0 79.3\nDeep-Ret+c14.4 33.3 47.7 61.6 73.4\nPlaNetd8.4 24.5 37.6 53.6 71.3\nTable 17. Geolocalization performance on the IM2GPS test set.\nMetric is percent of images localized within a given radius. Models\nare ordered by average performance.a(Muller-Budack et al., 2018)\nb(Hongsuck Seo et al., 2018)c(V o et al., 2017)c(Weyand et al.,\n2016)E.4. Geolocalization\nAnother behavior we noticed during the development of\nCLIP was its ability to recognize many places and locations.\nTo quantify this we created the Country211 dataset as de-\nscribed in Appendix A and report results on it throughout\nthe paper. However it is a new benchmark so to compare\nwith prior work on geolocalization we also report results\non the IM2GPS test set from Hays & Efros (2008) in Table\n17. Since IM2GPS is a regression benchmark, we guess the\nGPS coordinates of the nearest image in a set of reference\nimages using CLIP\u2019s embedding space. This is not a zero-\nshot result since it uses nearest-neighbor regression. Despite\nquerying only 1 million images, which is much less than\nprior work, CLIP performs similarly to several task speci\ufb01c\nmodels. It is not, however, competitive with the current state\nof the art.\nE.5. Robustness to Distribution Shift\nSection 3.3 provides a high level summary and analysis of\nImageNet-related robustness results. We brie\ufb02y provide\nsome additional numerical details in this appendix. Per-\nformance results per dataset are provided in Table 16 and\ncompared with the current state of the art results reported\nin Taori et al. (2020)\u2019s evaluation suite. Zero-shot CLIP im-\nproves the state of the art on 5 of the 7 datasets, ImageNet-R,\nObjectNet, ImageNet-Sketch, ImageNet-Vid, and Youtube-\nBB. CLIP\u2019s improvements are largest on ImageNet-Vid and\nYoutube-BB due to its \ufb02exible zero-shot capability and on\nImageNet-R, which likely re\ufb02ects CLIP\u2019s pre-training dis-\ntribution including signi\ufb01cant amounts of creative content.\nA similar behavior has been documented for the Instagram\npre-trained ResNeXt models as discussed in Taori et al.\n(2020).\nLearning Transferable Visual Models From Natural Language Supervision 48\nF. Model Hyperparameters\nHyperparameter Value\nBatch size 32768\nV ocabulary size 49408\nTraining epochs 32\nMaximum temperature 100.0\nWeight decay 0.2\nWarm-up iterations 2000\nAdam\f1 0.9\nAdam\f2 0.999 (ResNet), 0.98 (ViT)\nAdam\u000f 10\u00008(ResNet), 10\u00006(ViT)\nTable 18. Common CLIP hyperparameters\nLearning Embedding Input ResNet Text Transformer\nModel rate dimension resolution blocks width layers width heads\nRN50 5\u000210\u000041024 224 (3, 4, 6, 3) 2048 12 512 8\nRN101 5\u000210\u00004512 224 (3, 4, 23, 3) 2048 12 512 8\nRN50x4 5\u000210\u00004640 288 (4, 6, 10, 6) 2560 12 640 10\nRN50x16 4\u000210\u00004768 384 (6, 8, 18, 8) 3072 12 768 12\nRN50x64 3:6\u000210\u000041024 448 (3, 15, 36, 10) 4096 12 1024 16\nTable 19. CLIP-ResNet hyperparameters\nLearning Embedding Input Vision Transformer Text Transformer\nModel rate dimension resolution layers width heads layers width heads\nViT-B/32 5\u000210\u00004512 224 12 768 12 12 512 8\nViT-B/16 5\u000210\u00004512 224 12 768 12 12 512 8\nViT-L/14 4\u000210\u00004768 224 24 1024 16 12 768 12\nViT-L/14-336px 2\u000210\u00005768 336 24 1024 16 12 768 12\nTable 20. CLIP-ViT hyperparameters",
      "summary": "This paper introduces a method for training visual models using natural language supervision by pairing images with their corresponding captions, enabling the model to learn from large-scale, uncurated data. The resulting model, CLIP, demonstrates strong zero-shot transfer capabilities across a wide range of vision tasks without the need for task-specific training.",
      "key_contributions": [
        "Proposes a scalable approach for learning visual representations from 400 million image-text pairs collected from the internet.",
        "Demonstrates that natural language supervision enables zero-shot transfer to diverse vision tasks.",
        "Shows competitive or superior performance to fully supervised models on over 30 benchmarks without using any task-specific labeled data.",
        "Releases code and pre-trained models to facilitate further research."
      ],
      "methodology": "The model is pre-trained to predict which caption matches which image using contrastive learning on a massive dataset of image-text pairs, aligning visual and textual representations in a shared embedding space.",
      "results": "CLIP achieves competitive performance with supervised baselines, matching ResNet-50's accuracy on ImageNet in a zero-shot setting and transferring effectively to a wide variety of vision tasks.",
      "keywords": [
        "vision-language models",
        "zero-shot learning",
        "contrastive learning",
        "image-text pairs",
        "transfer learning",
        "representation learning",
        "CLIP"
      ],
      "status": "new",
      "notes": null,
      "created_at": "2025-11-23T20:12:49.690618",
      "needs_llm_processing": false
    },
    "2": {
      "id": "2511.16674v1",
      "title": "Dataset Distillation for Pre-Trained Self-Supervised Vision Models",
      "authors": [
        {
          "name": "George Cazenavette",
          "affiliation": null
        },
        {
          "name": "Antonio Torralba",
          "affiliation": null
        },
        {
          "name": "Vincent Sitzmann",
          "affiliation": null
        }
      ],
      "abstract": "The task of dataset distillation aims to find a small set of synthetic images such that training a model on them reproduces the performance of the same model trained on a much larger dataset of real samples. Existing distillation methods focus on synthesizing datasets that enable training randomly initialized models. In contrast, state-of-the-art vision approaches are increasingly building on large, pre-trained self-supervised models rather than training from scratch. In this paper, we investigate the problem of distilling datasets that enable us to optimally train linear probes on top of such large, pre-trained vision models. We introduce a method of dataset distillation for this task called Linear Gradient Matching that optimizes the synthetic images such that, when passed through a pre-trained feature extractor, they induce gradients in the linear classifier similar to those produced by the real data. Our method yields synthetic data that outperform all real-image baselines and, remarkably, generalize across pre-trained vision models, enabling us, for instance, to train a linear CLIP probe that performs competitively using a dataset distilled via a DINO backbone. Further, we show that our distilled datasets are exceptionally effective for fine-grained classification and provide a valuable tool for model interpretability, predicting, among other things, how similar two models' embedding spaces are under the platonic representation hypothesis or whether a model is sensitive to spurious correlations in adversarial datasets.",
      "arxiv_id": "2511.16674v1",
      "arxiv_url": "http://arxiv.org/abs/2511.16674v1",
      "pdf_url": "https://arxiv.org/pdf/2511.16674v1",
      "published": "2025-11-20T18:59:57Z",
      "updated": "2025-11-20T18:59:57Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "full_text": "Dataset Distillation for Pre-Trained\nSelf-Supervised Vision Models\nGeorge Cazenavette Antonio Torralba Vincent Sitzmann\nMassachusetts Institute of Technology\ngeorgecazenavette.github.io/linear-gm\nAbstract\nThe task ofdataset distillationaims to find a small set of synthetic images such\nthat training a model on them reproduces the performance of the same model\ntrained on a much larger dataset of real samples. Existing distillation methods\nfocus on synthesizing datasets that enable trainingrandomly initializedmodels.\nIn contrast, state-of-the-art vision approaches are increasingly building on large,\npre-trained self-supervised models rather than training from scratch. In this paper,\nwe investigate the problem of distilling datasets that enable us to optimally train\nlinear probeson top of such large, pre-trained vision models. We introduce a\nmethod of dataset distillation for this task calledLinear Gradient Matchingthat\noptimizes the synthetic images such that, when passed through a pre-trained feature\nextractor, they induce gradients in the linear classifier similar to those produced\nby the real data. Our method yields synthetic data that outperform all real-image\nbaselines and, remarkably, generalize across pre-trained vision models, enabling\nus, for instance, to train a linear CLIP probe that performs competitively using a\ndataset distilled via a DINO backbone. Further, we show that our distilled datasets\nare exceptionally effective for fine-grained classification and provide a valuable\ntool for model interpretability, predicting, among other things, how similar two\nmodels\u2019 embedding spaces are under the platonic representation hypothesis or\nwhether a model is sensitive to spurious correlations in adversarial datasets.\n1 Introduction\nThe task ofDataset Distillationinvolves the synthesis of a small set of synthetic samples such that a\nmodel trainedfrom scratchon this synthetic set will achieve test-time performance comparable to\nthat of a model trained on the full real dataset. Since this problem\u2019s first introduction and proposed\nsolution in the self-titled paper [ 47], many new methods [ 6,27,51,54\u201356] and extensions thereof [ 7,\n11,16,25,28,40,46,53] have made strides towards the lofty goal of learning a high-quality model\nfrom just a handful of synthetic images.\nMeanwhile, computer vision has increasingly adopted a paradigm of using the representations of large,\npre-trained self-supervised vision models for downstream tasks, either via fine-tuning or by using these\nmodels as feature extraction backbones. Given this trend, in this work, we explore dataset distillation\nin the regime of training modelson top offeatures extracted by pre-trained vision foundation models.\nSpecifically, we studylinear classificationon top of apre-trained feature representation.\nIn our new method,Linear Gradient Matching, we distill synthetic datasets by optimizing such\nthat their representations extracted by pre-trained feature extractors inducegradientsin a linear\nclassifier similar to those obtained from real images. We find that a single synthetic image per class\nsuffices to train linear classifiers to competitive performance across a wide variety of large vision\n39th Conference on Neural Information Processing Systems (NeurIPS 2025).arXiv:2511.16674v1  [cs.CV]  20 Nov 2025\n/uni00000033/uni0000004c/uni0000005d/uni0000005d/uni00000044/uni00000026/uni0000002f/uni0000002c/uni00000033\n/uni00000039 /uni00000052/uni0000004f/uni00000046/uni00000044/uni00000051/uni00000052\n /uni0000002f/uni00000052/uni00000055/uni0000004c/uni0000004e/uni00000048/uni00000048/uni00000057\n /uni00000030/uni00000052/uni00000051/uni00000044/uni00000055/uni00000046/uni0000004b\n /uni0000002d/uni00000044/uni00000046/uni0000004e/uni00000010/uni00000032/uni0000000a/uni00000010/uni0000002f/uni00000044/uni00000051/uni00000057/uni00000048/uni00000055/uni00000051\n /uni00000026/uni00000044/uni00000055/uni00000045/uni00000052/uni00000051/uni00000044/uni00000055/uni00000044\n /uni0000002d/uni00000044/uni0000005c\n /uni0000002d/uni00000048/uni0000004f/uni0000004f/uni0000005c/uni00000049/uni0000004c/uni00000056/uni0000004b\n/uni00000036/uni00000046/uni00000052/uni00000057/uni00000046/uni0000004b/uni00000003/uni00000037/uni00000048/uni00000055/uni00000055/uni0000004c/uni00000048/uni00000055/uni00000027/uni0000002c/uni00000031/uni00000032/uni00000010/uni00000059/uni00000015\n/uni00000025/uni00000048/uni00000044/uni00000059/uni00000048/uni00000055\n /uni00000037/uni0000004c/uni0000004a/uni00000048/uni00000055\n /uni0000002f/uni00000044/uni00000047/uni0000005c/uni00000045/uni00000058/uni0000004a\n /uni00000026/uni00000048/uni0000004f/uni0000004f/uni00000052\n /uni00000026/uni0000004b/uni00000048/uni00000048/uni00000056/uni00000048/uni00000045/uni00000058/uni00000055/uni0000004a/uni00000048/uni00000055\n /uni00000025/uni00000044/uni00000051/uni00000044/uni00000051/uni00000044\n /uni00000037/uni0000004c/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni0000003a/uni00000052/uni0000004f/uni00000049\n/uni00000037/uni00000055/uni00000048/uni00000048/uni00000003/uni00000029/uni00000055/uni00000052/uni0000004a/uni00000028/uni00000039 /uni00000024/uni00000010/uni00000013/uni00000015\n/uni0000002e/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000033/uni00000048/uni00000051/uni0000004a/uni00000058/uni0000004c/uni00000051\n /uni00000026/uni00000044/uni00000045\n /uni00000029/uni0000004f/uni00000044/uni00000050/uni0000004c/uni00000051/uni0000004a/uni00000052\n /uni00000025/uni00000048/uni00000048\n /uni00000025/uni00000044/uni0000004f/uni0000004f/uni00000052/uni00000052/uni00000051\n /uni00000025/uni00000052/uni00000052/uni0000004e/uni00000046/uni00000044/uni00000056/uni00000048\n /uni00000036/uni00000057/uni00000055/uni00000044/uni0000005a/uni00000045/uni00000048/uni00000055/uni00000055/uni0000005c\n/uni00000026/uni00000044/uni00000056/uni00000057/uni0000004f/uni00000048/uni00000030/uni00000052/uni00000026/uni00000052/uni00000010/uni00000059/uni00000016\n/uni00000029/uni00000055/uni00000048/uni00000051/uni00000046/uni0000004b/uni00000003/uni0000002b/uni00000052/uni00000055/uni00000051\n /uni00000026/uni0000004b/uni00000058/uni00000055/uni00000046/uni0000004b\n /uni00000025/uni0000004f/uni00000058/uni00000048/uni00000003/uni0000002d/uni00000048/uni00000044/uni00000051/uni00000056\n /uni0000002e/uni0000004c/uni00000050/uni00000052/uni00000051/uni00000052\n /uni00000028/uni00000056/uni00000053/uni00000055/uni00000048/uni00000056/uni00000056/uni00000052\n /uni00000036/uni00000046/uni0000004b/uni00000052/uni00000052/uni0000004f/uni00000003/uni00000025/uni00000058/uni00000056\n /uni00000026/uni00000044/uni00000055/uni00000047/uni00000052/uni00000052/uni00000051Figure 1:ImageNet-1k Distilled for Self-Supervised Models:Using our method ofLinear Gradient Matching,\nwe distill vision datasets to just one synthetic image per class using different pre-trained self-supervised backbone\nmodels. These learned images can then be used to train linear probes that achieve high accuracy on unseen test\ndata, outperforming all real-image baselines. Furthermore, each backbone model seems to yield its own \u201cstyle\u201d of\ndistilled image, giving insights into the aspects on which these models tend to focus (structure, texture, color, etc.).\nmodel backbones, outperforming all real-image baselines. Figure 1 shows samples distilled from\nImageNet-1k [12] with our method using various self-supervised feature extractors.\nMotivated by recent hypotheses that different large models converge to similar representations\neven when trained on different modalities [ 20], we investigate whether distilled datasets transfer\nacross architectures. We find that a gradient matching objective alone leads to images that are\noverfit to a particular model architecture and do not yield competitive performance across founda-\ntion models. However, we overcome this issue through differentiable augmentations and a simple\nre-parameterization of images via a multi-scale pyramid. Compared to those retrieved via na\u00efve\npixel optimization, the resulting distilled images not only look remarkably realistic but also readily\ntransfer across foundation models, such that a dataset distilled using, for example, a DINO backbone\nyields competitive performance when used to train a linear classifier on top of a different model\u2019s\nrepresentation, such as CLIP\u2019s.\nWe also observe that our distilled datasets offer several interesting interpretability results, including\npredicting alignment between different models, explaining susceptibility (or robustness) to spurious\ncorrelations in adversarial datasets, and highlighting out-of-distribution capabilities.\nExtensive experiments and ablations validate ourLinear Gradient Matchingmethod\u2019s effectiveness\non this new dataset distillation task and highlight its potential as an interpretability tool.\n2 Related Work\nDataset Distillation.As dataset and model sizes continue to grow, so has the interest in more\nefficient forms of learning. To this end, researchers have worked towards methods oflearning\noptimal training data such that one could train an effective model from scratch using as few\nsamples as possible. One such solution to this problem was the initial proposal ofDataset\nDistillation[ 47] in which the model\u2019s final performance was expressed as a function of thesynthetic\ntraining datathat was optimized end-to-end by back-propagating through many inner training\niterations. Follow-up works introduced proxy losses and learned synthetic images that matched\ngradients [ 55], feature distributions [ 54], training trajectories [ 6] and more [ 27,51,56]. Some\nworks extend dataset distillation to large models [ 50,51], but these methods do not excel in the\nultra-small data regime, i.e., one image per class. For such settings, Trajectory Matching [ 6] and\n2\n                                        \n                                     \n                                                   \n                                                  \n                                                                            \n                                                                           Cosine\nDistance\n                                        \n                                     \nReal Images\nSynthetic ImagesForwardGradientMeta -Gradient\nFigure 2:Linear Gradient Matching for Pre-Trained Vision Models:Given a pre-trained self-supervised\nvision model ( \u03d5), we perform a distillation step by first passing a batch of real and synthetic data through \u03d5\nand a randomly-initialized linear classifier ( W) to get the real and synthetic classification losses ( \u2113realand\u2113syn).\nOur meta loss ( Lmeta) is then defined as the cosine distance between thegradientsof these classification losses\n(\u2113realand\u2113 syn)with respect to the random linear probe ( W). This meta loss is then back-propagated through the ini-\ntial synthetic gradient calculation and used to update our synthetic images. This technique allows us to distill large\ndatasets to just a single image per class while still achieving high performance when training new linear probes.\nits modifications [ 7,11,16,32,40] still reign supreme. However, this method fails to scale up to\nlarge models due to memory constraints and instability in the bi-level optimization.\nThis work introduces a new problem in the space of dataset distillation: learning synthetic images\nfor the purpose of traininglinear probeson top ofpre-trained self-supervised feature extractors\ninstead of training randomly initialized models from scratch. Our proposed solution,Linear Gradient\nMatching, takes inspiration from prior work on gradient matching [ 55] and trajectory matching [ 6]\nbut only considers gradients of the linear classifier as opposed to the entire model.\nSelf-Supervised Learning.Given that the overwhelming majority of available visual data lacks\nany useful labels,Self-Supervised Learninghas become the defacto method of pre-training neural\nnetworks to be later used for down-stream tasks. In recent years, several different paradigms of\nself-supervised training have emerged, including contrastive learners [ 5,8\u201310,18,31], masked\nauto-encoders [ 17,37], vision-language models [ 34,41,44,52], and hybrid approaches [ 13,14].\nDespite the various training methods, researchers have noticed that these different models tend\nto learn similar representations, even across different modalities, and dubbed this observation the\n\u201cPlatonic Representation Hypothesis\u201d [20].\nIn this work, we focus on distilling datasets using four pre-trained self-supervised models in\nparticular: CLIP [ 34], DINO-v2 [ 31], EV A-02 [ 13], and MoCo-v3 [ 10]. Since our feature extractors\nwere pre-trained in a purely self-supervised manner, our linear probes still only see a singlelabeled\nsample per class while achieving competitive performance.\n3 Method\nDataset Distillationaims to synthesize a tiny synthetic dataset of (typically) images that are optimal\nfor training. Unlike previous works designed to produce synthetic samples for training new models\nfrom scratch, our new method,Linear Gradient Matching, aims to distill datasets for the purpose of\ntraininglinear classifiersin the embedding space ofpre-trained feature extractors.\n3.1 Linear Gradient Matching\nFormally, given a pre-trained self-supervised feature extractor, \u03d5, along with a real dataset Drealof\nimages Xrealand labels Yreal, we wish to distill a small synthetic set Dsyn={X syn, Ysyn}such that\nDsyncan be used to train a linear classifier in feature extractor \u03d5\u2019s embedding space that will have\nsimilar test-time performance to one trained onD real.\n3\nInspired by previous works that aimed to match single-step gradients [ 55] or multi-step trajectories [ 6],\nour solution is designed to ensure that training on our synthetic dataset results in similar updates as\ntraining on real data. In other words, the gradients of the classification loss using synthetic images\n(with respect to the linear classifier) should match those using real images.\nTo achieve this, we sample a random linear classifier matrix W\u223c N(0,1)c\u00d7fat each distillation step\nwhere cis the number of classes and fis the feature extractor \u03d5\u2019s output dimension. After passing\nthe real and synthetic images through the feature extractor \u03d5and linear classifier W, we find the real\nand synthetic classification losses where CE is the multiclass cross-entropy loss:\n\u2113real=CE(W\u03d5(X real);Y real)\n\u2113syn=CE(W\u03d5(X syn);Y syn)(1)\nWe then take the gradients of these classification losses with respect to the linear classifier W, and\nourmeta lossis then the cosine distance between them:\nLmeta= 1\u2212cos\u0012\nvec\u0012\u2202\u2113real\n\u2202W\u0013\n,vec\u0012\u2202\u2113syn\n\u2202W\u0013\u0013\n(2)\nThis meta loss is then back-propagated through the inner gradient computation, linear classifier, and\nfeature extractor to find \u2202Lmeta/\u2202X synand update our synthetic images. This process is repeated until\ndistillation is complete. An outline of thisLinear Gradient Matchingmethod can be found in Figure 2.\n3.2 Implicit Regularization\nAs noted in prior works [ 6,7,55],Dataset Distillationtends to yield synthetic images that areoverfit\nto the model used to perform the distillation. This issue manifests in what appear like adversarial\npatterns and inhibit the images\u2019 usefulness when used to train other models.\nOne recent work [ 15] proposed using a pyramid representation rather than na\u00efve pixels for image opti-\nmization problems. While the authors used this technique for CLIP inversion, we find it works remark-\nably well for our Dataset Distillation task. Rather than simply optimizing pixels, each synthetic sample\nis instead stored as a set of images of different resolutions \u03c1={1\u00d71,2\u00d72,4\u00d74, . . . ,256\u00d7256} .\nBefore each optimization step, the composite images Xare \u201crendered\u201d by bilinearly upsampling\neach level of the pyramids P={P r|r\u2208\u03c1} to the max resolution (256) and adding them together\nbefore smoothly clamping the pixel values with a sigmoid function:\nX=sigmoid X\nr\u2208\u03c1resize 256(Pr)!\n(3)\nFurthermore, we progressively optimize our pyramid, starting with just the lowest-resolution com-\nponent and periodically adding more tiers during distillation. The effects of using the pyramid\nrepresentation are quite dramatic, as seen in Figure 5.\nAs an additional step to combat overfitting, we also learn our distilled images in adecorrelatedcolor\nspace, as described in prior feature visualization work [ 30]. In short, we apply a fixed linear transform\nto the channels of our images after the pyramid reconstruction that brings them back into the standard\ncorrelated color space. This helps ward off any potential color-based biases induced by the model\nused during distillation.\n3.3 Differentiable Augmentations\nAs first noted in the work onDifferentiable Siamese Augmentation[ 53], applying differentiable\naugmentations to the synthetic images during the distillation process greatly improves the quality of\nthe distilled data. As such, we applyhorizontal flipping,random resized cropping, andGaussian\nnoisingto our distilled images at each step. In practice, we actually apply multiple rounds of this\naugmentation to different copies of the synthetic data at each iteration and concatenate the results.\nWe find this improves distillation since the optimization now encourages all these augmented copies\ntogetherto be the ideal training set rather than attempting to fit all pertinent information into a single\naugmented version of the images.\n4\nTrain Set\n(1 Img/Cls)ImageNet-100 ImageNet-1k\nCLIP DINO-v2 EV A-02 MoCo-v3 Average CLIP DINO-v2 EV A-02 MoCo-v3 Average\nDistilled (Ours)84.9\u00b10.191.5\u00b10.189.0\u00b10.083.4\u00b10.187.2\u00b10.163.0\u00b10.075.0\u00b10.170.3\u00b10.163.2\u00b10.067.9\u00b10.0\nNeighbors 67.8\u00b10.386.0\u00b10.278.8\u00b10.277.1\u00b10.177.4\u00b10.238.8\u00b10.167.7\u00b10.149.9\u00b10.156.4\u00b10.053.2\u00b10.1\nCentroids 77.1\u00b10.186.9\u00b10.380.9\u00b10.277.7\u00b10.180.6\u00b10.253.9\u00b10.069.5\u00b10.158.1\u00b10.157.4\u00b10.059.7\u00b10.1\nRandom 56.6\u00b11.674.8\u00b12.864.5\u00b12.761.4\u00b12.664.3\u00b12.431.7\u00b10.550.3\u00b10.537.7\u00b10.438.8\u00b10.639.6\u00b10.5\nFull Dataset 92.5\u00b10.095.2\u00b10.194.1\u00b10.189.4\u00b10.392.8\u00b10.178.7\u00b10.083.0\u00b10.081.7\u00b10.176.5\u00b10.080.0\u00b10.0\nTable 1:Linear Probes with One Image-per-Class:We compare our method (Distilled) to several real-image\nbaselines on ImageNet-100 (left) and ImageNet-1k (right). Images are distilled (or selected) using the given\nmodel in each column. \u201cNeighbors\u201d are the real images with embeddings closest to those of our distilled images.\n\u201cCentroids\u201d are the real images with embedding closest to the mean of each class. \u201cRandom\u201d is a random\nselection of real images. Our method outperforms each baseline across all models and both datasets.\nDistill\nModelImageNet-100 ImageNet-1k\nCLIP DINO-v2 EV A-02 MoCo-v3 Average CLIP DINO-v2 EV A-02 MoCo-v3 Average\nCLIP 84.9\u00b10.1 80.8\u00b10.4 83.8\u00b10.2 61.6\u00b10.2 77.8\u00b10.2 63.0\u00b10.0 56.4\u00b10.1 59.7\u00b10.1 39.5\u00b10.0 54.7\u00b10.1\nDINO-v2 77.0\u00b10.1 91.5\u00b10.1 86.8\u00b10.1 78.8\u00b10.1 83.5\u00b10.1 54.1\u00b10.0 75.0\u00b10.1 65.4\u00b10.1 60.0\u00b10.0 63.7\u00b10.1\nEV A-02 75.5\u00b10.2 86.4\u00b10.1 89.0\u00b10.0 67.7\u00b10.1 79.7\u00b10.1 55.5\u00b10.1 65.9\u00b10.1 70.3\u00b10.1 51.8\u00b10.0 60.9\u00b10.1\nMoCo-v3 65.6\u00b10.1 86.6\u00b10.1 82.3\u00b10.2 83.4\u00b10.1 79.5\u00b10.1 41.4\u00b10.0 66.9\u00b10.1 57.2\u00b10.1 63.2\u00b10.0 57.2\u00b10.1\nFull Dataset 92.5\u00b10.095.2\u00b10.194.1\u00b10.189.4\u00b10.392.8\u00b10.178.7\u00b10.083.0\u00b10.081.7\u00b10.176.5\u00b10.080.0\u00b10.0\nTable 2:Cross-Model Performance of Distilled Datasets: Here, we see ImageNet-100 (left) and ImageNet-1k\n(right) distilled using a given model and then evaluated across all models. We see that images distilled from DINO\nhave the best average cross-model performance for both datasets. The distilled datasets generalize well, aside from\nan outlier pair of CLIP and MoCo. Columns are colored based on percentage of the \u201cFull Dataset\u201d benchmark.\n4 Experiments\nWe evaluate our method on various datasets, including ImageNet-1k [ 12] and ImageNet-100 [ 42] for\nour primary results, Spawrious [ 26] and Waterbirds [ 38] for a study on adversarial datasets, Stanford\nDogs [ 21] and CUB-200-2011 [ 45] for fine-grained visual classification, and ArtBench [ 24] to test\nthe method\u2019s out-of-distribution capabilities. The majority of our experiments use four pre-trained\nself-supervised feature extractors as backbones: CLIP [ 34], DINO-v2 [ 31], EV A-02 [ 14], and MoCo-\nv3 [10]. In all our experiments, we distill the given dataset for 5000 iterations before training linear\nprobes to convergence on the resulting synthetic images. All experiments are conducted at 224\u00d7224\nresolution and use the \u201cViT-B\u201d version of the given model. All distilled datasets by default use 10\nsets of augmentations per batch except for ImageNet-1k, for which only 3 sets of augmentations are\nused due to compute constraints. For further implementation details, please see the Appendix.\nEvaluation and Baselines.To measure our method\u2019s performance on a given feature extractor,\nwe randomly initialize a linear classifier and optimize it to convergence using the distilled images\nbefore evaluating on the test set. The same procedure is used to evaluate real-image baselines. In\nprior works on self-supervised learning [ 31], the evaluation consists of a grid search across a number\nof hyper-parameters, including from which layer(s) the features used to train the linear probe should\nbe taken from. For simplicity, we instead use only the features from the backbone\u2019slast layerand\nkeep the training hyper-parameters consistent across the training of all linear probes.\nWe compare our method against three real-image baselines. For theNeighborsbaseline, we choose\nthe real image for each class whosedistill-modelembedding is closest to that of the corresponding\nsynthetic image produced by our method. Similarly, forCentroids, we take the real training image\nwith embedding closest to themeanembedding for each class. Lastly, forRandom, we simply select\na random image for each class and average the performance over 10 different seeds.\n5\n/uni00000015 /uni00000017 /uni00000019 /uni0000001b /uni00000014/uni00000013\n/uni00000024/uni00000058/uni0000004a/uni00000050/uni00000048/uni00000051/uni00000057/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056/uni00000003/uni00000053/uni00000048/uni00000055/uni00000003/uni00000025/uni00000044/uni00000057/uni00000046/uni0000004b/uni0000001a/uni00000013/uni0000001a/uni00000018/uni0000001b/uni00000013/uni0000001b/uni00000018/uni0000001c/uni00000013/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni0000002c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000031/uni00000048/uni00000057/uni00000010/uni00000014/uni00000013/uni00000013/uni00000003/uni00000027/uni0000004c/uni00000056/uni00000057/uni0000004c/uni0000004f/uni0000004f/uni00000048/uni00000047/uni00000003/uni0000005a/uni0000004c/uni00000057/uni0000004b/uni00000003/uni00000027/uni0000002c/uni00000031/uni00000032/uni00000010/uni00000059/uni00000015\n/uni00000028/uni00000059/uni00000044/uni0000004f/uni00000003/uni00000030/uni00000052/uni00000047/uni00000048/uni0000004f\n/uni00000026/uni0000002f/uni0000002c/uni00000033\n/uni00000027/uni0000002c/uni00000031/uni00000032/uni00000010/uni00000059/uni00000015\n/uni00000028/uni00000039 /uni00000024/uni00000010/uni00000013/uni00000015\n/uni00000030/uni00000052/uni00000026/uni00000052/uni00000010/uni00000059/uni00000016Figure 3: Performing more rounds of differentiable\naugmentation on the synthetic data during each distil-\nlation step improves both the single-model and cross-\nmodel performance of the distilled images.\n/uni00000016/uni00000013\n /uni00000015/uni00000013\n /uni00000014/uni00000013\n /uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013\n/uni00000033/uni00000026/uni00000014/uni00000015/uni00000013\n/uni00000014/uni00000013\n/uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000033/uni00000026/uni00000015/uni0000002c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000031/uni00000048/uni00000057/uni00000010/uni00000029/uni00000055/uni00000058/uni0000004c/uni00000057/uni00000056/uni0000001d/uni00000003/uni00000027/uni0000002c/uni00000031/uni00000032/uni00000010/uni00000059/uni00000015/uni00000003/uni00000028/uni00000050/uni00000045/uni00000048/uni00000047/uni00000047/uni0000004c/uni00000051/uni0000004a/uni00000056\n/uni00000027/uni0000004c/uni00000056/uni00000057/uni0000004c/uni0000004f/uni0000004f/uni00000048/uni00000047\n/uni00000026/uni00000048/uni00000051/uni00000057/uni00000055/uni00000052/uni0000004c/uni00000047Figure 4: We distill ImageNet-Fruits and observe the\nPCA of the training image embeddings. Each color\nrepresents a class. Note that the distilled images typi-\ncally lie on the edge or outside of their class\u2019s cluster.\n4.1 Linear Gradient Matching Out-Performs Real-Image Baselines\nFirst, we evaluate our method on the original dataset distillation task: using a backbone model\nto distill a dataset and then training a modelof the same architectureon the resulting synthetic\nimages. Note that this setting differs from previous dataset distillation works in that we use a\npre-trained backbonedistillation and reuse that backbone during evaluation by training a new\nrandomly-initialized linear classifier on top of it.\nIn Table 1, we see results for the single-model setting when distilling ImageNet-100 and ImageNet-1k.\nAcross both datasets and all four models,our method convincingly out-performs all real-image\nbaselines. In particular, our method enables a linear probe trained on top of DINO-v2 [ 31] to reach\n75% test accuracyon ImageNet-1k while only ever having seen asingle labeled image per class.\nIn comparison, training on the full dataset of 1.3 million real images reaches just 7 points higher at\n83%. Similar results are seen for the other models and datasets as well.\nIn Figure 4 we visualize theembeddingsof the distilled images relative to the real training data\nby plotting the 2D principal component analysis (PCA). For the sake of visual clarity, we use\nImageNet-Fruits [6,7], a toy dataset of 10 fruit classes from ImageNet-1k. The embeddings in this\nfigure are from the same model used to distill the dataset (DINO-v2). We observe that the embeddings\nof the distilled images tend to lie far away from their respective class\u2019s centroid, often falling on\nthe outside edge of the class\u2019s distribution. We hypothesize that this is due in-part to the distillation\nembeddinghighly-discriminativefeatures within the synthetic images.\n4.2 Distilled Images Generalize Across Models\nNext, we investigate the cross-model generalization capabilities of our distilled datasets. That is, we\nfirst distill a dataset using one backbone model (e.g., CLIP) and then evaluate the synthetic images\u2019 per-\nformance on theothermodels (DINO-v2, EV A-02, MoCo-v3) that were not seen during distillation.\nWe visualize these results in Table 2. The diagonal elements are equivalent to the first row of Table 1\nand represent single-model performance while the off-diagonals show cross-model capabilities. We\nsee that the distilled datasets tend to generalize well to unseen models save for an outlier between\nCLIP and Moco, possibly due to poor modelalignment(Section 4.6). We also observe that the highest-\naccuracy model on the full dataset (DINO) also has the distilled dataset with the best cross-model per-\nformance, suggesting that a model\u2019s quality affects the generalization capabilities of its distilled data.\n4.3 Regularization and Augmentation Enable High Performance\nIn this section, we quantitatively (Figure 3) and qualitatively (Table 5) analyze the effects of the\nvarious regularization and augmentation techniques included in our distillation process. Table 3 shows\nthe effects of ablating various components when evaluating using the same model used to distill (top)\n6\nTrain Set\n(1 Img/Cls)ImageNet-100\nCLIP DINO-v2 EV A-02 MoCo-v3 AverageSame EvalFull (Ours)84.9\u00b10.191.5\u00b10.189.0\u00b10.083.4\u00b10.187.2\u00b10.1\n-Decorrelate 82.6\u00b10.191.3\u00b10.289.0\u00b10.183.2\u00b10.186.5\u00b10.1\n-Pyramid 83.5\u00b10.291.0\u00b10.387.9\u00b10.180.5\u00b10.185.7\u00b10.1\n-Augment 58.4\u00b10.282.6\u00b10.474.0\u00b10.359.6\u00b10.568.6\u00b10.4Avg CrossFull (Ours)75.4\u00b10.380.9\u00b10.176.6\u00b10.178.2\u00b10.177.8\u00b10.2\n-Decorrelate 69.4\u00b10.479.5\u00b10.176.8\u00b10.179.9\u00b10.276.4\u00b10.2\n-Pyramid 57.3\u00b10.274.5\u00b10.268.5\u00b10.268.1\u00b10.267.1\u00b10.2\n-Augment 35.3\u00b10.431.6\u00b10.234.3\u00b10.532.2\u00b10.533.3\u00b10.4\nFull Dataset 92.5\u00b10.095.2\u00b10.194.1\u00b10.189.4\u00b10.392.8\u00b10.1\nTable 3:Evaluating Ablations:While all three components\nprovide improvements, the Augmentation has the most dra-\nmatic effect, especially in the cross-model setting. Likewise,\nthe Pyramid optimization seems to matter more in the cross-\nmodel setting than the same-model setting by mitigating over-\nfitting to the model used during distillation.\n/uni00000026/uni0000002f/uni0000002c/uni00000033\n /uni00000027/uni0000002c/uni00000031/uni00000032/uni00000010/uni00000059/uni00000015\n /uni00000028/uni00000039 /uni00000024/uni00000010/uni00000013/uni00000015\n/uni00000032/uni00000058/uni00000055/uni00000056/uni00000030/uni00000052/uni00000026/uni00000052/uni00000010/uni00000059/uni00000016\n/uni00000010/uni00000027/uni00000048/uni00000046/uni00000052/uni00000055/uni00000055/uni00000048/uni0000004f/uni00000044/uni00000057/uni00000048\n /uni00000010/uni00000033/uni0000005c/uni00000055/uni00000044/uni00000050/uni0000004c/uni00000047\n /uni00000010/uni00000024/uni00000058/uni0000004a/uni00000050/uni00000048/uni00000051/uni00000057Figure 5:Visualizing Ablations:Removing\nvarious components of our pipeline causes\nvisual degradation in the distilled images.\nand averaging performances across the other three models (bottom). Figure 5 visualizes the distilled\nPineapple class from ImageNet-100 under the same ablations.\nColor DecorrelationInspired by prior work in feature visualization [ 30] and motivated by a desire\nto spare our synthetic images from any color-related biases imposed by the model used to distill\nthem, we apply a color decorrelation technique to our distillation process. In Figure 5, we see that the\nimages distilled without the color decorrelation (column 2) look less realistic than those using our\nfull method (column 1); they are over saturated and contain high levels of incorrect colors (blue in\nthis example). Quantitatively, however, this component has the lowest effect of the three analyzed in\nthis section. While generally slightly helpful, it only offers a large improvement in the cross-model\nsetting when distilling using CLIP.\nPyramid RepresentationIt has been shown in prior work [ 7] that pixel-based optimization for\ndataset distillation does not scale well to higher resolutions; the synthetic images tend to become\nriddled with high-frequency patterns that inhibit their usefulness as training data. As discussed in\nSection 3.2, we instead adopt a pyramid representation for the distillation process. In Figure 5, we\nsee samples distilled from ImageNet-100 optimized using either the pyramid (column 1) or na\u00efve\npixel representation (column 3). We encourage the reader to zoom in and observe the high-frequency\npatterns and overall lack of coherence in the pixel-based images. In Table 3, we see that the images\ndistilled without the pyramid representation do indeed make far worse training data in the cross-model\nsetting, causing the model to overfit to the high-frequency patterns.\nDifferentiable AugmentationsSince its first proposal [ 53], the incorporation of differentiable\naugmentations during the distillation process has proven critical to the efficacy of the synthesized\nimages as training data. As such, we also perform differentiable augmentations in this work, as\ndescribed in Section 3.3. In Figure 5, we see samples from ImageNet-100 distilled both with (col-\numn 1) and without (column 4) augmentations. Visually, we observe that the images distilledwithout\naugmentations lack much meaningful structure and appear to simply be blobs of color with just a hint\nof geometry. In Table 3, we see that the lack of augmentationsseverelylimits the synthetic images\u2019\nusefulness as training data, even when evaluation on the same model used to distill.\nAs earlier discussed, we also applymultiple roundsof augmentations at each distillation step and\nconcatenate the multiple augmented versions together to form the synthetic batch. Figure 3 illustrates\nhow the number ofaugmentations per batchaffects both the single-model and cross-model perfor-\nmance of the distilled data. Specifically, we see that when distilling ImageNet-100 with DINO-v2,\nraising the number of augmentations per batch from 1 to 5 and eventually 10 steadily increases the\ndistilled images\u2019 effectiveness as training data both on the backbone model used to distill (DINO-v2)\nand the other unseen models (CLIP, EV A-02, MoCo-v3).\n7\n/uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051\n/uni00000025/uni00000058/uni0000004f/uni0000004f/uni00000047/uni00000052/uni0000004a/uni00000037/uni00000048/uni00000056/uni00000057\n/uni00000026/uni00000052/uni00000055/uni0000004a/uni0000004c\n /uni00000027/uni00000044/uni00000046/uni0000004b/uni00000056/uni0000004b/uni00000058/uni00000051/uni00000047\n /uni0000002f/uni00000044/uni00000045/uni00000055/uni00000044/uni00000047/uni00000052/uni00000055\n/uni00000035/uni00000048/uni00000044/uni0000004f/uni00000003/uni0000002c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000056\n/uni00000027/uni0000002c/uni00000031/uni00000032/uni00000010/uni00000059/uni00000015\n/uni00000025/uni00000058/uni0000004f/uni0000004f/uni00000047/uni00000052/uni0000004a/uni00000030/uni00000052/uni00000026/uni00000052/uni00000010/uni00000059/uni00000016\n/uni00000026/uni00000052/uni00000055/uni0000004a/uni0000004c\n /uni00000027/uni00000044/uni00000046/uni0000004b/uni00000056/uni0000004b/uni00000058/uni00000051/uni00000047\n /uni0000002f/uni00000044/uni00000045/uni00000055/uni00000044/uni00000047/uni00000052/uni00000055\n/uni00000027/uni0000004c/uni00000056/uni00000057/uni0000004c/uni0000004f/uni0000004f/uni00000048/uni00000047/uni00000003/uni0000002c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000056Figure 6:Distilling Datasets with Spurious Correlations:The 4-class \u201cSpawrious\u201d dataset contains spurious\nbackground correlations in the training set that are then subverted in the test set (left). A DINO linear\nprobe trained on the full training set performs well, reaching 78% test accuracy, while a MoCo probe fails\ncatastrophically, only reaching 36%. The distilled images (right) hint as to why: those distilled with DINO-v2\nstill contain mostly decipherable dog breeds while the MoCo-v3 counterparts focus almost entirely on the\nbackground environments. These images likely reflect the same biases held by the models used to distill them.\nTrain Set\n(1 Img/Cls)Spawrious WaterBirds\nCLIP DINO-v2 EV A-02 MoCo-v3 Average CLIP DINO-v2 EV A-02 MoCo-v3 Average\nDistilled (Ours)43.1\u00b16.480.8\u00b13.136.5\u00b13.132.7\u00b12.348.3\u00b13.777.9\u00b10.282.1\u00b12.978.0\u00b10.877.8\u00b10.079.0\u00b11.0\nNeighbors41.7\u00b14.676.9\u00b11.240.7\u00b12.733.0\u00b13.448.1\u00b13.074.6\u00b16.567.3\u00b15.874.3\u00b13.345.2\u00b111.465.3\u00b16.8\nCentroids44.8\u00b15.080.2\u00b12.738.1\u00b13.330.5\u00b11.648.4\u00b13.269.8\u00b18.665.7\u00b13.464.6\u00b17.962.0\u00b15.265.5\u00b16.3\nRandom46.2\u00b14.468.1\u00b18.133.3\u00b15.331.8\u00b12.544.9\u00b15.171.9\u00b14.557.1\u00b18.659.8\u00b113.367.4\u00b15.764.0\u00b18.0\nFull Dataset 50.4\u00b10.378.1\u00b10.150.1\u00b10.435.8\u00b10.053.6\u00b10.286.0\u00b10.195.5\u00b10.190.4\u00b10.274.3\u00b10.186.5\u00b10.1\nTable 4:Performance on Datasets with Spurious Background Correlations:The Spawrious (left) and Water-\nbirds (right) training sets contain intentionally adversarial background correlations that are then subverted in the\ntest sets. On Spawrious, our method no longer out-performs the real-image baselines as with the standard datasets\n(Table 1). This is perhaps due to the synthetic data adopting the models\u2019 biases and overfitting to the backgrounds\non the training sets. We also see interesting interpretability results in the images themselves (Figure 6).\n4.4 Distilling Adversarial Datasets Reveals Interpretable Model Weaknesses\nThe \u201cSpawrious\u201d dataset [ 26] consists of four classes of dog breeds and is designed to evaluate a\nmodel\u2019s ability to focus on the relevant content of an image rather than spurious correlations. In the\ntraining data, each breed is paired with its own unique environment: (Bulldog, Forest), (Corgi, Desert) ,\n(Dachshund, Mountain), and (Labrador, Snow). However, in the test set, the environments are inten-\ntionally different: (Bulldog, Mountain), (Corgi, Forest), (Dachshund, Snow), and (Labrador, Desert) .\nWe also experiment with a similarly constructed data, \u201cWaterbirds\u201d [ 38] consisting of just two classes\n(land birds and water birds) with similarly spurious background correlations.\nQuantitative results on these datasets are shown in Table 4. Our method generally outperforms the\nreal-image baselines, but to a significantly smaller extent than on the standard datasets (Table 1). We\nalso observe that when training on the full Spawrious dataset, DINO-v2 [ 31]farout-performs the\nrest of the models (81% test accuracy) with MoCo-v3 [ 10] doing dramatically worse (37%), both to\ndegrees far greater than seen when training on standard datasets (as in Table 1).\nBydistillingSpawrious with our method, we gain potential insights as to why we might see such\ndramatic results. As seen in Figure 6, the images distilled with DINO-v2 contain clear (albeit\nabstract) depictions of the correct dog breeds. On the other hand, those yielded by MoCo-v3 are\nalmost entirely indecipherable, with the only clear portions being the spurious backgrounds from the\ntraining set. This interpretability result gives a clue towards explaining MoCo-v3\u2019s poor performance\non this dataset; the model clearly seems to focus on the spurious background correlations present\nin the training set rather than the relevant subjects, causing catastrophic overfitting.\n4.5 Distillation Excels in Fine-Grained Visual Classification\nWhile computer vision methods are typically benchmarked against datasets containing broad cate-\ngories of objects, such as ImageNet [ 12], much research also focuses on the task of Fine-Grained\nVisual Classification (FGVC) wherein datasets consist of many closely related classes that can be\n8\n/uni00000025/uni00000052/uni00000055/uni00000047/uni00000048/uni00000055/uni00000003/uni00000037/uni00000048/uni00000055/uni00000055/uni0000004c/uni00000048/uni00000055\n /uni0000002c/uni00000055/uni0000004c/uni00000056/uni0000004b/uni00000003/uni00000037/uni00000048/uni00000055/uni00000055/uni0000004c/uni00000048/uni00000055\n /uni00000031/uni00000052/uni00000055/uni00000049/uni00000052/uni0000004f/uni0000004e/uni00000003/uni00000037/uni00000048/uni00000055/uni00000055/uni0000004c/uni00000048/uni00000055\n /uni00000031/uni00000052/uni00000055/uni0000005a/uni0000004c/uni00000046/uni0000004b/uni00000003/uni00000037/uni00000048/uni00000055/uni00000055/uni0000004c/uni00000048/uni00000055\n /uni0000003c/uni00000052/uni00000055/uni0000004e/uni00000056/uni0000004b/uni0000004c/uni00000055/uni00000048/uni00000003/uni00000037/uni00000048/uni00000055/uni00000055/uni0000004c/uni00000048/uni00000055\n /uni0000002f/uni00000044/uni0000004e/uni00000048/uni0000004f/uni00000044/uni00000051/uni00000047/uni00000003/uni00000037/uni00000048/uni00000055/uni00000055/uni0000004c/uni00000048/uni00000055\n /uni0000003a/uni0000004b/uni00000048/uni00000044/uni00000057/uni00000048/uni00000051/uni00000003/uni00000037/uni00000048/uni00000055/uni00000055/uni0000004c/uni00000048/uni00000055\n /uni00000024/uni00000058/uni00000056/uni00000057/uni00000055/uni00000044/uni0000004f/uni0000004c/uni00000044/uni00000051/uni00000003/uni00000037/uni00000048/uni00000055/uni00000055/uni0000004c/uni00000048/uni00000055\n /uni00000036/uni0000004c/uni0000004f/uni0000004e/uni0000005c/uni00000003/uni00000037/uni00000048/uni00000055/uni00000055/uni0000004c/uni00000048/uni00000055\n /uni00000036/uni00000048/uni00000044/uni0000004f/uni0000005c/uni0000004b/uni00000044/uni00000050/uni00000003/uni00000037/uni00000048/uni00000055/uni00000055/uni0000004c/uni00000048/uni00000055\n/uni00000025/uni00000044/uni0000004c/uni00000055/uni00000047/uni00000003/uni00000036/uni00000053/uni00000044/uni00000055/uni00000055/uni00000052/uni0000005a\n /uni00000025/uni00000055/uni00000048/uni0000005a/uni00000048/uni00000055/uni00000003/uni00000036/uni00000053/uni00000044/uni00000055/uni00000055/uni00000052/uni0000005a\n /uni0000002b/uni00000052/uni00000058/uni00000056/uni00000048/uni00000003/uni00000036/uni00000053/uni00000044/uni00000055/uni00000055/uni00000052/uni0000005a\n /uni00000029/uni0000004c/uni00000048/uni0000004f/uni00000047/uni00000003/uni00000036/uni00000053/uni00000044/uni00000055/uni00000055/uni00000052/uni0000005a\n /uni00000029/uni00000052/uni0000005b/uni00000003/uni00000036/uni00000053/uni00000044/uni00000055/uni00000055/uni00000052/uni0000005a\n /uni0000002b/uni00000044/uni00000055/uni00000055/uni0000004c/uni00000056/uni00000003/uni00000036/uni00000053/uni00000044/uni00000055/uni00000055/uni00000052/uni0000005a\n /uni0000002f/uni0000004c/uni00000051/uni00000046/uni00000052/uni0000004f/uni00000051/uni00000003/uni00000036/uni00000053/uni00000044/uni00000055/uni00000055/uni00000052/uni0000005a\n /uni00000036/uni00000052/uni00000051/uni0000004a/uni00000003/uni00000036/uni00000053/uni00000044/uni00000055/uni00000055/uni00000052/uni0000005a\n /uni00000037/uni00000055/uni00000048/uni00000048/uni00000003/uni00000036/uni00000053/uni00000044/uni00000055/uni00000055/uni00000052/uni0000005a\n /uni00000039/uni00000048/uni00000056/uni00000053/uni00000048/uni00000055/uni00000003/uni00000036/uni00000053/uni00000044/uni00000055/uni00000055/uni00000052/uni0000005aFigure 7:Distilling Fine-Grained Datasets:Our distillation method captures the details necessary to teach a\nclassifier to distinguish between highly similar classes. Pictured above are just 10 of the 120 classes distilled\nfrom the Stanford Dogs [ 21] dataset (top) and 10 of the 200 classes distilled from Caltech-UCSD Birds [ 45]\nusing DINO-v2. For the full distilled datasets, please see the Appendix or our project page.\nTrain Set\n(1 Img/Cls)Stanford Dogs CUB-2011\nCLIP DINO-v2 EV A-02 MoCo-v3 Average CLIP DINO-v2 EV A-02 MoCo-v3 Average\nDistilled (Ours)52.1\u00b10.283.0\u00b10.174.8\u00b10.169.6\u00b10.269.9\u00b10.262.2\u00b10.286.0\u00b10.174.1\u00b10.242.5\u00b10.266.2\u00b10.2\nNeighbors 33.4\u00b10.171.3\u00b10.258.5\u00b10.256.3\u00b10.154.9\u00b10.239.4\u00b10.176.9\u00b10.052.6\u00b10.328.1\u00b10.049.2\u00b10.1\nCentroids 43.3\u00b10.173.0\u00b10.260.9\u00b10.255.2\u00b10.258.1\u00b10.254.3\u00b10.278.5\u00b10.259.9\u00b10.330.2\u00b10.155.7\u00b10.2\nRandom 23.3\u00b11.551.9\u00b11.838.3\u00b11.836.6\u00b11.437.5\u00b11.637.5\u00b11.664.4\u00b11.544.3\u00b11.519.1\u00b10.541.3\u00b11.3\nFull Dataset 76.9\u00b10.188.6\u00b10.182.6\u00b10.172.3\u00b10.580.1\u00b10.277.5\u00b10.790.2\u00b10.284.0\u00b10.343.7\u00b10.873.8\u00b10.5\nTable 5:Performance on Fine-Grained Datasets:Our distillation method captures the most discriminative\naspects of each class, thereby enabling a down-stream classifier to correctly identify samples from datasets\nwhere all classes are closely related. In particular, the performance gap between our method and the real-image\nbaselines is even higher on these fine-grained datasets (Stanford Dogs [ 21] and CUB-200-2011 [ 45]) than on the\nstandard ImageNet benchmarks.\nchallenging for even human experts to distinguish. Such datasets require models to learn fine-grained\nfeatures in order to identify the subtle differences between each class.\nWe apply our linear method to two common FGVC datasets: Stanford Dogs [ 21] and Caltech-UCSD\nBirds-200-2011 [ 45]. Samples of these datasets distilled using DINO-v2 can be seen in Figure 7. As\nseen in Table 5, in this more challenging fine-grained setting, our method outperforms the real-image\nbaselines to asignificantly higherdegree than on the standard ImageNet benchmarks (Table 1). It\nseems as though the distillation process\u2019s ability to store didactic information in a single image\nper class is even more relevant in this setting since any one real image is less likely to contain the\ninformation necessary to distinguish its entire respective class from the other highly similar classes.\n4.6 Cross-Model Performance Predicts Model Alignment\nRecent research has suggested an \u201calignment\u201d of various self-supervised models despite large\ndifferences in their training methods or even data modalities. The notion that all these models are\nindeed converging to a unified embedding space has been dubbed the \u201cPlatonic Representation\nHypothesis\u201d [ 20]. While the representations of today\u2019s models are not yet perfectly synchronized, it\nis still of interest to measure the degree to which they are aligned.\nIn the hypothesis\u2019s titular work, the authors introduce a method of measuring the alignment of two\nmodels called \u201cmutual knearest neighbors.\u201d In short, this method finds each sample\u2019s knearest\nneighbors in the embedding spaces of each model and then computes the fraction of neighbors that\nare shared between the two.\nWhen we distill images with a given model A and measure the 1 nearest neighbor performance\n(since there is just one image per class) on a different model B, we find that the normalized test\naccuracy is strongly correlated with the mutual k-nn alignment between models A and B, as seen\nin Table 6. This is an interesting interpretability result showing that the cross-model performance of\na distilled dataset highly depends on the alignment between the two models. Given this dependency,\nour distillation method then acts as a method ofvisualizinghow well these models align by backing\nthe discrepancies in the embedding space out to image space. For example, given the two least\naligned models (CLIP and MoCo), we can see in Figure 1 that even for different sets of classes, the\ntwo models clearly distill very differentstylesof images, alluding towards their misalignment.\n9\nDistill\nModelNormalizedk-NN Accuracy\nCLIP DINO-v2 EV A-02 MoCo-v3\nCLIP 100.0 69.5 72.3 46.6\nDINO-v2 77.2 100.0 95.5 91.7\nEV A-02 66.1 86.1 100.0 70.2\nMoCo-v3 57.2 87.4 76.8 100.0Source\nModelModel Alignment\nCLIP DINO-v2 EV A-02 MoCo-v3\nCLIP 1.00 0.21 0.26 0.18\nDINO-v2 0.21 1.00 0.39 0.31\nEV A-02 0.26 0.39 1.00 0.30\nMoCo-v3 0.18 0.31 0.30 1.00\nTable 6:Distilled datasets predict model alignment.We distill ImageNet-1k and find the synthetic data\u2019s\ncross-modelperformance (left) by evaluating on a model other than the one used during distillation. We find that\nthis cross-model performance correlates well with thealignmentbetween models\u2019 embedding spaces (right).\nNote the similarity of the per-row trends between the two tables. Rows are colored from highest to lowest.\n/uni00000027/uni0000004c/uni00000056/uni00000057/uni0000004c/uni0000004f/uni0000004f/uni00000048/uni00000047\n/uni00000024/uni00000055/uni00000057/uni00000003/uni00000031/uni00000052/uni00000058/uni00000059/uni00000048/uni00000044/uni00000058/uni00000031/uni00000048/uni0000004c/uni0000004a/uni0000004b/uni00000045/uni00000052/uni00000055/uni00000056\n/uni00000025/uni00000044/uni00000055/uni00000052/uni00000054/uni00000058/uni00000048\n /uni00000028/uni0000005b/uni00000053/uni00000055/uni00000048/uni00000056/uni00000056/uni0000004c/uni00000052/uni00000051/uni0000004c/uni00000056/uni00000050\n /uni0000002c/uni00000050/uni00000053/uni00000055/uni00000048/uni00000056/uni00000056/uni0000004c/uni00000052/uni00000051/uni0000004c/uni00000056/uni00000050\n /uni00000033/uni00000052/uni00000056/uni00000057/uni00000010/uni0000002c/uni00000050/uni00000053/uni00000055/uni00000048/uni00000056/uni00000011\n /uni00000035/uni00000048/uni00000044/uni0000004f/uni0000004c/uni00000056/uni00000050\n /uni00000035/uni00000048/uni00000051/uni00000044/uni0000004c/uni00000056/uni00000056/uni00000044/uni00000051/uni00000046/uni00000048\n /uni00000035/uni00000052/uni00000050/uni00000044/uni00000051/uni00000057/uni0000004c/uni00000046/uni0000004c/uni00000056/uni00000050\n /uni00000036/uni00000058/uni00000055/uni00000055/uni00000048/uni00000044/uni0000004f/uni0000004c/uni00000056/uni00000050\n /uni00000038/uni0000004e/uni0000004c/uni0000005c/uni00000052/uni00000010/uni00000048\nFigure 8:Distilling Out-of-Distribution Datasets:Despite only ever being trained on the real-world images\nin ImageNet-1k [ 12], our method can still use DINO-v1 [ 5] to distill out-of-distribution datasets such as\nArtBench [ 24] that have no overlapping content. We see that the distillation is not simply \u201ccopying\u201d samples\nfrom the dataset by comparing to the nearest real neighbor in the model\u2019s embedding space and observing the\nstark differences. This interpretability insight granted by our method highlights DINO\u2019s remarkable ability to\ngeneralize beyond its training distribution.\n4.7 Self-Supervised Backbones can Distill Out-of-Distribution Datasets\nNote that while we have thus far been discussing DINO-v2, this section in particular will reference\nDINO-v1 since this version\u2019s training set is open-sourced. Since DINO-v1 [ 5] was only trained on\nImageNet [ 12], a dataset of nearly all \u201creal-world\u201d images, we can easily test its ability to distill an\nout-of-distributiondataset such as ArtBench [ 24] that consists of 10 classes of art styles. In Figure 8,\nwe visualize the distilled images (top) as well as the nearest neighbor in DINO-v1\u2019s embedding\nspace. Surprisingly, despite having previously only ever seen \u201creal-world\u201d images from ImageNet,\nDINO-v1 is still able to effectively distill ArtBench into a single image per class. Furthermore, we\nconfirm that the distilling is not simply \u201ccopying\u201d real images from the training set by observing\nthe stark differences between the synthetic images and their nearest neighbors. This is especially\napparent in the cases of Realism, Renaissance, and Romanticism where the nearest neighbors are\nblack and white while the distilled images are colorful.\n5 Conclusion\nIn this work, we introduce a new task in the area ofDataset Distillation: learning a tiny set of synthetic\nimages designed to trainlinear probeson top ofpre-trained self-supervised vision models. Our\nproposed solution,Linear Gradient Matching, optimizes a meta gradient objective to ensure that our\nsynthetic images induce training updates similar to those obtained from the real data. Quantitatively,\nour method outperforms all baselines, enabling, for example, a DINO-v2 linear probe to reach 75%\ntest accuracy on ImageNet-1k while only having ever seenone labeled imageper class.\nWe also showcase the importance of secondary aspects of our Linear Gradient Matching method, such\nas the pyramid representation and differentiable augmentations, and highlight their importance in\nlearning highly efficacious distilled datasets. Furthermore, we evaluate our method on challenging fine-\ngrained datasets and show that it out-performs the real-image baselines by an even larger margin than\non the standard ImageNet benchmarks. Lastly, our method yields several interesting interpretability\nresults, such as giving insights into how these pre-trained models \u201csee,\u201d predicting how well different\nmodels align, or elucidating a model\u2019s ability to generalize beyond its training distribution.\nWe hope our work brings to light the interesting set of problems posed by this new task and inspires\nothers to continue in this area. Code and distilled datasets can be found on our project page.\n10\nAcknowledgments\nThis work was supported by the National Science Foundation under Grant No. 2211259, by the\nSingapore DSTA under DST00OECI20300823 (New Representations for Vision, 3D Self-Supervised\nLearning for Label-Efficient Vision), by the Intelligence Advanced Research Projects Activity\n(IARPA) via Department of Interior/Interior Business Center (DOI/IBC) under 140D0423C0075, by\nONR MURI grant #033697-00007, by the Amazon Science Hub, by the MIT-Google Program for\nComputing Innovation, by Sony Interactive Entertainment, and by a 2025 MIT ORCD Seed Fund\nCompute Grant.\n11\nReferences\n[1] Adam McLean. The surrealism website.https://surrealism.website/. 16\n[2]J. Ansel, E. Yang, H. He, N. Gimelshein, A. Jain, M. V oznesensky, B. Bao, P. Bell, D. Berard, E. Burovski,\nG. Chauhan, A. Chourdia, W. Constable, A. Desmaison, Z. DeVito, E. Ellison, W. Feng, J. Gong,\nM. Gschwind, B. Hirsh, S. Huang, K. Kalambarkar, L. Kirsch, M. Lazos, M. Lezcano, Y . Liang, J. Liang,\nY . Lu, C. K. Luk, B. Maher, Y . Pan, C. Puhrsch, M. Reso, M. Saroufim, M. Y . Siraichi, H. Suk, S. Zhang,\nM. Suo, P. Tillet, X. Zhao, E. Wang, K. Zhou, R. Zou, X. Wang, A. Mathews, W. Wen, G. Chanan, P. Wu,\nand S. Chintala. Pytorch 2: Faster machine learning through dynamic python bytecode transformation and\ngraph compilation. InProceedings of the 29th ACM International Conference on Architectural Support for\nProgramming Languages and Operating Systems, Volume 2, ASPLOS \u201924, New York, NY , USA, 2024.\nAssociation for Computing Machinery. 15, 16\n[3]L. Bossard, M. Guillaumin, and L. Van Gool. Food-101 \u2013 mining discriminative components with random\nforests. InECCV, 2014. 16, 17, 36\n[4]J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke,\nJ. VanderPlas, S. Wanderman-Milne, and Q. Zhang. JAX: composable transformations of Python+NumPy\nprograms, 2018. 15\n[5]M. Caron, H. Touvron, I. Misra, H. J\u00e9gou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging properties in\nself-supervised vision transformers. InICCV, 2021. 3, 10, 16\n[6]G. Cazenavette, T. Wang, A. Torralba, A. A. Efros, and J.-Y . Zhu. Dataset distillation by matching training\ntrajectories. InCVPR, 2022. 1, 2, 3, 4, 6\n[7]G. Cazenavette, T. Wang, A. Torralba, A. A. Efros, and J.-Y . Zhu. Generalizing dataset distillation via\ndeep generative prior. InCVPR, 2023. 1, 3, 4, 6, 7\n[8]T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning of visual\nrepresentations. InICML, 2020. 3\n[9]X. Chen, H. Fan, R. Girshick, and K. He. Improved baselines with momentum contrastive learning.arXiv\npreprint arXiv:2003.04297, 2020.\n[10] X. Chen, S. Xie, and K. He. An empirical study of training self-supervised vision transformers. InICCV,\n2021. 3, 5, 8, 17, 21\n[11] J. Cui, R. Wang, S. Si, and C.-J. Hsieh. Scaling up dataset distillation to imagenet-1k with constant memory.\nInICML, 2023. 1, 3\n[12] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. InCVPR, 2009. 2, 5, 8, 10, 16, 17, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31\n[13] Y . Fang, Q. Sun, X. Wang, T. Huang, X. Wang, and Y . Cao. Eva-02: A visual representation for neon\ngenesis.Image and Vision Computing, 2024. 3, 16, 20\n[14] Y . Fang, W. Wang, B. Xie, Q. Sun, L. Wu, X. Wang, T. Huang, X. Wang, and Y . Cao. Eva: Exploring the\nlimits of masked visual representation learning at scale. InCVPR, 2023. 3, 5, 17\n[15] S. Fort and J. Whitaker. Direct ascent synthesis: Revealing hidden generative capabilities in discriminative\nmodels.arXiv preprint arXiv:2502.07753, 2025. 4\n[16] Z. Guo, K. Wang, G. Cazenavette, H. LI, K. Zhang, and Y . You. Towards lossless dataset distillation via\ndifficulty-aligned trajectory matching. InICLR, 2024. 1, 3\n[17] K. He, X. Chen, S. Xie, Y . Li, P. Doll\u00e1r, and R. Girshick. Masked autoencoders are scalable vision learners.\nInCVPR, 2022. 3\n[18] K. He, H. Fan, Y . Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual representation\nlearning. InCVPR, 2020. 3\n[19] Hugging Face. Hugging Face: The AI community building the future. https://huggingface.co ,\n2016. 16\n[20] M. Huh, B. Cheung, T. Wang, and P. Isola. Position: The platonic representation hypothesis. InICML,\n2024. 2, 3, 9\n12\n[21] A. Khosla, N. Jayadevaprakash, B. Yao, and L. Fei-Fei. Novel dataset for fine-grained image categorization.\nInCVPR, 2011. 5, 9, 16, 17, 32\n[22] D. Kingma and J. Ba. Adam: A method for stochastic optimization. InICLR, 2015. 15\n[23] P. W. Koh, S. Sagawa, H. Marklund, S. M. Xie, M. Zhang, A. Balsubramani, W. Hu, M. Yasunaga, R. L.\nPhillips, I. Gao, T. Lee, E. David, I. Stavness, W. Guo, B. A. Earnshaw, I. S. Haque, S. Beery, J. Leskovec,\nA. Kundaje, E. Pierson, S. Levine, C. Finn, and P. Liang. WILDS: A benchmark of in-the-wild distribution\nshifts. InICML, 2021. 16\n[24] P. Liao, X. Li, X. Liu, and K. Keutzer. The artbench dataset: Benchmarking generative models with\nartworks.arXiv preprint arXiv:2206.11404, 2022. 5, 10, 16\n[25] D. Liu, J. Gu, H. Cao, C. Trinitis, and M. Schulz. Dataset distillation by automatic training trajectories. In\nECCV, 2024. 1\n[26] A. Lynch, G. J.-S. Dovonon, J. Kaddour, and R. Silva. Spawrious: A benchmark for fine control of spurious\ncorrelation biases, 2023. 5, 8, 16\n[27] T. Nguyen, Z. Chen, and J. Lee. Dataset meta-learning from kernel ridge-regression. InICLR, 2020. 1, 2\n[28] T. Nguyen, R. Novak, L. Xiao, and J. Lee. Dataset distillation with infinitely wide convolutional networks.\nInNeurIPS, 2021. 1\n[29] M.-E. Nilsback and A. Zisserman. Automated flower classification over a large number of classes. In\nProceedings of the Indian Conference on Computer Vision, Graphics and Image Processing (ICVGIP),\nKolkata, India, 2008. 16, 17, 35\n[30] C. Olah, A. Mordvintsev, and L. Schubert. Feature visualization.Distill, 2017.\nhttps://distill.pub/2017/feature-visualization. 4, 7\n[31] M. Oquab, T. Darcet, T. Moutakanni, H. V . V o, M. Szafraniec, V . Khalidov, P. Fernandez, D. HAZIZA,\nF. Massa, A. El-Nouby, M. Assran, N. Ballas, W. Galuba, R. Howes, P.-Y . Huang, S.-W. Li, I. Misra,\nM. Rabbat, V . Sharma, G. Synnaeve, H. Xu, H. Jegou, J. Mairal, P. Labatut, A. Joulin, and P. Bojanowski.\nDINOv2: Learning robust visual features without supervision.TMLR, 2024. 3, 5, 6, 8, 16, 17, 19, 22, 23,\n24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36\n[32] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and\nA. Lerer. Automatic differentiation in pytorch. InICLR Workshop, 2017. 3\n[33] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,\nL. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner,\nL. Fang, J. Bai, and S. Chintala. Pytorch: An imperative style, high-performance deep learning library. In\nNeurIPS. 2019. 15, 16\n[34] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin,\nJ. Clark, et al. Learning transferable visual models from natural language supervision. InICML, 2021. 3,\n5, 16, 17, 18\n[35] J. Resig. Ukiyo-e search.https://ukiyo-e.org/, 2012. 16\n[36] E. Riba, D. Mishkin, D. Ponsa, E. Rublee, and G. Bradski. Kornia: an open source differentiable computer\nvision library for pytorch. InWACV, 2020. 16\n[37] C. Ryali, Y .-T. Hu, D. Bolya, C. Wei, H. Fan, P.-Y . Huang, V . Aggarwal, A. Chowdhury, O. Poursaeed,\nJ. Hoffman, et al. Hiera: A hierarchical vision transformer without the bells-and-whistles. InICML, 2023.\n3\n[38] S. Sagawa, P. W. Koh*, T. B. Hashimoto, and P. Liang. Distributionally robust neural networks. InICLR,\n2020. 5, 8, 16\n[39] S. Sagawa, P. W. Koh, T. Lee, I. Gao, S. M. Xie, K. Shen, A. Kumar, W. Hu, M. Yasunaga, H. Marklund,\nS. Beery, E. David, I. Stavness, W. Guo, J. Leskovec, K. Saenko, T. Hashimoto, S. Levine, C. Finn, and\nP. Liang. Extending the wilds benchmark for unsupervised adaptation. InICLR, 2022. 16\n[40] B. Son, Y . Oh, D. Baek, and B. Ham. Fyi: Flip your images for dataset distillation. InECCV, 2024. 1, 3\n[41] Q. Sun, Y . Fang, L. Wu, X. Wang, and Y . Cao. Eva-clip: Improved training techniques for clip at scale.\narXiv preprint arXiv:2303.15389, 2023. 3\n13\n[42] Y . Tian, D. Krishnan, and P. Isola. Contrastive multiview coding. InECCV, 2020. 5, 16, 17, 18, 19, 20, 21\n[43] TorchVision maintainers and contributors. Torchvision: Pytorch\u2019s computer vision library. https:\n//github.com/pytorch/vision, 2016. 16\n[44] M. Tschannen, A. Gritsenko, X. Wang, M. F. Naeem, I. Alabdulmohsin, N. Parthasarathy, T. Evans,\nL. Beyer, Y . Xia, B. Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic\nunderstanding, localization, and dense features.arXiv preprint arXiv:2502.14786, 2025. 3\n[45] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. Caltech-UCSD birds 200. Technical Report\nCNS-TR-2011-001, California Institute of Technology, 2011. 5, 9, 17, 33, 34\n[46] K. Wang, B. Zhao, X. Peng, Z. Zhu, S. Yang, S. Wang, G. Huang, H. Bilen, X. Wang, and Y . You. Cafe:\nLearning to condense dataset by aligning features.CVPR, 2022. 1\n[47] T. Wang, J.-Y . Zhu, A. Torralba, and A. A. Efros. Dataset distillation.arXiv preprint arXiv:1811.10959,\n2018. 1, 2\n[48] R. Wightman. Pytorch image models.https://huggingface.co/timm, 2019. 16\n[49] WikiArt.org. WikiArt Visual Art Encyclopedia.https://www.wikiart.org/, 2024. 16\n[50] Z. Yin and Z. Shen. Dataset distillation via curriculum data synthesis in large data era. 2024. 2\n[51] Z. Yin, E. Xing, and Z. Shen. Squeeze, recover and relabel: Dataset condensation at imagenet scale from a\nnew perspective. InNeurIPS, 2023. 1, 2\n[52] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer. Sigmoid loss for language image pre-training. InICCV,\n2023. 3\n[53] B. Zhao and H. Bilen. Dataset condensation with differentiable siamese augmentation. InICML, 2021. 1,\n4, 7\n[54] B. Zhao and H. Bilen. Dataset condensation with distribution matching.WACV, 2023. 1, 2\n[55] B. Zhao, K. R. Mopuri, and H. Bilen. Dataset condensation with gradient matching. InICLR, 2020. 2, 3, 4\n[56] Y . Zhou, E. Nezhadarya, and J. Ba. Dataset distillation using neural feature regression.NeurIPS, 2022. 1, 2\n14\nAppendix\nTo begin the appendix, we first speak on our method\u2019s limitations, broader impact, and compute\nbudget. In the following sections, we include more information that was omitted from the main\npaper due to space constraints. In Section A, we provide a more detailed description of our method\u2019s\nimplementation details. In Section C, we include visualizations of complete distilled datasets.\nLimitationsOur method is largely limited by memory and data loading. For example, we were lim-\nited in our ImageNet-1k experiments to only using 3 rounds of augmentations per batch due to a data\nloading bottleneck. We hope to eventually develop a method that does not require the loading of thou-\nsands of real images per step. Additionally, due to the bi-level optimization, we are limited to using Py-\ntorch\u2019s [ 2,33]nn.DistributedParallel , rather than nn.DistributedDataParallel ,\nwhich causes significant slowdowns due to our very large batch sizes. Using a different automatic\ndifferentiation framework, such as Jax [ 4], might alleviate this issue, but this would also require\nporting all our self-supervised backbones to the new framework.\nBroader ImpactWhile our method quantitatively out-performs the real image baselines, the\nbroader impacts likely lie in the introduction of this new task as well as the resulting images acting as\nan interpretability tool. As shown in the paper, the distilled images offer insights into things such as\nmodel alignment and how different networks see the world in varying ways.\nFurthermore, the ultimate goal of dataset distillation is the reduction of training time, which equates\nto less energy spent. A linear probe trained on our distilled datasets can be trained to convergence in\njust a few minutes while training on the full dataset can take up to an entire day.\nCompute BudgetWe used a variety of GPUs for this work depending on what was available on\nthe shared cluster. Specifically, we used a combination of H200, A100, L40s, Ada6000, and 4090\nGPUs. Distilling ImageNet-100 with the default settings takes about 3 hours using 1 H200 GPU, and\nImageNet-1k takes about 12 hours using 4 H200 GPUs.\nA Implementation Details\nIn this section, we provide a more detailed explanation of our methodological and experimental\nimplementation details.\nA.1 Distillation\nWe implement our method in Pytorch [2, 33], which has a unique license to which we comply.\nWe optimize our pyramid representations using Adam [22] with a learning rate of 0.002.\nWe distill for 5000 iterations and add a new level to the pyramids every 200 iterations until we reach\nthe maximum resolution (256).\nEach level of the pyramid is initialized with N(0,1) normalized by the current number of levels in\nthe pyramid. When a new level is added, the existing levels are appropriately re-normalized.\nAt each distillation iteration, we first sample a new linear classifier consisting of a weight matrix W\nand bias vector busing the default initialization for Pytorch\u2019s nn.Linear . For simplicity, we omit\nthe bias in the main paper.\nThen, we reconstruct the synthetic images from their pyramid representations using the following\nequation (copied from the main paper for convenience):\nX=1\n2+1\n2tanh X\nr\u2208\u03c1resize 256(Pr)!\n(4)\nThe synthetic images are then augmented several (10 by default)different(not sequential) times and\nconcatenated together to form the full synthetic batch. We obtain our synthetic loss ( \u2113syn) by passing\nthis batch through the feature extractor \u03d5and linear classifier (W,b) and then compute the gradient of\nthe synthetic loss with respect to the linear classifier (and bias). These gradients are then vectorized.\n15\nFor our real batch, we sample a number of real images equal to the length of thefullsynthetic batch\n(i.e., after the different augmentations and concatenation) and augment them once (since the batch\nsize already matches that of the full synthetic batch). We then get the real loss and gradients via the\nsame procedure. The meta loss is then calculated as the cosine distance between the real and synthetic\nvectorized gradients.\nA.2 Augmentation\nFor our differentiable augmentations, we use a custom re-implementation of torchvision trans-\nforms that run on GPU and support different random transforms for each element in a batch. We ini-\ntially used the Kornia [ 36] implementations, but found that they cause CPU bottlenecks with very large\nbatch sizes. We use RandomHorizontalFlip with default parameters, RandomResizedCrop\nwithsize=(224, 224), andRandomGaussianNoisewithstd=0.2.\nA.3 Evaluation\nLinear Probes.To evaluate a given distilled dataset (or selected coreset for the real baselines) on\na given target model, we first randomly initialize a new linear classifier. We then train the linear\nclassifier for 1000 epochs with a batch size of 100. We use an Adam optimizer with a learning rate of\n0.001/256 (taken from the DINO-v1 [ 5] evaluation protocol) along with a cosine decay learning rate\nschedule. We stop training early if the test accuracy has not improved over the last 50 epochs. For the\ntraining set, we perform the same set of augmentations as during distillation (horizontal flip, random\nresized crop, and Gaussian noise). The output of the random resized crop is of size 224 \u00d7224. For the\ntest set, we resize the shortest side to 256 and then do a center crop of 224\u00d7224.\nNearest Neighbors.For our nearest neighbor evaluation, we first find the train set embeddings\nby resizing the shortest side of each test image to 256, taking a 224 \u00d7224 center crop, and passing\nthrough the feature extractor. We then do the same for the test set and find the training embedding\nclosest to each test embedding (by cosine distance) and report the class of said neighbor. We only do\none nearest neighbor since we distill down to one image per class.\nA.4 Datasets\nFor ImageNet [ 12] and ImageNet [ 42], we build on the Torchvision [ 43] implementation as part of the\nPytorch [ 2,33] ecosystem. ImageNet has a unique non-commercial license, to which we have agreed.\nSpawrious [26] is taken from the paper\u2019s official GitHub repository and uses a CC0-1.0 license.\nWaterbirds [ 38] is taken from the WILDS [ 23,39] benchmark. Both the Waterbirds and WILDS\nrepositories use an MIT license.\nStanford Dogs [ 21] is taken from the project website. It presumably uses the same license as\nImageNet [12].\nCaltech-UCSD-200-2011 is taken from the project website. No license is given.\nArtBench [ 24] is taken from the paper\u2019s GitHub repository. The repository itself uses an MIT license.\nThe data is sourced from WikiArt [ 49], Ukiyo-e Search [ 35], and The Surrealism Website [ 1], all of\nwhich have Fair Use licenses.\nFlowers-102 [29] is taken from Torchvision. No license is given.\nFood-101 [3] is taken from Torchvision. No license is given.\nA.5 Models\nFor CLIP [34], we use the official GitHub repository which has an MIT license.\nFor DINO-v1 [5], we use the official GitHub repository which has an Apache-2.0 license.\nFor DINO-v2 [31], we use the official GitHub repository which has an Apache-2.0 license.\nFor EV A-02 [ 13], we use the Pytorch Image Models [ 48] implementation hosted on Hugging Face [ 19]\nwhich has an MIT license.\n16\nTrain Set\n(1 Img/Cls)Flowers-102 Food-101\nCLIP DINO-v2 EV A-02 MoCo-v3 Average CLIP DINO-v2 EV A-02 MoCo-v3 Average\nDistilled (Ours)79.5\u00b11.299.6\u00b10.097.8\u00b10.156.4\u00b13.183.3\u00b11.178.7\u00b11.683.7\u00b10.383.9\u00b10.231.6\u00b13.269.5\u00b11.3\nNeighbors 69.4\u00b10.899.4\u00b10.290.8\u00b10.248.6\u00b11.877.0\u00b10.758.8\u00b11.373.8\u00b10.471.5\u00b10.325.2\u00b11.357.3\u00b10.8\nCentroids 77.5\u00b10.799.4\u00b10.195.6\u00b10.348.2\u00b12.680.2\u00b10.974.1\u00b10.675.6\u00b10.176.4\u00b10.422.7\u00b12.062.2\u00b10.8\nRandom 67.9\u00b11.198.8\u00b10.491.0\u00b10.836.8\u00b12.273.6\u00b11.148.5\u00b11.052.8\u00b11.949.9\u00b10.613.5\u00b11.741.2\u00b11.3\nFull Dataset 93.4\u00b10.199.7\u00b10.098.9\u00b10.082.2\u00b10.193.5\u00b10.191.9\u00b10.092.7\u00b10.192.0\u00b10.078.3\u00b10.088.7\u00b10.0\nTable B.1:Additional Datasets:Distilling Flowers-102 [ 29] and Food-101 [ 3] show similar results; our distilled\ndataset consistently out-perform the real-image baselines.\nFor MoCo-v3 [ 10], we use the official GitHub repository which has a Creative Commons Attribution-\nNonCommercial 4.0 International Public License.\nB Additional Results\nIn Table B.1, we include results for Flowers-102 [ 29] and Food-102 [ 3]. We see results following the\nsame trends as the rest of the paper; the distilled datasets out-perform the real-image baselines across\nthe board.\nC Visualizing Full Datasets\nIn Figures C.1, C.2, C.3, and C.4 we present the entirety of ImageNet-100 [ 42] distilled with\nCLIP [ 34], DINO-v2 [ 31], EV A-02 [ 14], and MoCo-v3 [ 10] respectively. In Figures C.5\u2013C.14, we\nshow ImageNet-1k [ 12] distilled with DINO-v2 and omit the other models for the sake of brevity\n(and ink).\nWe also include Stanford Dogs [ 21] (Figure C.15), CUB-200-2011 [ 45] (Figures C.16 and C.17),\nFlowers-102 [ 29] (Figure C.18), and Food-101 [ 3] (Figure C.19) distilled with DINO-v2 and once\nagain omit the other models. All distilled datasets, including those not inculded here, can be viewed\non our project page.\n17\nFigure C.1: ImageNet-100 [42] distilled using CLIP [34]\n18\nFigure C.2: ImageNet-100 [42] distilled using DINO-v2 [31]\n19\nFigure C.3: ImageNet-100 [42] distilled using EV A-02 [13]\n20\nFigure C.4: ImageNet-100 [42] distilled using MoCo-v3 [10]\n21\nFigure C.5: ImageNet-1k [12] distilled with DINO-v2 [31] classes [0-99]\n22\nFigure C.6: ImageNet-1k [12] distilled with DINO-v2 [31] classes [100-199]\n23\nFigure C.7: ImageNet-1k [12] distilled with DINO-v2 [31] classes [200-299]\n24\nFigure C.8: ImageNet-1k [12] distilled with DINO-v2 [31] classes [300-399]\n25\nFigure C.9: ImageNet-1k [12] distilled with DINO-v2 [31] classes [400-499]\n26\nFigure C.10: ImageNet-1k [12] distilled with DINO-v2 [31] classes [500-599]\n27\nFigure C.11: ImageNet-1k [12] distilled with DINO-v2 [31] classes [600-699]\n28\nFigure C.12: ImageNet-1k [12] distilled with DINO-v2 [31] classes [700-799]\n29\nFigure C.13: ImageNet-1k [12] distilled with DINO-v2 [31] classes [800-899]\n30\nFigure C.14: ImageNet-1k [12] distilled with DINO-v2 [31] classes [900-999]\n31\nFigure C.15: Stanford Dogs [21] distilled using DINO-v2 [31]\n32\nFigure C.16: CUB-200-2011 [45] distilled using DINO-v2 [31] classes [0-99]\n33\nFigure C.17: CUB-200-2011 [45] distilled using DINO-v2 [31] classes [100-200]\n34\nFigure C.18: Flowers-102 [29] distilled using DINO-v2 [31]\n35\nFigure C.19: Food-101 [3] distilled using DINO-v2 [31]\n36",
      "summary": "This paper introduces a novel dataset distillation method, Linear Gradient Matching, tailored for pre-trained self-supervised vision models. The approach enables the creation of small synthetic datasets that effectively train linear probes, achieving performance comparable to training on large real datasets.",
      "key_contributions": [
        "Proposes Linear Gradient Matching, a new dataset distillation method for pre-trained vision models.",
        "Demonstrates that distilled synthetic datasets can outperform real-image baselines for training linear probes.",
        "Shows that the distilled datasets generalize across different pre-trained models (e.g., CLIP, DINO).",
        "Provides evidence that the method is particularly effective for fine-grained classification and model interpretability."
      ],
      "methodology": "The method optimizes synthetic images so that, when processed by a pre-trained feature extractor, they induce gradients in a linear classifier similar to those produced by real data, enabling effective linear probe training.",
      "results": "The distilled datasets outperform real-image baselines, generalize across different pre-trained models, and are especially effective for fine-grained classification and model interpretability tasks.",
      "keywords": [
        "dataset distillation",
        "self-supervised learning",
        "vision models",
        "linear probes",
        "synthetic data",
        "gradient matching",
        "model interpretability"
      ],
      "status": "new",
      "notes": null,
      "created_at": "2025-11-23T20:13:57.789973",
      "needs_llm_processing": false
    },
    "3": {
      "id": "2511.16672v1",
      "title": "EvoLMM: Self-Evolving Large Multimodal Models with Continuous Rewards",
      "authors": [
        {
          "name": "Omkat Thawakar",
          "affiliation": null
        },
        {
          "name": "Shravan Venkatraman",
          "affiliation": null
        },
        {
          "name": "Ritesh Thawkar",
          "affiliation": null
        },
        {
          "name": "Abdelrahman Shaker",
          "affiliation": null
        },
        {
          "name": "Hisham Cholakkal",
          "affiliation": null
        },
        {
          "name": "Rao Muhammad Anwer",
          "affiliation": null
        },
        {
          "name": "Salman Khan",
          "affiliation": null
        },
        {
          "name": "Fahad Khan",
          "affiliation": null
        }
      ],
      "abstract": "Recent advances in large multimodal models (LMMs) have enabled impressive reasoning and perception abilities, yet most existing training pipelines still depend on human-curated data or externally verified reward models, limiting their autonomy and scalability. In this work, we strive to improve LMM reasoning capabilities in a purely unsupervised fashion (without any annotated data or reward distillation). To this end, we propose a self-evolving framework, named EvoLMM, that instantiates two cooperative agents from a single backbone model: a Proposer, which generates diverse, image-grounded questions, and a Solver, which solves them through internal consistency, where learning proceeds through a continuous self-rewarding process. This dynamic feedback encourages both the generation of informative queries and the refinement of structured reasoning without relying on ground-truth or human judgments. When using the popular Qwen2.5-VL as the base model, our EvoLMM yields consistent gains upto $\\sim$3\\% on multimodal math-reasoning benchmarks, including ChartQA, MathVista, and MathVision, using only raw training images. We hope our simple yet effective approach will serve as a solid baseline easing future research in self-improving LMMs in a fully-unsupervised fashion. Our code and models are available at https://github.com/mbzuai-oryx/EvoLMM.",
      "arxiv_id": "2511.16672v1",
      "arxiv_url": "http://arxiv.org/abs/2511.16672v1",
      "pdf_url": "https://arxiv.org/pdf/2511.16672v1",
      "published": "2025-11-20T18:59:54Z",
      "updated": "2025-11-20T18:59:54Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "full_text": "EvoLMM: Self-Evolving Large Multimodal Models with\nContinuous Rewards\nOmkar Thawakar1\u2020Shravan Venkatraman1\u2020Ritesh Thawkar1\u2020Abdelrahman M Shaker1\nHisham Cholakkal1Rao Muhammad Anwer1,3Salman Khan1,3Fahad Khan1,4\n1Mohamed bin Zayed University of AI,2Aalto University,\n3Australian National University,4Link \u00a8oping University\n/enve\u2640e{omkar.thawakar, shravan.venkatraman, ritesh.thawkar}@mbzuai.ac.ae\nhttps://mbzuai-oryx.github.io/EvoLMM/\nAbstract\nRecent advances in large multimodal models (LMMs) have\nenabled impressive reasoning and perception abilities, yet\nmost existing training pipelines still depend on human-\ncurated data or externally verified reward models, limiting\ntheir autonomy and scalability. In this work, we strive to\nimprove LMM reasoning capabilities in a purely unsuper-\nvised fashion (without any annotated data or reward distil-\nlation). To this end, we propose a self-evolving framework,\nnamed EvoLMM, that instantiates two cooperative agents\nfrom a single backbone model: a Proposer, which generates\ndiverse, image-grounded questions, and a Solver, which\nsolves them through internal consistency, where learning\nproceeds through a continuous self-rewarding process. This\ndynamic feedback encourages both the generation of in-\nformative queries and the refinement of structured reason-\ning without relying on ground-truth or human judgments.\nWhen using the popular Qwen2.5-VL as the base model,\nour EvoLMM yields consistent gains upto\u223c3% on mul-\ntimodal math-reasoning benchmarks, including ChartQA,\nMathVista, and MathVision, using only raw training im-\nages. We hope our simple yet effective approach will serve\nas a solid baseline easing future research in self-improving\nLMMs in a fully-unsupervised fashion. Our code and mod-\nels are available athttps://github.com/mbzuai-\noryx/EvoLMM.\n1. Introduction\nLarge multimodal models (LMMs) that jointly handle vi-\nsion and language have made substantial progress in recent\nyears. They excel at tasks like image captioning [14, 16, 22,\n\u2020Equal contribution.\nQwen2.5-VL-7B (Baseline): \u201cB\u201dQwen2.5-VL-7B (Ours) : \u201cC\u201d\nQuestion: Which point in the labyrinth can we get to, starting at point $O$?\nFrom raw images to reasoning -> our model evolves through self-play.\n\u201cProposer\u201d\n\u201cSolver\u201d1.In a different circular maze, starting at S, which labeled goal is reachable without crossing walls?2.Another maze: from the center node, which exit (A/B/C/D) can be reached?3.New maze: starting at K, which labeled point is not reachable?4.Different maze: which destination from the start requires passing through exactly one gate?5.\u2026\u2026\nProposing questionsTrying to solve questionsInference\nContinuous Reward-based Self Play Training\nFigure 1.Illustration of our fully unsupervised self-evolving\nLMM framework (EvoLMM).Our EvoLMM enables a base\nLMM to improve its reasoning abilitywithout any human labels,\nmetadata, or external reward models. Given only raw images, a\nproposerfirst generates visually grounded questions, and asolver\nattempts to answer them multiple times. The degree of agreement\namong solver responses produces acontinuous self-consistency re-\nward, forming a closed-loop training signal that drives both mod-\nules to co-evolve.\n29], visual question answering [13, 28, 42, 51], and multi-\nmodal reasoning [30, 34, 37, 43, 46] by leveraging large-\nscale pre-training and fine-tuning on annotated datasets.\nHowever, despite these advances, two fundamental depen-\ndencies remain: (i) the dependence on human-curated an-\nnotations or metadata [1, 8, 11, 19, 50], and (ii) the de-\npendency on externally defined reward models or evalua-\ntors (e.g., human-labeled correctness, knowledge-distilled\nreward networks) [4, 12, 17, 32, 36]. These dependencies\n1arXiv:2511.16672v1  [cs.CV]  20 Nov 2025\npose scalability limitations, hinder domain generalization,\nand restrict deployment in settings where annotations or re-\nward models are unavailable.\nRecent research in large language models (LLMs) ex-\nplores the idea of self-evolving models that not only solve\ntasks but also generate the tasks they learn from. For in-\nstance, few works [5, 6] aim to improve base LLM by gen-\nerating its own questions and attempting to answer them,\nusing the agreement among multiple sampled answers as\na self-reward signal. Such a technique can be further im-\nproved [6] by splitting the model into interacting roles\nsuch asProposer,Solver, and Judge that co-exist and train\nthemselves through reinforcement learning. This research\ndirection is pushed further [45] by incorporating open-\nended scheme where the system incorporates an open-ended\nevolution to continually refine internal behaviors. How-\never, these research efforts are largely constrained to the\nlanguage-onlydomain, where reasoning is explicit and cor-\nrectness can be inferred through textual consistency or sym-\nbolic execution. Moreover, these approaches still either rely\non implicit evaluators (e.g., external Judge module, exe-\ncution feedback) or explicit evaluators, implying that the\nlearning signal is not yet fully free of external priors.\nIn the multimodal (LMM) domain, few recent works\naim to improve LMMs through self-rewarding training sig-\nnals, thereby reducing human supervision. For instance, the\nwork of [18] proposes a self-rewarding reasoning strategy\nwhere the model decomposes each task into a perception\nstep and a reasoning step, and then evaluates with internal\nbootstrapping. Similarly, ViPER [47] introduces a closed-\nloop framework that gradually strengthens visual perception\nability through image-level and instance-level reconstruc-\ntion, combined with reinforcement style updates driven by\ninternally synthesized data. While these approaches rep-\nresent progress in self-improving LMMs, they still rely on\nstructured intermediate signals such as reconstruction ob-\njectives, heuristic quality filtering, or semantic similarity-\nbased scoring. Consequently, their self-evolution remains\npartially guided by external priors rather than emerging\npurely from the model\u2019s own reasoning dynamics.\nIn this work, we investigate a more fundamental form of\nautonomy:Can a large multimodal model (LMM) enhance\nits own reasoning capability without relying on human-\nannotated supervision or any external reward signals?To\nthis end, we propose a purely unsupervised self-evolving\nframework, named EvoLMM, that learns entirely through\ninternal consistency (see Figure. 1). Our approach in-\nstantiates two cooperative roles from a single backbone\nmodel: aProposer, which generates diverse and visually\ngrounded mathematical questions from unlabeled images,\nand aSolver, which attempts to solve these questions. The\nframework evolves through a continuous self-reward mech-\nanism, where theSolveris rewarded by the fraction of con-sistent or correct answers within its own generations. It is\nworth mentioning that no ground-truth labels, metadata, or\nexternal reward models are used at any stage. This simple\nyet effective design enables the model to construct, eval-\nuate, and refine its own training signal directly from raw\nmultimodal inputs. The framework naturally scales to large\nmodels and domains where ground-truth supervision is un-\navailable or expensive. Despite training exclusively on raw\nimages, we show consistent improvements across multi-\nple multimodal math reasoning benchmarks over the base\nmodel. Further, our qualitative analysis reveals that thePro-\nposerlearns to generate increasingly complex visual prob-\nlems, while theSolverdevelops more structured reasoning\nchains over time, suggesting emergent self-evolution.\nIn summary, our main contributions are:\n\u2022 We introduce a self-evolving multimodal framework,\nnamed EvoLMM, that enables a base LMM to improve\nwithout human labels, metadata, or external reward mod-\nels. The framework decomposes the model into two in-\nternal roles,ProposerandSolver, forming a closed-loop\npropose-solve cycle trained solely through internal con-\nsistency feedback.\n\u2022 We develop a continuous self-rewarding mechanism\nbased on multi-sample answer consistency, which re-\nplaces both learned discrete reward models and seman-\ntic similarity scoring used in prior LMM self-evolution\napproaches. This continuous internal reward signal pro-\nvides smooth gradients and stable optimization, enabling\nconsistent improvement in performance.\n\u2022 We empirically validate EvoLMM on mathematical vi-\nsual reasoning benchmarks, with absolute gains of\u223c\n2\u20133%over the Qwen-2.5-VL-7B baseline using only raw\nimages during training. We further analyze the evolution\nof our propose-solve mechanism where the difficulty level\ngradually progresses and maintains stable learning, show-\ning that the model naturally develops more structured and\ngrounded reasoning behaviors over time. Furthermore,\nwe show that internal consistency can serve as a viable\nsupervision signal for open-ended multimodal learning.\n2. Related Work\nIn the context of large language models (LLMs), several\nrecent works [5, 6, 31] aim to self-improve the model capa-\nbilities (e.g., scientific reasoning) without significant human\nannotations. The work of [5] proposes an approach to im-\nprove the implicit reasoning capabilities of the base model\nthrough self-play and majority-vote reward signals. Sheikh\nShafayat et al. [31] explores self-consistency as a train-\ning reward within an reinforcement learning (RL) frame-\nwork, demonstrating performance on synthetic reasoning\ntask. Recently, the work of [6] introduces a self-play based\nmethod where three roles (Proposer,Solver, andJudge) are\ninstantiated from the same base LLM and trained via RL to\n2\nconstruct questions, answers, and the evaluation inter-play\nbetween them. While achieving promising results, these ap-\nproaches assume a certain form of verifiable environment\nin the form of a judge [6] or heuristic and discrete rewards\n(e.g., majority vote) [5, 31] which can likely struggle with\nmodel collapse.\nIn the context of large multimodal models (LMMs), Li\net al. [18] introduces a self-rewarding technique to improve\nvisual understanding by decomposing it into perception and\nlanguage-reasoning via explicit perception reward by the\nLMM itself. However, the self-reward mechanism relies\non the SFT cold-start dataset to learn the desired format.\nThe work of Zhang et al. [47] leverages image-level and\ninstance-level reconstructions in a two-stage RL loop to\niteratively improve perceptual capabilities. However, this\nimage-and instance-level reconstruction of meta-data relies\non an external model (e.g., OmniGen2 [40] and Qwen-\nImage [39]). Additionally, a recent work Vision-Zero [36],\ntrains LMMs through a multi-agent social deduction game\n(\u201cWho is the Spy?\u201d) played directly on images. The frame-\nwork introduces adversarial roles (spy vs. civilians), iter-\native clue and voting rounds, and alternates between self-\nplay and verifiable reinforcement learning (RLVR) depend-\ning on whether the identity of the spy is known. Although\nVision-Zero does not require explicit human annotations, it\nrelies on constructing image pairs with subtle, controlled\ndifferences produced through automated rendering, image\nediting, or regeneration pipelines relying on external gener-\native systems (GPT-4o [27] and Gemini [7]).\nOur Approach.Different to aforementioned approaches,\nour approach aims to self-improve the multimodal capabili-\nties (e.g., multimodal math reasoning) of a base LMMwith-\noutany annotations and external reward-model based self-\nevolution loop. Specifically, our framework uses only raw\nimages (without metadata, bounding-boxes, and human la-\nbels) and employs aProposer\u2013Solverpair where (i) thePro-\nposergenerates questions from images and (ii) theSolver\nattempts answers over multiple attempts, where the reward\nsignal is the fraction of correct answers among the model\u2019s\nown outputs, effectively scaling with the degree of answer\nagreement. In this way, the proposed scheme avoids exter-\nnal judge, human annotated datasets (e.g., SFT cold-start),\nknowledge-distillation, and discrete rewards. By leveraging\nacontinuous rewarddesign, our training process systemat-\nically induces a curriculum, where theProposergradually\nexplores questions that are neither trivial nor impractical,\nthereby stabilizing theSolverlearning and mitigating the\nProposerto degenerate to zero-rewards.\n3. Method\nProblem Formulation.We consider theunsupervised\nself-evolvingvision\u2013language (multimodal) reasoning set-\nting, where no question\u2013answer annotations, metadata, orexternal reward models are available. LetX={x}denote\nan unlabeled collection of images. Our goal is to enable\na single pre-trained base large multimodal model (LMM)\nimprove itself by generating its own training signal directly\nfrom these images. To this end, we instantiate two policies:\n\u2022Proposer\u03c0 \u03d5(q|x)that produces a visually grounded\nquestionqfor imagex, and\n\u2022Solver\u03c0 \u03b8(y|x, q)that attempts to answerq.\nFor each proposed question, we drawNindependent an-\nswer samplesy 1:N\u223c\u03c0\u03b8(\u00b7|x, q)and compute the empirical\nanswer distributionp(a|x, q). Let\u02c6a= arg max ap(a|x, q)\ndenote the majority answer, and let\nH(x, q) =\u2212X\nap(a|x, q) logp(a|x, q)(1)\nbe theSolver-consensus entropy. This quantity serves as a\nproxy for question difficulty: trivial questions lead to low\nentropy, whereas ambiguous or poorly grounded questions\nproduce high entropy.\n3.1. Unsupervised Self-Evolving LMM\nIn this work, we design an unsupervised self-evolving\nLMM approach (EvoLMM) motivated by the recent self-\nquestion-based technique (SQLM) [5] in the LLM domain.\nThe SQLM improves an LLM by generating its own ques-\ntions and reinforcing answer agreement. However, SQLM\nrelies on adiscretemajority-vote reward, which we em-\npirically show to be sub-optimal in the multimodal setting\nwhere the LMM is required to ground its questions based\non the input image and perform complex multimodal rea-\nsoning (e.g., visual math reasoning). We observe that early-\nstageSolveroutputs over images are highly variable, lead-\ning to frequent zero-reward updates and unstable optimiza-\ntion. This prevents the model from making gradual progress\nwhen using discrete majority-vote reward.\nIn our EvoLMM approach, we replace the discrete re-\nward with acontinuous self-consistencysignal that scales\nwith the degree of answer-agreement, as shown in Figure 2.\nThe continuous self-consistency signal provides meaningful\ngradient feedback even when visual reasoning is only par-\ntially correct. We observe this to stabilize the learning in the\npresence of visual uncertainty that requires domain knowl-\nedge. Our continuous self-consistency reward enables the\nProposerandSolverto co-evolve smoothly, forming a cur-\nriculum in which question difficulty and reasoning quality\nimprove gradually using only raw images.\nContinuous Self-Consistency Reward.We reward the\nSolverusing a continuous self-supervised signal that scales\nsmoothly with self-consistency and gently penalizes ver-\nbosity. For each sampley i, we define\nrsol\ni=\u0000\np(yi|x, q)\u0001\u03b3\u00b7\u0010\n1\u2212\u03bb len\u00b7max{0,(w i\u2212\u03c4)/\u03c4}\u0011\n(2)\n3\n\u2018\u2019\u2019 triangle ABC$ is an isosceles triangle with base $BC$. What is the perimeter of the triangle?\u2018\u2019\u2019\n\u2019\u2019\u2019 Generate Question for given <image> \u2018\u2019\u2019\n\u201cProposer\u201d\n\u201cSolver\u201d\n\u2018\u2019\u2019triangle ABC$ is an isosceles triangle with base $BC$. What is the perimeter of the triangle?\u2018\u2019\u2019\u2018\u2019\u2019A1: 38A2: 40A3: 42A4: 40A5: 36\u2018\u2019\u2019AnswerCount0.00.51.01.52.0\n38404236Answer Histogram (N=5)exp\u2212\ud835\udc3b\ud835\udc65,\ud835\udc5e\u2212\ud835\udf07!\"2\ud835\udf0e!\"\u201cProposer-Reward (\ud835\udc5f!\"#!)\u201d\n\u201cSolver-Reward (\ud835\udc5fsol)\u201d00.5101Entropy H (nats)\ud835\udc5d\ud835\udc66#\u2009\u2009\ud835\udc65,\ud835\udc5e\\\ud835\udc4f\ud835\udc56\ud835\udc54$\u22c51\u2212\ud835\udf06len\u22c5max{0,\ud835\udc64#\u2212\ud835\udf0f/\ud835\udf0f}\u201cq\u201d\n\u201d\ud835\udc66$:&\u201d\u201cInput-Image\u201d\n\u201cInput-Image\u201d\u201cq\u201d\n\ud835\udc5d\ud835\udc4e\u2009\u2009\ud835\udc65,\ud835\udc5eFigure 2.Overview of ourProposer\u2013Solverbased self-evolving framework (EvoLMM).Given only a raw visual input (e.g., multimodal\nchart), theProposermodule generates a questionqabout the image content. TheSolverthen produces multiple answer samplesy 1:N,\nforming an empirical answer distributionp(a|x, q). TheSolver rewardrsolis a continuous, self-supervised signal based on the likelihood\nof each answer sample, modulated by a length penalty that constrains theSolver\u2019sresponse format. TheProposer rewardrpropis an\nentropy-based band-pass function that encourages moderate difficulty questions whereSolveris not completely correct and certain. By\nrewarding this moderate-entropy window, theProposergradually learns to generate questions that are challenging enough to stimulate\nreasoning while remaining solvable, forming an automatic curriculum without external supervision. Both modules are optimized with\nstandard REINFORCE objectives regularized by token-level KL constraints to reference policies. This closed-loop training enables jointly\nrefining question generation and reasoningusing only images, without any annotated Q&A pairs, discrete rewards, or external verifiers.\nwherep(y i|x, q)is theSolver\u2019sown agreement score,\n\u03b3\u2208(0,1]controls reward softness (lower values accentu-\nate mid-confidence differences),w icounts the number of\nwords preceding the<answer>tag, and\u03c4is target brevity\nthreshold. This yields dense, bounded rewards that preserve\ngradient flow even without clear majority, encouraging both\nstable reasoning(high agreement among outputs) andcon-\ncise format, as shown in Figure 4. Unlike discrete majority-\nvote rewards, it provides a continuous signal that better sep-\narates mid-probability outcomes (see Figure 3).\nEntropy-Guided ContinuousProposerReward.To avoid\ntrivial or unsolvable questions, theProposerreceives a\nsmooth band-pass reward based on theSolver\u2019sanswer en-\ntropy:\nrprop= exp\u0012\n\u2212(H(x, q)\u2212\u00b5 H)2\n2\u03c32\nH\u0013\n(3)\nwhich peaks when theSolver\u2019suncertainty lies within a\nmoderate range\u2014neither overly certain (H\u22480, trivial or\nleading) nor overly uncertain (ambiguous or unsolvable), as\nshown in Figure 3. This encourages theProposerto operate\nnear theSolver\u2019sdecision boundary, inducing an adaptive\ncurriculum: as theSolverimproves, theProposermust gen-\nerate slightly harder yet still solvable questions to remain\nwithin the entropy band (see Figure 6).\nTraining Optimization.In our framework, theProposer\nandSolverare trained jointly in an online reinforcement-\nlearning loop. Since no ground-truth labels or external scor-\ning functions are available, both policies rely entirely on in-\nternally generated reward signals. To ensure stable learning,we adopt a REINFORCE policy gradient [38] with two key\ncomponents: (i) moving-average baselines to reduce vari-\nance, and (ii) token-level KL regularization to monitor de-\nviation from the pretrained base LMM.\nBoth policies share the same optimization form,\nLA=\u2212E\"\n(rA\u2212bA)1\nTATAX\nt=1log\u03c0 A(yt|ht)#\n+\u03b2AKLA,\n(4)\nwhereA\u2208 {Solver,Proposer},h tdenotes the text\u2013image\ncontext up to tokentwhereas,\nKLA=1\nTAX\ntKL\u0000\n\u03c0A(\u00b7 |h t)\u2225\u03c0ref\nA(\u00b7 |h t)\u0001\n(5)\nmeasures per-token divergence from a frozen reference\nmodel.\nEach policy employs an exponential moving-average base-\nlineb Afor variance reduction and a dynamicKL controller\nthat adjusts\u03b2 Ato maintain a target divergence budget:\n\u03b2A\u2190clip\u0010\n\u03b2A\u00b7exp\u0000\n\u03b7KLA\u2212\u03c4A\n\u03c4A\u0001\n, \u03b2min, \u03b2max\u0011\n.(6)\nFor theSolver, this controller stabilizes the learning while\nallowing gradual adaptation towards better visual reason-\ning and consistent answer generation. For theProposer,\nthe KL term is not tightly constrained: as it explores new\nquestion spaces, its divergence from the backbone (base\nLMM) increases, reflecting the emergence of more chal-\nlenging queries. Please refer to Algorithm 1 for our end-\nto-end training optimization process.\n4\n\u0006\u0007\b\t\n\u0005\u0004\u0005\u0005\u0004\u0007\u0005\u0004\t\u0005\u0004\u000b\u0005\u0004\f\u0006\u0004\u0005\n\u0006\u0007\b\t\n\u0005\u0004\u0005\u0005\u0004\u0007\u0005\u0004\t\u0005\u0004\u000b\u0005\u0004\f\u0006\u0004\u0005\n\u0006\u0007\b\t\n\u0005\u0004\u0005\u0005\u0004\u0007\u0005\u0004\t\u0005\u0004\u000b\u0005\u0004\f\u0006\u0004\u0005\u0013\u000b\u0017\t\u0013\n\u0003\u0013\t\u0010\f\u000b\b\u0011\u0017\u0006\u000b\u0010\u0015\u0013\u0011\u0012\u0018\u0003\u0004\r\u000e\f\r\u0003\t\f\u0013\u000b\u000b\u000f\u000b\u0010\u0015\u0005\u0007\u000e\f\r\u0006\u000b\u0010\u0015\u0013\u0011\u0012\u0018\u0003\u0004\n\u000e\u0016\u000b\u0013\u0014\u000b\u0003\t\u0010\u0014\u0017\u000b\u0013\u0014\u0005RewardMajority Count \ud835\udc8e(out of \ud835\udc75=\ud835\udfd3) Majority Count \ud835\udc8e(out of \ud835\udc75=\ud835\udfd3) Majority Count \ud835\udc8e(out of \ud835\udc75=\ud835\udfd3) Majority Count \ud835\udc8e(out of \ud835\udc75=\ud835\udfd3) \u0006\u0007\b\t\n\u0005\u0004\u0005\u0005\u0004\u0007\u0005\u0004\t\u0005\u0004\u000b\u0005\u0004\f\u0006\u0004\u0005(a) Proposer (Discrete) (b) Proposer (Continuous) (c) Solver (Discrete) (d) Solver (Continuous)\nFigure 3.Comparison ofProposerandSolverrewards under discrete and our continuous formulations for a single iteration.Each\npanel shows rewards forlow-entropy(high-agreement) andhigh-entropy(diverse-answer) cases for a single iteration for number ofSolver\nresponsesN=5. In case ofProposer(left), discrete rewards collapse them into identical plateaus, providing weaker learning signals. In\ncontrast, our continuous reward varies smoothly and distinguishesSolverresponse patterns. In case ofSolver(right), the discrete reward\nincreases linearly only with majority count and does not reflect partial progress, leading to sparse and unstable learning signals during early\ntraining. Instead, ourcontinuous Solver rewardscales smoothly with agreement, enabling theSolverto improve reasoning consistency\ngradually. Collectively, our continuous reward formulation creates a more stable self-evolution loop, where theProposerandSolverco-\nadapt toward more grounded and consistent reasoning behavior.\nProposer Reward\nTraining Steps\nSolver RewardDiscrete RewardContinuous Reward\nFigure 4.Comparison between the discrete vs. our contin-\nuous reward progression during training.Top: The discrete\nmajority-vote reward (red) remains low and unstable during train-\ning, providing a weak learning signal due toSolveroutput vari-\nability in early stages. In contrast, our continuous self-consistency\nreward (green) produces stable and valuable feedback, enabling\ntheProposerto consistently generate moderate difficulty, informa-\ntive questions. Bottom: Guided by ourProposerwith continuous\nreward, we observe theSolver(green) to be more consistent and\nstable compared to its discrete (red) counterpart.\n3.2. EvoLMM Training Insights\nAs discussed earlier, ourProposer-Solvertraining loop pro-\nduces a joint self-evolving learning dynamics in which both\nquestion generation and answer reasoning gradually im-\nprove over the time using only images. In the absence of\nhuman annotations or external reward models, our model\nlearns to rely entirely on the structure of its own outputs to\nProposer: How many carnivores have H2 as their sole herbivore source?Solver: [\"1\", \"1\", \u201d2\", \"1\", \u201d2\"]Proposer_Reward= 0.8103Solver _Rewards= [0.6990, 0.6990, 0.5270, 0.6990, 0.5270]\nProposer: Which lunar phase is directly opposite the new moon? Solver:[\"Full Moon\", \"FULL MOON\", \"Full Moon\", \"Full moon\", \"FULL MOON\"]Proposer_Reward= 0.0366Solver _Rewards= [1.0, 1.0, 1.0, 1.0, 1.0]\nProposer: Which state has the highest number of charitable organizations?Solver:[\"2\", \"3\", \"3\", \u201d1\", \"2\"]Proposer_Reward= 0.9072Solver _Rewards= [0.5270, 0.5270, 0.5270, 0.3241, 0.5270]\nProposer: What is the maximum number of paths from point A to B that do not pass through J?Solver:[\"2\", \"3\", \u201d1\", \"2\", \u201d4\"]Proposer_Reward= 0.4672Solver _Rewards= [0.5270, 0.3241, 0.3241, 0.5270, 0.3241]\nProposer: Which environmental issue ranks third in frequency according to the chart? Solver:[\"Climate Change\", \u201dWater Pollution\", \"Waste Management\", \"Waste Management\", \"Climate Change\"]Proposer_Reward= 0.9072Solver _Rewards= [0.5270, 0.3241, 0.5270, 0.5270, 0.5270]\nSteps\n10\n2\nSteps\n2\n018\nSteps\n5\n122Figure 5.Example showing progression of our continuous\nreward-basedProposerquestions generation along with re-\nwards.From top (step 102) to bottom (step 5122),Proposerin-\ncreases question complexity, which in turn enhances the reasoning\ncapabilities ofSolver. Refer suppl. material for more examples.\nguide the learning.\nTo present further insights into our continuous self-\nconsistency reward-based training, we conduct an experi-\nment to post-train Qwen2.5-VL-7B [3] on ChartQA [23]\ntraining images (without any question-answer pairs or meta-\ndata). As shown in Figure 5, our self-consistency reward\nyields agradedscore proportional to the empirical agree-\nment amongSolveroutputs. Even partial agreement (e.g.,\n2 out of 5 consistent samples) produces a meaningful pos-\n5\nAlgorithm 1:EvoLMM: Self-Evolving Training\nstrategy with continious rewards.\nInput:Unlabeled imagesX; pretrained LMM\nbackbone\u03c0 ref\nOutput:Trained Proposer\u03c0 \u03d5and Solver\u03c0 \u03b8\n// Initialize Proposer, Solver\nInitialize Proposer\u03c0 \u03d5\u2190LoRA(\u03c0 ref)\nInitialize Solver\u03c0 \u03b8\u2190LoRA(\u03c0 ref)\nInitialize EMA baselinesb prop, bsol\nSet number of answer samplesN, entropy targets\n(\u00b5H, \u03c3H)\nforeach training stepdo\nSample imagex\u223c X\n// Step 1: Proposer generates a\nquestion\nGenerate questionq\u223c\u03c0 \u03d5(q|x)\n// Step 2: Solver answers the\nquestion multiple times\nSampleNanswers{y i}N\ni=1\u223c\u03c0\u03b8(\u00b7|x, q)\n// Step 3: Compute empirical\nanswer distribution\nEstimatep(a|x, q)and entropyH(x, q)\n// Step 4: Continuous Solver\nReward\nfori= 1toNdo\nCompute agreement scorep(y i|x, q)\nCompute reward:\nrsol\ni= [p(y i|x, q)]\u03b3\u00b7(1\u2212\u03bb lenmax{0,(w i\u2212\u03c4)/\u03c4})\n// Step 5: Entropy-Based\nProposer Reward\nrprop= exp\u0012\n\u2212(H(x, q)\u2212\u00b5 H)2\n2\u03c32\nH\u0013\n// Step 6: Policy Gradient\nUpdates\nUpdate Solver using REINFORCE:\n\u2207\u03b8\u223c(rsol\ni\u2212b sol)\u2207log\u03c0 \u03b8\nUpdate Proposer everyKsteps:\n\u2207\u03d5\u223c(rprop\u2212b prop)\u2207log\u03c0 \u03d5\nApply KL-regularization controller:\n\u03b2\u2190\u03b2\u00b7exp\u0012\n\u03b7KL\u2212\u03c4\n\u03c4\u0013\nPercentagesTraining StepsEasyModerateHard\nFigure 6.Evolution of proposer question difficulty over train-\ning. As self-evolution progresses, the proposer shifts from mostly\neasy or overly hard questions to a stable distribution dominated by\nmoderate-difficulty ones. This reflects the emergence of an im-\nplicit curriculum: mid-entropy questions provide the most infor-\nmative signal for the solver, leading to more stable and effective\nlearning.\nitive reward. Furthermore, our continuous feedback re-\nduces collapse and acceleratesSolverimprovement (see\nFigure 4). The continuousSolverreward changes smoothly\nand slightly dips at partial consensus, while theProposer\npeaks at moderate difficulty (see step 5122 in Figure 5).\nIn summary, our proposed continuous self-evolving de-\nsign provides a non-zero learning signal even when the\nmodel is uncertain, avoiding stagnation observed in the\ndiscrete-reward self-questioning scheme. Further, our de-\nsign enables theProposerto continuously adjust question\ndifficulty to match theSolver\u2019sevolving capability, thereby\nmitigating model collapse.\n4. Experiments\n4.1. Dataset\nWe train and evaluate our method in the mathematical\nand visually grounded reasoning domain, which naturally\nrequires integrating perception with structured, multi-step\nreasoning making it a strong testbed for self-evolving mul-\ntimodal learning. These datasets contain rich visual patterns\n(trends, proportions, spatial relations, geometric structures)\nthat allow models to generate meaningful questions directly\nfrom images without human annotations.\nOur training uses only raw images, with no ques-\ntion\u2013answer pairs or metadata. We sample roughly 1,000\nimages each from six widely used benchmarks such as\nChartQA [23], AI2D [15], InfographicVQA [24], PlotQA\n[26], ChartX [41], and Geometry3K [2] totaling\u223c6k im-\nages. These datasets cover charts, plots, scientific diagrams,\nand geometric figures, providing diverse visual contexts\nfor theProposerto generate grounded mathematical ques-\ntions. This diversity encourages theSolverto develop sta-\nble, structured reasoning as the system self-evolves using\ninternal consistency alone.\n6\nModel ChartQA MathVista MathVision MathVerse InfoGraphic-VQA val AI2D ScienceQA MMMU val\nVision-Zero\u2020(CLEVR) [36]84.24 68.43 23.96 43.86 80.35 82.64 88.50 51.44\nQwen2.5-VL-7B (Baseline) [3]84.00 68.46 23.91 43.78 80.44 82.61 88.30 51.11\nQwen2.5-VL-7B + Discrete Reward84.62 68.88 22.52 42.10 80.52 82.18 87.98 50.84\nQwen2.5-VL-7B + Cont. Reward 86.70 70.52 24.81 44.88 81.06 83.41 89.50 52.01\n\u2206Improvement +2.7% +2.06% +0.9% +1.1% +0.62% +0.8% +1.2% +0.9%\nTable 1.Evaluation results across eight multimodal mathematical and visual reasoning benchmarks.We compare the pretrained\nQwen2.5-VL-7B baseline, a discrete self-questioning adaptation, and our continuous self-evolving framework. The discrete reward pro-\nvides little to no improvement, reflecting its instability in multimodal settings. In contrast, our continuous self-consistency reward yields\nconsistent gains across all benchmarks, such as84.00%\u219286.70%on ChartQA and88.30%\u219289.50%on ScienceQA, demonstrating\nstable self-improvement from raw images without external supervision. Methods marked with an (\u2020) uses external supervision.\nModel ChartQA MathVista MathVision MathVerse InfoGraphic-VQA val AI2D ScienceQA MMMU val\nQwen2.5-VL-7B (Baseline)84.00 68.46 23.91 43.78 80.44 82.61 88.30 51.11\nQwen2.5-VL-7B + LoRA 86.70 70.52 24.81 44.88 81.06 83.41 89.50 52.01\nQwen2.5-VL-7B + QLoRA85.32 68.92 23.97 43.82 80.83 82.75 88.73 51.71\nQwen2.5-VL-7B + Full-Finetune84.20 68.41 23.37 43.77 80.37 82.64 88.12 51.23\nTable 2.Comparison of our EvoLMM self-evolving framework under different parameter update strategies.We evaluate LoRA,\nQLoRA, and full-parameter fine-tuning on Qwen2.5-VL-7B while keeping the same self-evolving training setup. LoRA achieves the\nstrongest improvements, e.g.,84.00%\u219286.70%on ChartQA and88.30%\u219289.50%on ScienceQA, demonstrating stable and effective\nadaptation while preserving pretrained multimodal grounding. QLoRA also improves performance but is slightly limited by quantization\nnoise, while full fine-tuning yields minimal gains and in some cases regresses, likely due to overfitting without external supervision. These\nresults highlight that parameter-efficient update schemes are better suited for continuous self-evolution without labeled data.\n4.2. Training Details\nWe fine-tune the pretrained Qwen-2.5-VL-7B model using\ntwo lightweight LoRA adapters for theProposerandSolver\nroles, keeping the backbone frozen to retain general multi-\nmodal capability. Training runs on a single node with 8\u00d7\nAMD MI250X GPUs using bfloat16 precision. The solver\nis optimized with our continuous self-consistency reward\ncomputed fromN= 5sampled answers, while the pro-\nposer receives an entropy-based band-pass reward encour-\naging mid-difficulty questions. Both policies are trained us-\ning KL-regularized REINFORCE with adaptive KL control\nfor stability. The proposer is updated every 5 iterations fol-\nlowing [5], while the solver receives gradients at each step.\nWe apply AdamW with a1e\u22126learning rate, weight decay\nof0.01, and gradient clipping at1.0. The continuous re-\nwards use a length penalty of0.10toward a 6-word target,\nsolver softness exponent\u03b3= 0.7, and a Gaussian entropy\nreward for the proposer centered at\u00b5= 0.90with\u03c3= 0.35\n(more details and comparisons on the update schedule and\nreward design are provided in the suppl. material.). Train-\ning is performed for6000steps with a batch size of1us-\ning a constant learning rate schedule. No QA annotations,\nmetadata, or external reward models are used at any stage.\n4.3. Evaluation\nWe evaluate our self-evolving framework on a broad suite\nof mathematical and visually grounded reasoning bench-\nmarks, including ChartQA [23], MathVista [21], MathVi-sion [35], MathVerse [49], InfoGraphic-VQA [24], AI2D\n[15], ScienceQA [20], and MMMU [44]. These datasets\ntest a range of capabilities from chart interpretation and\nsymbolic math to diagram understanding\u2014and all metrics\nfollow their standard accuracy protocols. We use the of-\nficial evaluation splits and apply identical inference set-\ntings across models, ensuring that performance differences\narise solely from our self-evolving training rather than task-\nspecific tuning. All evaluations are run on AMD MI250X\nGPUs using the lmms-eval framework [48], with Hugging-\nFace Transformers v4.38 and bfloat16 precision for consis-\ntency with training.\n4.4. Main Results\nIn Table 1, we compare our proposed EvoLMM frame-\nwork against two baselines: (1) the original pretrained\nQwen2.5-VL-7B model [3] without any additional training,\nand (2) a direct multimodal adaptation [5] that uses a dis-\ncrete majority-vote reward during training.Qwen2.5-VL-\n7B (Baseline).The baseline model performs well on visu-\nally grounded reasoning tasks that require direct reading or\nsurface-level interpretation (e.g., ChartQA trend identifica-\ntion, InfoGraphic-VQA object recognition). However, the\nmodel\u2019s performance drops on tasks requiring multi-step\ninference or symbolic reasoning (e.g., MathVista, Math-\nVerse), indicating that pretrained multimodal alignment\nalone is insufficient for deeper reasoning. We also report re-\nsults from recent Vision-Zero [36], whose self-play formu-\nlation bears partial similarity to ours through autonomous\n7\nModel ChartQA MathVista MathVision MathVerse InfoGraphic-VQA val AI2D ScienceQA MMMU val\nQwen2.5-VL-7B (Base) [3]84.00 68.46 23.91 43.78 80.44 82.61 88.30 51.11\nQwen2.5-VL-7B (Ours) 86.70 70.52 24.81 44.88 81.06 83.41 89.50 52.01\nInternVL3-8B-Instruct (Base) [52]82.40 65.2 25.36 31.62 68.77 83.19 97.77 52.78\nInternVL3-8B-Instruct (Ours) 84.97 67.20 26.44 32.92 69.39 83.95 98.13 53.77\nGemma3-12B-It (Base) [33]55.64 60.13 24.53 28.96 50.69 79.05 83.89 48.11\nGemma3-12B-It (Ours) 58.61 62.13 25.61 30.26 51.37 79.85 84.97 49.10\nLlama-3.2-11B-Vision-Instruct (Base) [25]29.24 46.59 23.47 37.23 56.69 46.44 56.87 47.93\nLlama-3.2-11B-Vision-Instruct (Ours) 32.24 48.59 24.55 38.53 57.37 47.32 58.07 48.92\nTable 3.Effectiveness of our EvoLMM self-evolving framework across different large multimodal backbones.We apply the same\nProposer\u2013Solvercontinuous self-consistency training to four LMM families without changing architecture, data, or supervision. On\nQwen2.5-VL-7B, our method improves visual math reasoning performance consistently across all benchmarks (e.g.,84.00%\u219286.70%\non ChartQA and88.30%\u219289.50%on ScienceQA). We observe similar trends when transferring to, InternVL3-8B, and Llama-3.2-11B-\nVision-Instruct: each model benefits from continuous self-evolution, typically gaining+1\u20133accuracy points on chart/diagram reasoning\ntasks and+1\u20132points on math and science QA. These improvements occurwithoutannotations, handcrafted curriculum, image editing,\nor external verifiers, demonstrating that our continuous reward mechanism is model-agnostic and transferable across diverse LMMs.\nModel ChartQA MathVista MathVision MathVerse InfoGraphic-VQA val AI2D ScienceQA MMMU val\nQwen2.5-VL-7B (base)84.00 68.20 23.91 43.78 80.44 82.61 88.30 51.11\nQwen2.5-VL-7B (Ours) 86.70 70.52 24.81 44.88 81.06 83.41 89.50 52.01\nQwen2.5-VL-72B (base)88.20 73.93 36.92 54.09 85.97 87.34 93.36 65.86\nQwen2.5-VL-72B (Ours) 91.04 76.44 38.31 55.45 86.63 88.19 94.63 67.02\nTable 4.Scaling behaviour of our EvoLMM self-evolving framework across model sizes in the Qwen2.5-VL family.We apply the\nsameProposer\u2013Solvercontinuous self-consistency training on 7B and 72B variants without changing data, reward design, or optimization\nsettings. Across all scales, our method consistently improves multimodal mathematical reasoning, scientific diagram understanding, and\nchart interpretation accuracy. Notably, larger models exhibit stronger absolute gains, reflecting their greater capacity to refine internal\nreasoning when guided by continuous agreement feedback. For example, we observe progressively increasing improvements on MathVista\nas model size increases, and similar upward trends on ScienceQA and AI2D. Please refer suppl. material for additional experiments.\nmulti-agent learning. However, Vision-Zero leverages syn-\nthetic image pairs generated using external models such as\nGPT-4o [27] and Gemini [7], introducing implicit super-\nvision that falls outside our unsupervised setting. Vision-\nSR1 [18] differs entirely from our constraints, as it relies on\npaired question\u2013answer supervision across both perception\nand reasoning stages.\nDiscrete-Reward Adaptation.We first test a direct mul-\ntimodal adaptation of the discrete self-questioning reward,\nwhere theSolveris rewarded only when multiple sampled\nanswers exactly match. In practice, the reward quickly be-\ncomes sparse asSolveroutputs remain highly diverse in\nearly training, leading to unstable optimization. The dis-\ncrete variant yields only marginal gains (+0.3\u22120.6%)\nand occasionally degrades performance, confirming that\nmajority-vote rewards are too brittle and unstable for multi-\nmodal reasoning.\nContinuous-Reward (Ours).Replacing the discrete sig-\nnal with our continuous self-consistency reward provides\na smoother and more informative gradient. Training pro-\ngresses steadily even under uncertainty, leading to consis-\ntent improvements of +2\u20133% across major benchmarks. For\ninstance, accuracy rises from 68.4%\u219270.5% onMathVista\nand 84.0%\u219286.7% onChartQA. Other than mathemati-cal reasoning, our method also keep competitive improve-\nment of +1.2% onScienceQA. These results demonstrate\nthat continuous internal rewards yield stable, annotation-\nfree self-evolution, strengthening both reasoning reliability\nand answer consistency.\n4.5. Comparison Across Training Settings\nWe evaluate the self-evolving framework under three fine-\ntuning strategies such as LoRA [10], QLoRA [9], and full-\nparameter fine-tuning\u2014while keeping theProposer\u2013Solver\npipeline and continuous reward formulation identical across\nsettings (see Table 2).\nLoRA (default).LoRA [10] achieves the best trade-off\nbetween stability and adaptability. By updating only a\nsmall fraction of parameters, it preserves pretrained multi-\nmodal alignment while enabling effectiveProposer\u2013Solver\nco-evolution. LoRA delivers consistent and stable gains,\nimproving accuracy from 68.4%\u219270.5% onMathVistaand\n84.0%\u219286.7% onChartQA, without any supervised data.\nQLoRA.The quantized variant QLoRA [9] (4-bit base +\nhigh-precision adapters) reduces memory usage by nearly\n3\u00d7, making it suitable for limited hardware. How-\never, quantization noise slightly weakens solver con-\nsistency, yielding smaller but still positive gains (e.g.,\n8\n68.4%\u219268.92% onMathVista). It indicates that high-\nprecision activations are important when learning from fine-\ngrained continuous rewards.\nFull Fine-Tuning.Full-parameter updates lead to faster\ninitial learning but exhibit reduced stability in the self-\nevolving setting. Without explicit supervision, theSolver\ntends to overfit early biases, and theProposeroccasionally\ncollapses into repetitive or low-diversity question modes.\nMoreover, because our reward formulation includes a KL\nregularization term that constrains divergence from the pre-\ntrained base model, full fine-tuning interacts unfavorably\nwith this objective causing the model to drift away from its\noriginal alignment and degrade baseline performance. Con-\nsequently, full fine-tuning yields lower final accuracy than\nLoRA, despite higher compute cost and greater instability.\n4.6. Self Evolution Across Different Backbones\nTo evaluate the generality of our approach, we apply\nthe sameProposer\u2013Solvertraining setup to several large\nmultimodal models, includingQwen3-VL-8B-Instruct[3],\nInternVL3-8B[52],Gemma-3-12B-it[33], andLlama-3.2-\n11B-Vision-Instruct[25]. As shown in Table 3, in all cases,\nthe models are trained without question\u2013answer supervi-\nsion, using only the proposed continuous self-consistency\nreward. We observe consistent improvements of roughly\n+2\u22123%across all backbones, confirming that our method\nis architecture-agnostic and does not rely on specific\ndataset alignments or handcrafted supervision. Models\nwith stronger visual grounding (e.g., InternVL3-8B [52])\nshow faster convergence in early training, while those with\nstronger linguistic reasoning (e.g., Gemma-3-12B [33]) dis-\nplay larger late-stage gains as solver reasoning stabilizes.\nThese results demonstrate that continuous self-reward ef-\nfectively generalizes across diverse LMM families. This\nshows that our reward mechanism enhances structured vi-\nsual reasoning rather than surface-level pattern matching,\nproviding a scalable and backbone-independent path toward\nautonomous multimodal reasoning.\n4.7. Scaling Behavior Across Model Sizes\nWe evaluate our self-evolving framework across the\nQwen2.5-VL family (7B & 72B) using the samePro-\nposer\u2013Solversetup and continuous reward, without mod-\nifying data or optimization settings (refer Table 4). Con-\nsistent gains are observed at all scales, with the 7B model\nimproving from84.0%\u219286.7%on ChartQA and68.4%\n\u219270.5%on MathVista, while larger variants (e.g., 72B)\nachieve stronger absolute performance88.20%\u219291.04%\non ChartQA and93.36%\u219294.63%. Overall, these re-\nsults confirm our continuous self-consistency reward scales\nreliably across model sizes, enabling stable improvements\nwithout task-specific supervision or external verifiers.5. Conclusion\nWe introduced EvoLMM, a self-evolving training frame-\nwork for large multimodal models that improves visual rea-\nsoning ability without relying on human-annotated super-\nvision, metadata, or external reward models. By coupling a\nProposerandSolverwithin a shared backbone and optimiz-\ning them using acontinuous self-consistency reward, the\nmodel learns to generate informative, mid-difficulty ques-\ntions and to refine its reasoning through stable internal feed-\nback. Our experiments show consistent gains across mul-\ntiple mathematical and scientific visual reasoning bench-\nmarks, and ablations demonstrate that the approach is ro-\nbust across different multimodal backbones. The results\nhighlight that continuous self-reward is essential for en-\nabling autonomous learning in multimodal settings, where\ndiscrete majority-vote rewards are insufficient due to early-\nstage uncertainty and perception noise. This work takes a\nstep toward open-ended, fully self-improving multimodal\nintelligence and suggests promising future directions in cur-\nriculum emergence, self-generated data scaling, and long-\nhorizon reasoning without supervision.\nReferences\n[1] Haider Al-Tahan, Q. Garrido, Randall Balestriero, Diane\nBouchacourt, C. Hazirbas, and Mark Ibrahim. Unibench:\nVisual reasoning requires rethinking vision-language beyond\nscaling.ArXiv, abs/2408.04810, 2024. 1\n[2] Avinash Anand, Raj Jaiswal, Abhishek Dharmadhikari,\nAtharva Marathe, Harsh Popat, Harshil Mital, Ashwin R\nNair, Kritarth Prasad, Sidharth Kumar, Astha Verma, et al.\nGeovqa: A comprehensive multimodal geometry dataset for\nsecondary education. In2024 IEEE 7th International Con-\nference on Multimedia Information Processing and Retrieval\n(MIPR), pages 102\u2013108. IEEE, 2024. 6\n[3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin\nGe, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun\nTang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhao-\nhai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu,\nYiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng,\nHang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin.\nQwen2.5-vl technical report, 2025. 5, 7, 8, 9\n[4] Yuxuan Cai, Jiangning Zhang, Haoyang He, Xinwei He, Ao\nTong, Zhenye Gan, Chengjie Wang, Zhucun Xue, Yong Liu,\nand Xiang Bai. Llava-kd: A framework of distilling multi-\nmodal large language models, 2025. 1\n[5] Lili Chen, Mihir Prabhudesai, Katerina Fragkiadaki, Hao\nLiu, and Deepak Pathak. Self-questioning language models.\narXiv preprint arXiv:2508.03682, 2025. 2, 3, 7\n[6] Yixing Chen, Yiding Wang, Siqi Zhu, Haofei Yu, Tao Feng,\nMuhan Zhang, Mostofa Patwary, and Jiaxuan You. Multi-\nagent evolve: Llm self-improve through co-evolution.arXiv\npreprint arXiv:2510.23595, 2025. 2, 3\n[7] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice\nPasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blis-\ntein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5:\n9\nPushing the frontier with advanced reasoning, multimodality,\nlong context, and next generation agentic capabilities.arXiv\npreprint arXiv:2507.06261, 2025. 3, 8\n[8] Dawei Dai, Xu Long, Yutang Li, Yuanhui Zhang, and\nShuy Xia. Humanvlm: Foundation for human-scene vision-\nlanguage model.ArXiv, abs/2411.03034, 2024. 1\n[9] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke\nZettlemoyer. Qlora: Efficient finetuning of quantized llms.\nAdvances in neural information processing systems, 36:\n10088\u201310115, 2023. 8\n[10] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLoRA: Low-rank adaptation of large language models. InIn-\nternational Conference on Learning Representations, 2022.\n8\n[11] Drew A. Hudson and Christopher D. Manning. Gqa: A new\ndataset for real-world visual reasoning and compositional\nquestion answering.2019 IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 6693\u2013\n6702, 2019. 1\n[12] Jinseong Jang, Chunfei Ma, and Byeongwon Lee. Vl2lite:\nTask-specific knowledge distillation from large vision-\nlanguage models to lightweight networks. In2025\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 30073\u201330083, 2025. 1\n[13] Songtao Jiang, Chenyi Zhou, Yan Zhang, Yeying Jin, and\nZuozhu Liu. Fast or slow? integrating fast intuition and de-\nliberate thinking for enhancing visual question answering. In\nAnnual Meeting of the Association for Computational Lin-\nguistics, 2025. 1\n[14] Mingi Jung, Saehuyng Lee, Eunji Kim, and Sungroh Yoon.\nVisual attention never fades: Selective progressive attention\nrecalibration for detailed image captioning in multimodal\nlarge language models.ArXiv, abs/2502.01419, 2025. 1\n[15] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon\nSeo, Hannaneh Hajishirzi, and Ali Farhadi. A diagram is\nworth a dozen images. InEuropean conference on computer\nvision, pages 235\u2013251. Springer, 2016. 6, 7\n[16] Kaiyu Li, Zixuan Jiang, Xiangyong Cao, Jiayu Wang,\nYuchen Xiao, Deyu Meng, and Zhi Wang. Describeearth:\nDescribe anything for remote sensing images.ArXiv,\nabs/2509.25654, 2025. 1\n[17] Xinwei Li, Li Lin, Shuai Wang, and Chen Qian. Self-\nimproving teacher cultivates better student: Distillation cal-\nibration for multimodal large language models. InProceed-\nings of the 47th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval, pages\n882\u2013892, 2024. 1\n[18] Zongxia Li, Wenhao Yu, Chengsong Huang, Rui Liu,\nZhenwen Liang, Fuxiao Liu, Jingxi Che, Dian Yu, Jor-\ndan Boyd-Graber, Haitao Mi, et al. Self-rewarding vision-\nlanguage model via reasoning decomposition.arXiv preprint\narXiv:2508.19652, 2025. 2, 3, 8\n[19] Ziyu Liu, Tao Chu, Yuhang Zang, Xilin Wei, Xiao wen\nDong, Pan Zhang, Zijian Liang, Yuanjun Xiong, Yu Qiao,\nDahua Lin, and Jiaqi Wang. Mmdu: A multi-turn multi-\nimage dialog understanding benchmark and instruction-\ntuning dataset for lvlms.ArXiv, abs/2406.11833, 2024. 1[20] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei\nChang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and\nAshwin Kalyan. Learn to explain: Multimodal reasoning via\nthought chains for science question answering, 2022. 7\n[21] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li,\nHannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel\nGalley, and Jianfeng Gao. Mathvista: Evaluating mathemat-\nical reasoning of foundation models in visual contexts.arXiv\npreprint arXiv:2310.02255, 2023. 7\n[22] Yiting Lu, Jiakang Yuan, Zhen Li, Shitian Zhao, Qi Qin,\nXinyue Li, Le Zhuo, Licheng Wen, Dongyang Liu, Yuewen\nCao, Xiangchao Yan, Xin Li, Botian Shi, Tao Chen, Zhibo\nChen, Lei Bai, Bo Zhang, and Peng Gao. Omnicaptioner:\nOne captioner to rule them all.ArXiv, abs/2504.07089, 2025.\n1\n[23] Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and\nEnamul Hoque. Chartqa: A benchmark for question answer-\ning about charts with visual and logical reasoning. InFind-\nings of the association for computational linguistics: ACL\n2022, pages 2263\u20132279, 2022. 5, 6, 7\n[24] Minesh Mathew, Viraj Bagal, Rub `en Tito, Dimosthenis\nKaratzas, Ernest Valveny, and CV Jawahar. Infographicvqa.\nInProceedings of the IEEE/CVF Winter Conference on Ap-\nplications of Computer Vision, pages 1697\u20131706, 2022. 6,\n7\n[25] Meta AI. Llama 3.2: Revolutionizing edge ai and vision\nwith open, customizable models.https://ai.meta.\ncom/blog/llama-3-2-connect-2024-vision-\nedge-mobile-devices/, 2024. Accessed: 2025-11-\n13. 8, 9\n[26] Nitesh Methani, Pritha Ganguly, Mitesh M Khapra, and\nPratyush Kumar. Plotqa: Reasoning over scientific plots.\nInProceedings of the ieee/cvf winter conference on appli-\ncations of computer vision, pages 1527\u20131536, 2020. 6\n[27] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,\nLama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo\nAlmeida, Janko Altenschmidt, Sam Altman, Shyamal Anad-\nkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Bal-\ncom, Paul Baltescu, Haiming Bao, Mohammad Bavarian,\nJeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-\nShapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko,\nMadelaine Boyd, Anna-Luisa Brakman, Greg Brockman,\nTim Brooks, Miles Brundage, Kevin Button, Trevor Cai,\nRosie Campbell, Andrew Cann, Brittany Carey, Chelsea\nCarlson, Rory Carmichael, Brooke Chan, Che Chang, Fo-\ntis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Ja-\nson Chen, Mark Chen, Ben Chess, Chester Cho, Casey\nChu, Hyung Won Chung, Dave Cummings, Jeremiah Cur-\nrier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah\nDeutsch, Damien Deville, Arka Dhar, David Dohan, Steve\nDowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti,\nTyna Eloundou, David Farhi, Liam Fedus, Niko Felix,\nSim\u00b4on Posada Fishman, Juston Forte, Isabella Fulford, Leo\nGao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogi-\nneni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon,\nMorgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross,\nShixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han,\nJeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke,\n10\nChris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele,\nBrandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost\nHuizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela\nJiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto,\nBillie Jonn, Heewoo Jun, Tomer Kaftan, \u0141ukasz Kaiser,\nAli Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar,\nTabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina\nKim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros,\nMatt Knight, Daniel Kokotajlo, \u0141ukasz Kondraciuk, An-\ndrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen\nKrueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee,\nJan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel\nLim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa\nLopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Mal-\nfacini, Sam Manning, Todor Markov, Yaniv Markovski,\nBianca Martin, Katie Mayer, Andrew Mayne, Bob Mc-\nGrew, Scott Mayer McKinney, Christine McLeavey, Paul\nMcMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob\nMenick, Luke Metz, Andrey Mishchenko, Pamela Mishkin,\nVinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu,\nMira Murati, Oleg Murk, David M \u00b4ely, Ashvin Nair, Rei-\nichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard\nNgo, Hyeonwoo Noh, Long Ouyang, Cullen O\u2019Keefe, Jakub\nPachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Gi-\nambattista Parascandolo, Joel Parish, Emy Parparita, Alex\nPassos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Fil-\nipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde\nde Oliveira Pinto, Michael, Pokorny, Michelle Pokrass,\nVitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power,\nElizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya\nRamesh, Cameron Raymond, Francis Real, Kendra Rim-\nbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder,\nMario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sas-\ntry, Heather Schmidt, David Schnurr, John Schulman, Daniel\nSelsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh,\nSarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler,\nMaddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl,\nBenjamin Sokolowsky, Yang Song, Natalie Staudacher, Fe-\nlipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie\nTang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet,\nAmin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick\nTurley, Jerry Tworek, Juan Felipe Cer \u00b4on Uribe, Andrea Val-\nlone, Arun Vijayvergiya, Chelsea V oss, Carroll Wainwright,\nJustin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward,\nJason Wei, CJ Weinmann, Akila Welihinda, Peter Welin-\nder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner,\nClemens Winter, Samuel Wolrich, Hannah Wong, Lauren\nWorkman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao,\nTao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech\nZaremba, Rowan Zellers, Chong Zhang, Marvin Zhang,\nShengjia Zhao, Tianhao Zheng, Juntang Zhuang, William\nZhuk, and Barret Zoph. Gpt-4 technical report, 2024. 3,\n8\n[28] Jingwei Peng, Jiehao Chen, Mateo Alejandro Rojas, and\nMeilin Zhang. Mv-core: Multimodal visual-conceptual\nreasoning for complex visual question answering.ArXiv,\nabs/2508.07023, 2025. 1\n[29] Ruotian Peng, Haiying He, Yake Wei, Yandong Wen, andDi Hu. Matters: Training-free fine-grained image caption\nenhancement via local perception.2025 IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\npages 3963\u20133973, 2025. 1\n[30] Yi Peng, Peiyu Wang, Xiaokun Wang, Yichen Wei, Jiangbo\nPei, Weijie Qiu, Ai Jian, Yunzhuo Hao, Jiachun Pan, Tianyi-\ndan Xie, Li Ge, Rongxian Zhuang, Xuchen Song, Yang Liu,\nand Yahui Zhou. Skywork r1v: Pioneering multimodal rea-\nsoning with chain-of-thought.ArXiv, abs/2504.05599, 2025.\n1\n[31] Sheikh Shafayat, Fahim Tajwar, Ruslan Salakhutdinov, Jeff\nSchneider, and Andrea Zanette. Can large reasoning models\nself-train?arXiv preprint arXiv:2505.21444, 2025. 2, 3\n[32] Fangxun Shu, Yue Liao, Le Zhuo, Chenning Xu, Lei Zhang,\nGuanghao Zhang, Haonan Shi, Long Chen, Tao Zhong,\nWanggui He, Siming Fu, et al. Llava-mod: Making\nllava tiny via moe knowledge distillation.arXiv preprint\narXiv:2408.15881, 2024. 1\n[33] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya\nPathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Ta-\ntiana Matejovicova, Alexandre Ram \u00b4e, Morgane Rivi `ere,\net al. Gemma 3 technical report.arXiv preprint\narXiv:2503.19786, 2025. 8, 9\n[34] Omkar Thawakar, Dinura Dissanayake, Ketan Pravin More,\nRitesh Thawkar, Ahmed Heakl, Noor Ahsan, Yuhao Li, Il-\nmuz Zaman Mohammed Zumri, Jean Lahoud, Rao Muham-\nmad Anwer, et al. Llamav-o1: Rethinking step-by-step vi-\nsual reasoning in llms. InFindings of the Association for\nComputational Linguistics: ACL 2025, pages 24290\u201324315,\n2025. 1\n[35] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing\nRen, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Mea-\nsuring multimodal mathematical reasoning with math-vision\ndataset.Advances in Neural Information Processing Sys-\ntems, 37:95095\u201395169, 2024. 7\n[36] Qinsi Wang, Bo Liu, Tianyi Zhou, Jing Shi, Yueqian Lin,\nYiran Chen, Hai Helen Li, Kun Wan, and Wentian Zhao.\nVision-zero: Scalable vlm self-improvement via strategic\ngamified self-play.arXiv preprint arXiv:2509.25541, 2025.\n1, 3, 7\n[37] Weiyun Wang, Zhangwei Gao, Lianjie Chen, Zhe Chen, Jin-\nguo Zhu, Xiangyu Zhao, Yangzhou Liu, Yue Cao, Sheng-\nlong Ye, Xizhou Zhu, Lewei Lu, Haodong Duan, Yu Qiao,\nJifeng Dai, and Wenhai Wang. Visualprm: An effective\nprocess reward model for multimodal reasoning.ArXiv,\nabs/2503.10291, 2025. 1\n[38] Ronald J Williams. Simple statistical gradient-following al-\ngorithms for connectionist reinforcement learning.Machine\nlearning, 8(3):229\u2013256, 1992. 4\n[39] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan\nGao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei\nChen, et al. Qwen-image technical report.arXiv preprint\narXiv:2508.02324, 2025. 3\n[40] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin\nLuo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie\nZhou, et al. Omnigen2: Exploration to advanced multimodal\ngeneration.arXiv preprint arXiv:2506.18871, 2025. 3\n11\n[41] Renqiu Xia, Hancheng Ye, Xiangchao Yan, Qi Liu, Hongbin\nZhou, Zijun Chen, Botian Shi, Junchi Yan, and Bo Zhang.\nChartx & chartvlm: A versatile benchmark and foundation\nmodel for complicated chart reasoning.IEEE Transactions\non Image Processing, 2025. 6\n[42] Shuo Yang, Siwen Luo, S. Han, and Eduard Hovy. Magic-\nvqa: Multimodal and grounded inference with common-\nsense knowledge for visual question answering.ArXiv,\nabs/2503.18491, 2025. 1\n[43] Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan\nDeng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun\nRao, Minfeng Zhu, Bo Zhang, and Wei Chen. R1-onevision:\nAdvancing generalized multimodal reasoning through cross-\nmodal formalization.ArXiv, abs/2503.10615, 2025. 1\n[44] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi\nLiu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming\nRen, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Ren-\nliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo\nLiu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen.\nMmmu: A massive multi-discipline multimodal understand-\ning and reasoning benchmark for expert agi. InProceedings\nof CVPR, 2024. 7\n[45] Jenny Zhang, Shengran Hu, Cong Lu, Robert Lange, and\nJeff Clune. Darwin godel machine: Open-ended evolution\nof self-improving agents.arXiv preprint arXiv:2505.22954,\n2025. 2\n[46] Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu,\nXikun Zhang, Shijian Lu, and Dacheng Tao. R1-vl:\nLearning to reason with multimodal large language mod-\nels via step-wise group relative policy optimization.ArXiv,\nabs/2503.12937, 2025. 1\n[47] Juntian Zhang, Song Jin, Chuanqi Cheng, Yuhan Liu, Yankai\nLin, Xun Zhang, Yufei Zhang, Fei Jiang, Guojun Yin, Wei\nLin, et al. Viper: Empowering the self-evolution of visual\nperception abilities in vision-language model.arXiv preprint\narXiv:2510.24285, 2025. 2, 3\n[48] Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu,\nJoshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan\nZhang, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Lmms-\neval: Reality check on the evaluation of large multimodal\nmodels, 2024. 7\n[49] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin,\nZiyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei\nChang, Yu Qiao, et al. Mathverse: Does your multi-modal\nllm truly see the diagrams in visual math problems? In\nEuropean Conference on Computer Vision, pages 169\u2013186.\nSpringer, 2024. 7\n[50] Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma,\nKaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wen-\njuan Han, and Baobao Chang. Mmicl: Empowering\nvision-language model with multi-modal in-context learn-\ning.ArXiv, abs/2309.07915, 2023. 1\n[51] Ao Zhou, Zebo Gu, Tenghao Sun, Jiawen Chen, Mingsheng\nTu, Zifeng Cheng, Yafeng Yin, Zhiwei Jiang, and Qing\nGu. Hierarchical vision-language reasoning for multimodal\nmultiple-choice question answering.Proceedings of the 33rd\nACM International Conference on Multimedia, 2025. 1[52] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shen-\nglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su,\nJie Shao, et al. Internvl3: Exploring advanced training and\ntest-time recipes for open-source multimodal models.arXiv\npreprint arXiv:2504.10479, 2025. 8, 9\n12",
      "summary": "EvoLMM introduces a self-evolving framework for large multimodal models that improves reasoning abilities without relying on human-annotated data or external reward models. The approach leverages two cooperative agents\u2014a Proposer and a Solver\u2014that iteratively generate and solve image-grounded questions using continuous self-rewarding feedback.",
      "key_contributions": [
        "Proposes a fully unsupervised training framework for LMMs, eliminating the need for annotated data or external rewards.",
        "Introduces a dual-agent system (Proposer and Solver) for dynamic question generation and reasoning.",
        "Implements a continuous self-rewarding mechanism based on internal consistency to drive learning.",
        "Demonstrates consistent performance improvements on multimodal math-reasoning benchmarks using only raw images."
      ],
      "methodology": "EvoLMM instantiates two agents from a single backbone model: the Proposer generates diverse, image-grounded questions, while the Solver answers them, with learning guided by internal consistency and continuous self-rewarding feedback.",
      "results": "Using Qwen2.5-VL as the base model, EvoLMM achieves up to ~3% improvement on multimodal math-reasoning benchmarks such as ChartQA, MathVista, and MathVision, relying solely on raw training images.",
      "keywords": [
        "large multimodal models",
        "unsupervised learning",
        "self-evolving",
        "internal consistency",
        "self-rewarding",
        "reasoning",
        "image-grounded questions"
      ],
      "status": "new",
      "notes": null,
      "created_at": "2025-11-23T20:14:00.757592",
      "needs_llm_processing": false
    },
    "4": {
      "id": "2511.16673v1",
      "title": "NoPo-Avatar: Generalizable and Animatable Avatars from Sparse Inputs without Human Poses",
      "authors": [
        {
          "name": "Jing Wen",
          "affiliation": null
        },
        {
          "name": "Alexander G. Schwing",
          "affiliation": null
        },
        {
          "name": "Shenlong Wang",
          "affiliation": null
        }
      ],
      "abstract": "We tackle the task of recovering an animatable 3D human avatar from a single or a sparse set of images. For this task, beyond a set of images, many prior state-of-the-art methods use accurate \"ground-truth\" camera poses and human poses as input to guide reconstruction at test-time. We show that pose-dependent reconstruction degrades results significantly if pose estimates are noisy. To overcome this, we introduce NoPo-Avatar, which reconstructs avatars solely from images, without any pose input. By removing the dependence of test-time reconstruction on human poses, NoPo-Avatar is not affected by noisy human pose estimates, making it more widely applicable. Experiments on challenging THuman2.0, XHuman, and HuGe100K data show that NoPo-Avatar outperforms existing baselines in practical settings (without ground-truth poses) and delivers comparable results in lab settings (with ground-truth poses).",
      "arxiv_id": "2511.16673v1",
      "arxiv_url": "http://arxiv.org/abs/2511.16673v1",
      "pdf_url": "https://arxiv.org/pdf/2511.16673v1",
      "published": "2025-11-20T18:59:54Z",
      "updated": "2025-11-20T18:59:54Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "full_text": "NoPo-Avatar: Generalizable and Animatable Avatars\nfrom Sparse Inputs without Human Poses\nJing Wen Alexander G. Schwing Shenlong Wang\nUniversity of Illinois Urbana-Champaign\n{jw116, aschwing, shenlong}@illinois.edu\nhttps://wenj.github.io/NoPo-Avatar/\nAbstract\nWe tackle the task of recovering an animatable 3D human avatar from a single or a\nsparse set of images. For this task, beyond a set of images, many prior state-of-the-\nart methods use accurate \u201cground-truth\u201d camera poses and human poses as input\nto guide reconstruction at test-time. We show that pose -dependent reconstruction\ndegrades results significantly if pose estimates are noisy. To overcome this, we\nintroduce NoPo-Avatar, which reconstructs avatars solely from images, without\nany pose input. By removing the dependence of test-time reconstruction on human\nposes, NoPo-Avatar is not affected by noisy human pose estimates, making it\nmore widely applicable. Experiments on challenging THuman2.0, XHuman, and\nHuGe100K data show that NoPo-Avatar outperforms existing baselines in practical\nsettings (without ground -truth poses) and delivers comparable results in lab settings\n(with ground-truth poses).\n1 Introduction\nAnimatable human rendering aims to 1) reconstruct an animatable 3D representation from given\nimages, and to 2) synthesize novel views of possible novel human poses. The task is of great utility\nin VR/AR applications. Recent advances in rendering techniques have significantly improved the\nrealism and fidelity of the rendered results.\nConcretely, recent generalizable human rendering approaches [ 8\u201310,28] reconstruct an animatable\nrepresentation in a single deep net feed-forward pass. This significantly speeds up the reconstruction to\nsubsecond levels without compromising the rendering quality. In addition, large-scale training enables\nthese methods to perform well even with very sparse inputs. These advances make generalizable\nhuman rendering more practical for real-world applications compared to per-scene optimized methods.\nDespite the pros, most generalizable human rendering approaches assume that accurate camera poses\nand human poses are available for reconstruction at test-time. These input poses serve as strong\nguidance in locating the correspondences and gathering the aligned image features. However, the\nuse of accurate input poses during test-time reconstruction introduces a challenge, as illustrated in\nFig. 1: we assess the dependence of the rendering quality on test-time input pose quality used for\nreconstruction, either by 1) injecting Gaussian noise of different standard deviations into accurate\nground-truth test-time input poses used for reconstruction; or by 2) using a predicted pose for\nreconstruction at test-time. For the results shown in Fig. 1, we always used ground-truth poses for\ntest-time rendering. The noisier the test-time poses used for reconstruction, the worse the rendering\nquality the methods achieve. For in-the-wild scenarios where poses are estimated, the performance\nof existing methods degrades significantly, as shown in Fig. 1(a) horizontal axis label \u201cpred\u201d. This\nsensitivity to input poses is not a desirable property. Ideally, we expect the method to produce\nconsistent and high-quality results, no matter the quality of the input poses at reconstruction.\n39th Conference on Neural Information Processing Systems (NeurIPS 2025).arXiv:2511.16673v1  [cs.CV]  20 Nov 2025\nno noise std=0.01 std=0.03 std=0.05pred\nNoise Setting110120130140150160170180LPIPS*\nLPIPS* vs Noise\nGHG\nLIFe-GoM\nOurs\nno noise std=0.01 std=0.03 std=0.05pred\nNoise Setting455055606570FID\nFID vs Noise\nGHG\nLIFe-GoM\nOurs(a) Rendering quality vs. input-pose noises\nLIFe-GoMOursOutputInput (b) Qualitative comparisons\nFigure 1: (a)Sensitivity to input pose noises.Previous methods [ 10,28] take camera poses and\nhuman poses as inputs. We measure their sensitivity to input poses by injecting Gaussian noise of\ndifferent standard deviations or using a predicted pose. (Averaged over 5 runs for Gaussian noise;\nstd are multiplied by 3 for better visualization.) (b)Comparisons on rendering quality.With the\npredicted inaccurate input poses, LIFe-GoM cannot produce high-fidelity rendering. In contrast, our\nmethods, which does not take any poses as inputs, produce high-quality rendering.\nTo achieve this, we develop NoPo-Avatar, which completely eliminates the dependence of the\nanimatable avatar reconstruction on camera poses and human poses. Hence, our reconstruction and\nthe subsequent rendering is not affected by the quality of the input poses used for reconstruction, as\nshown in Fig. 1(a). Importantly, we represent our reconstruction in the canonical T-pose, which can\nbe animated to arbitrary novel poses without any post-processing. To obtain the canonical T-pose\nrepresentation, we design a dual-branch model that captures observed details and inpaints missing\nregions. Specifically, our model consists of two types of branches, a template branch and image\nbranches. The template branch starts from an encoding of the shape in the canonical pose and\noutputs Gaussians relative to the average SMPL-X template in T-pose. The image branches predict\npixel-aligned Gaussians (splatter images) in the same coordinate system as the template branch. Not\nonly can we model the fine details in the observed input images via the spatter images obtained from\nthe image branches, but we can also inpaint unseen regions via the template branch. For this, we\nadopt an encoder-decoder architecture, similar to NoPoSplat [ 34]: the template and the input images\nare independently embedded in each encoder and interact with each other via cross-attention in the\ndecoder. Our model is trained end-to-end using photometric losses and auxiliary regularization on\nGaussians\u2019 3D positions as well as linear blend skinning weights. We note that recent works such as\nLHM [ 16] and IDOL [ 37] scale training to improve generalization, and, like our method, eliminate\nthe need for pose data. However, unlike IDOL and LHM which operate on a single input image,\nwe support multiple input views. More importantly, we introduce a dual-branch design. In contrast,\nLHM and IDOL rely solely on a template branch, which subtly differs from ours in the template\nencoding and the fusion of template embeddings and image features. We find the image branch\nenhances reconstruction of details in observed regions.\nWe evaluate our method on THuman2.0, XHuman and HuGe100K and compare to the state-of-\nthe-arts. Our method reconstructs the avatars in high quality without any pose priors, significantly\noutperforming state-of-the-arts that use predicted poses, as shown in Fig. 1(b), and on par with those\nthat use ground-truth poses.\nOur contributions are twofold:\n\u2022We propose NoPo-Avatar, a novel model that reconstructs an animatable human avatar given only\ninput images. Our model does not use camera and human poses.\n\u2022We demonstrate that the reconstruction obtained without camera and human poses leads to high-\nquality novel view and novel pose rendering.\n2 Related Work\nGeneralizable human rendering from sparse inputs or a single image.Generalizable methods\nlearn inductive biases from large-scale datasets, enabling use of these methods in case of sparse inputs,\nor even a single image. We categorize methods tackling this task into one-stage and multi-stage\nmethods. Multi-stage methods [ 29,7,2,12,5,31,25] usually generate multiview images or videos\nusing a 2D generative model such as a diffusion model, and then reconstruct the 3D representation\n2\nfrom the multiview images. One-stage methods adopt a feed-forward neural network to produce the\n3D representation [ 8,9,36,10,28,38,33,4,37,16] or directly generate the representation with a\ndiffusion model given a single image as the condition [ 32]. Among these works, several [ 4,9,28,16,\n5,38] reconstruct in a canonical pose space, which enables animatation without post-processing such\nas skeleton binding. Our approach is a one-stage method and requires only a single feed-forward\npass for reconstruction. We also reconstruct in the canonical pose space to support animation.\nTo maximize the use of human priors, most of the recent work [ 4,9,10,28,38,33] takes the\nposed SMPL [ 11] or SMPL-X [ 14] meshes as input and assumes that accurate human poses and\ncamera poses are available during test-time reconstruction. The SMPL/SMPL-X poses are used to\nsample aligned features from the inputs explicitly or implicitly. While this assumption eases the\ntask, it also limits generalization to in-the-wild subjects, as accurate pose estimation either takes too\nlong or estimated poses are not accurate enough. More recently, RoGSplat [ 30] tackles the task of\ngeneralizable rendering with inaccurate pose estimation. Differently, we eliminate the need for both\nhuman pose and camera pose altogether and only operate on images and subject masks. Consequently,\nour method is not only insensitive to the accuracy of input poses, but also saves the time required for\npose estimation.. The very recent two works IDOL [ 37] and LHM [ 16] also consider reconstructing\nin the canonical pose space without use of human poses and camera poses. Our method differs\nfrom IDOL and LHM in two main ways: 1) Our method operates on any number of input images,\nwhile IDOL and LHM only address the single image setting, leaving an extension to multiple images\nopen. 2) We predict two sets of Gaussians, one in the projected UV space and the other aligned\nwith all foreground pixels in the input images, while IDOL and LHM only predict Gaussians in\nSMPL/SMPL-X\u2019s projected UV space. This enables our method to model the observed details.\nGeneralizable scene rendering without pose priors.Our setup aligns with recent efforts in\ngeneralizable scene rendering without pose priors. Note that pose priors here refer to camera\nposes only. The success of DUSt3R [ 26] sheds light on pixel-aligned geometry estimation without\nknowledge of the relative camera poses. This idea was adopted by generaliable rendering [ 34,3,24],\nsince inaccurate camera localization can potentially lead to corrupted renderings.\nWhile the above works reconstruct the scene in one input image\u2019s coordinate system, our task is\ninherently more challenging: the reconstruction is in a canonical T-pose which differs from the human\nposes in input images. Use of the T-pose reconstruction is important as it enables us to animate new\nposes without post processing.\n3 NoPo-Avatar\nGiven input images {In}N\nn=1,In\u2208RHI\u00d7WI\u00d73, and masks {Mn}N\nn=1,Mn\u2208 {0,1}HI\u00d7WI, indi-\ncating the pixels which correspond to a human subject, we aim to synthesize novel views/poses given\ntarget camera extrinsics E, intrinsic K, and the target human shape and pose P= (\u03b2,\u03b8) . Here, N\nis the number of input images, and HIandWIdenote the height and width of the input images.\nThe human shape \u03b2and pose \u03b8are represented in SMPL-X\u2019s format. Importantly, unlike prior\nwork [36, 10, 28], our method does not use any human poses and camera poses of the input images.\nTo achieve this goal, we develop NoPo-Avatar, which consists of two modules: reconstruction and\nrendering. Given target camera intrinsics K, extrinsics E, and human poses P, the rendering module\n(Render) computes the target image and alpha opacity mask:\n(I,M) =Render(G;E,K,P)(1)\nfrom the reconstructed canonical 3D T-pose representation G. It is the output of the reconstruction\nmodule (Recon), which operates on input images and subject masks, i.e.,\nG=GT\u222a GI=Recon({I n}N\nn=1,{M n}N\nn=1).(2)\nThe reconstructed 3D T-pose representation Gincludes two sets of Gaussians represented as splatter\nimages [ 22]:GT={GT\nij}HT\ni=1WT\nj=1from the template branch and GI={GI\nn,ij}N\nn=1HI\ni=1WI\nj=1from\nthe image branches. We will provide details regarding the two branches in Sec. 3.1. HT, WTand\nHI, WIare the height and width of the template branch and image branches, respectively. Each\nelement of the splatter image consists of the following components:\nG\u2217\n\u00b7= (\u00b5\u2217\n\u00b7,s\u2217\n\u00b7,r\u2217\n\u00b7, o\u2217\n\u00b7,h\u2217\n\u00b7,w\u2217\n\u00b7),(3)\n3\n<latexit sha1_base64=\"bWD2JGyhBJFZ4+he1/mEF+qp61U=\">AAAB8nicbVDLSgMxFL1TX7W+qi7dBIvgqsxIUZdFEVxWsA+YDiWTZtrQTDIkGaEM/Qw3LhRx69e482/MtLPQ1gOBwzn3knNPmHCmjet+O6W19Y3NrfJ2ZWd3b/+genjU0TJVhLaJ5FL1QqwpZ4K2DTOc9hJFcRxy2g0nt7nffaJKMykezTShQYxHgkWMYGMlvx9jMyaYZ3ezQbXm1t050CrxClKDAq1B9as/lCSNqTCEY619z01MkGFlGOF0VumnmiaYTPCI+pYKHFMdZPPIM3RmlSGKpLJPGDRXf29kONZ6God2Mo+ol71c/M/zUxNdBxkTSWqoIIuPopQjI1F+PxoyRYnhU0swUcxmRWSMFSbGtlSxJXjLJ6+SzkXdu6w3Hhq15k1RRxlO4BTOwYMraMI9tKANBCQ8wyu8OcZ5cd6dj8VoySl2juEPnM8feVCRZA==</latexit>E\n<latexit sha1_base64=\"bWD2JGyhBJFZ4+he1/mEF+qp61U=\">AAAB8nicbVDLSgMxFL1TX7W+qi7dBIvgqsxIUZdFEVxWsA+YDiWTZtrQTDIkGaEM/Qw3LhRx69e482/MtLPQ1gOBwzn3knNPmHCmjet+O6W19Y3NrfJ2ZWd3b/+genjU0TJVhLaJ5FL1QqwpZ4K2DTOc9hJFcRxy2g0nt7nffaJKMykezTShQYxHgkWMYGMlvx9jMyaYZ3ezQbXm1t050CrxClKDAq1B9as/lCSNqTCEY619z01MkGFlGOF0VumnmiaYTPCI+pYKHFMdZPPIM3RmlSGKpLJPGDRXf29kONZ6God2Mo+ol71c/M/zUxNdBxkTSWqoIIuPopQjI1F+PxoyRYnhU0swUcxmRWSMFSbGtlSxJXjLJ6+SzkXdu6w3Hhq15k1RRxlO4BTOwYMraMI9tKANBCQ8wyu8OcZ5cd6dj8VoySl2juEPnM8feVCRZA==</latexit>EInput Images(w/o poses)shared weightEncoderViTTemplate TokenImage Token 1Image Token N\n<latexit sha1_base64=\"YPN6HWASVu1hWz7oAMMZz2HVXQI=\">AAAB8nicbVDLSgMxFL1TX7W+qi7dBIvgqsxIUZdFXbisYB8wHUomzbShmWRIMkIZ+hluXCji1q9x59+YaWehrQcCh3PuJeeeMOFMG9f9dkpr6xubW+Xtys7u3v5B9fCoo2WqCG0TyaXqhVhTzgRtG2Y47SWK4jjktBtObnO/+0SVZlI8mmlCgxiPBIsYwcZKfj/GZkwwz+5mg2rNrbtzoFXiFaQGBVqD6ld/KEkaU2EIx1r7npuYIMPKMMLprNJPNU0wmeAR9S0VOKY6yOaRZ+jMKkMUSWWfMGiu/t7IcKz1NA7tZB5RL3u5+J/npya6DjImktRQQRYfRSlHRqL8fjRkihLDp5ZgopjNisgYK0yMbaliS/CWT14lnYu6d1lvPDRqzZuijjKcwCmcgwdX0IR7aEEbCEh4hld4c4zz4rw7H4vRklPsHMMfOJ8/d8uRYw==</latexit>D\n<latexit sha1_base64=\"YPN6HWASVu1hWz7oAMMZz2HVXQI=\">AAAB8nicbVDLSgMxFL1TX7W+qi7dBIvgqsxIUZdFXbisYB8wHUomzbShmWRIMkIZ+hluXCji1q9x59+YaWehrQcCh3PuJeeeMOFMG9f9dkpr6xubW+Xtys7u3v5B9fCoo2WqCG0TyaXqhVhTzgRtG2Y47SWK4jjktBtObnO/+0SVZlI8mmlCgxiPBIsYwcZKfj/GZkwwz+5mg2rNrbtzoFXiFaQGBVqD6ld/KEkaU2EIx1r7npuYIMPKMMLprNJPNU0wmeAR9S0VOKY6yOaRZ+jMKkMUSWWfMGiu/t7IcKz1NA7tZB5RL3u5+J/npya6DjImktRQQRYfRSlHRqL8fjRkihLDp5ZgopjNisgYK0yMbaliS/CWT14lnYu6d1lvPDRqzZuijjKcwCmcgwdX0IR7aEEbCEh4hld4c4zz4rw7H4vRklPsHMMfOJ8/d8uRYw==</latexit>D\n<latexit sha1_base64=\"YPN6HWASVu1hWz7oAMMZz2HVXQI=\">AAAB8nicbVDLSgMxFL1TX7W+qi7dBIvgqsxIUZdFXbisYB8wHUomzbShmWRIMkIZ+hluXCji1q9x59+YaWehrQcCh3PuJeeeMOFMG9f9dkpr6xubW+Xtys7u3v5B9fCoo2WqCG0TyaXqhVhTzgRtG2Y47SWK4jjktBtObnO/+0SVZlI8mmlCgxiPBIsYwcZKfj/GZkwwz+5mg2rNrbtzoFXiFaQGBVqD6ld/KEkaU2EIx1r7npuYIMPKMMLprNJPNU0wmeAR9S0VOKY6yOaRZ+jMKkMUSWWfMGiu/t7IcKz1NA7tZB5RL3u5+J/npya6DjImktRQQRYfRSlHRqL8fjRkihLDp5ZgopjNisgYK0yMbaliS/CWT14lnYu6d1lvPDRqzZuijjKcwCmcgwdX0IR7aEEbCEh4hld4c4zz4rw7H4vRklPsHMMfOJ8/d8uRYw==</latexit>D\nDecoderViTcrossattentioncrossattention\u22ee\nHeadDPT\nPer-Branch Gaussian Avatar\nFinal Avatar & Novel Pose/ View Synthesis\n\u22ee\nAnimation & Splat renderingCanonical UV MapFigure 2:Model architecture of the reconstruction module.The reconstruction module reconstructs\nthe canonical T-pose representation solely from images. It follows the encoder-decoder structure and\nconsists of two types of branches: a template branch and image branches. We show two views of\nthe predictions of each branch: the splatter images in the 2D format and their visualizations in 3D.\nGaussians predicted from all branches are combined and fed into the articulation and rendering.\nwhich refers to either GT\nijorGI\nn,ij, and subsumes a set of variables: \u00b5\u2217\n\u00b7\u2208R3is the mean of the\nGaussian at location \u201c \u00b7\u201d in branch \u201c*\u201d, s\u2217\n\u00b7\u2208R3andr\u2217\n\u00b7\u2208R4are the scale and rotation representated\nvia quaternions, o\u2217\n\u00b7is the opacity, h\u2217\n\u00b7\u2208RDeg\u00d73refers to the spherical harmonics of degree Deg,\nw\u2217\n\u00b7\u2208ROdenotes the LBS weights assigned to the bones withObeing the number of bones.\nWe detail the reconstruction module in Sec. 3.1, the articulation and rendering module in Sec. 3.2,\nand we describe the training losses in Sec. 3.3.\n3.1 Reconstruction\nThe reconstruction module ( Recon ), defined in Eq. (2), reconstructs the Gaussian primitives Gin\nthe canonical T-pose space given Ninput images {In}N\nn=1and the corresponding subject masks\n{Mn}N\nn=1. To achieve this we use the architecture illustrated in Fig. 2. Inspired by DUSt3R and its\nfollow-ups [ 26,34,23], we adopt an encoder-decoder architecture: the encoders embed each input\ninto tokens independently, and the decoders exchange information between inputs. Lastly, we use a\nregression head to predict the Gaussian primitives from the tokens.\nOur model consists of two types of branches: thetemplate branchand Nimage branches, one for\neach input image In, with n\u2208 {1, . . . , N} . Abstractly, the template branch captures the overall\nstructure of the human body and reconstructs the unseen regions of the subject, while the image\nbranches predict pixel-aligned Gaussians that focus only on the visible regions in the input images,\nyielding a fine-grained but incomplete reconstruction.\nEncoder:Thetemplate encoderis intended to capture articulated human-shape knowledge agnostic to\nsubjects identity \u2014here, the average SMPL -X template in T -pose. To this end, we simply implement\nit as a learnable embedding map FT\n0\u2208RHT\n0\u00d7WT\n0\u00d7C, where HT\n0andWT\n0denote the embedding\u2019s\nheight and width, and Cits channel dimension. Since this embedding takes no input, it is identical for\nall subjects and independent of the input images. The Nimage encodersshare weights and embed\neach input image Ininto tokens with a ViT-based architecture, i.e., FI\nn,0=EncI(In)\u2208RHI\n0\u00d7WI\n0\u00d7C,\nwithn\u2208 {1, . . . , N}.\nDecoder:The ViT-based decoders exchange information across all branches. Formally, let FT\nb\ndenote the tokens in the template branch, and let FI\nn,brefer to those in the n-th image branch, while\nb\u2208 {1, . . . , B} indicates the b-th decoder block. We compute the tokens as follows: At the b-th\ndecoder stage, the decoder block updates each feature\u2014either FT\nborFI\nn,b\u2014by cross -attending its\nown feature to all other features from stage b\u22121 . Information across images and between images\nand the template is exchanged and implicitly aligned\u2014this is key to enabling pose independence\nand allowing the template to \u201cinpaint\u201d missing content in unseen regions. Note, we employ separate\n4\ndecoder blocks for the template and image branches, however, all Nimage-branch decoders are\nmulti-layer transformers with shared weights.\nPrediction head:The last modules are the DPT-based [ 17] prediction heads. As before, we use\ndifferent heads for the template branch and the Nimage branches. In the template branch, the\nnetwork predicts residuals relative to the SMPL-X T-pose template in the average shape. When added\nto the rasterized template in UV space, this yields GTin Eq. (2). The image branches are rather\nstraightforward: in the n-th image branch, we directly predict the Gaussian primitives {GI\nn,ij}HI\ni=1WI\nj=1\nin Eq. (2), except that we multiply the predicted opacities by the input subject mask to exclude the\nbackground pixels from rendering. Here, HIandWIare the height and width of theinputimage.\nThat is, we predict one Gaussian for each foreground pixel in the input image, ensuring that all\nvisible fine-grained details are captured by the representation. Note that the set of Gaussians in the\nimage branches GIis in the canonical space (T-pose), not the space of the input poses, making it\nindependent of the human poses in the input images.\nFinally, we merge the predicted template and image Gaussians into our final reconstructed avatar by\ntaking the union of the two sets:G=GT\u222a GI.\n3.2 Articulation and Rendering\nWe now describe how to render the target image using our recovered avatar G, given target ex-\ntrinsics E, intrinsics K, and human shape and pose P= (\u03b2,\u03b8) . We use two steps:articulation\nandsplat rendering. In the articulation step, we warp the canonical T -posed Gaussian avatar G\ninto an articulated Gaussian under the target pose via linear blend skinning, i.e., GP=LBS(G;P) ,\nwhere the LBS weights for each Gaussian are part of the reconstruction module\u2019s output (shown\nin Eq. (3)). Given the warped GP, we render the image and its mask via Gaussian splatting,\n(I,M) =SplatRender(G P;E,K) . Hence, the Render function defined in Eq. (1)can be written\nas:Render(G;E,K,P) =SplatRender(LBS(G;P);E,K) . We discuss the details of this stage\nand the justification of the design choices in the appendix.\n3.3 Training losses\nThe reconstruction and rendering modules are end-to-end trainable. During training, we assume\nthat we have the corresponding camera poses and human poses for the target ground-truth images,\nwhich are only used for rendering during training. Neither at training-time nor at test-time does\nreconstruction depend on any poses of the input images.\nOur training loss is\nL=L mse+\u03b1 lpipsLlpips+\u03b1 chamfer Lchamfer +\u03b1 projLproj+\u03b1 lbsLlbs,(4)\nwhere\u03b1 \u2217are hyper-parameters.\nLmseandLlpipsare the MSE loss and the LPIPS loss between the rendered image Iand the ground\ntruthIgt.Lchamfer is the chamfer distance between the Gaussians from the template branch and the\nimage branchesGTandGI.\nWe use two additional losses. First, theprojection loss Lprojencourages each image-branch Gaus-\nsian can well explain its corresponding image. It consists of two terms: 1) MSE and LPIPS\nbetween the n-th input image and the rendering using only Gaussians from image branches. 2)\n\u21132distance between each pixel (i, j) and the projected mean of its predicted Gaussian GI\nn,ij:\r\r(i, j)\u2212Project\u0000\nLBS(\u00b5I\nn,ij,wI\nn,ij,P);K,E\u0001\r\r2\n2.Here, LBS(\u00b5;w,P) warps \u00b5via linear blend\nskinning with weights wunder pose P, and Project(\u00b5;K,E) projects the 3D point \u00b5to 2D using\nintrinsics Kand extrinsics E. Second, theLBS loss Llbs, which encourages the predicted LBS weights\nw\u2217\n\u00b7in Eq. (3)to follow the pseudo LBS weights obtained from the SMPL-X mesh provided in the\ntraining data. We call the LBS weights \u201cpseudo\u201d as they are rasterized from the SMPL-X mesh\nwithout clothes and are not strictly aligned with the clothed bodies.\n5\nw/ground-truthinput posesw/predictedinput posesw/o input poses\nGround truthGHGLIFe-GoMGHGLIFe-GoMOursFigure 3:Novel view synthesis from sparse input images on THuman2.0.Our approach performs\non par with the state-of-the-art in the lab setting (with ground-truth input poses in the reconstruction\nphase in test-time). Sometimes, ours even captures sharper details. In the real setting (with predicted\ninput poses in the reconstruction in test-time), the rendering quality of GHG and LIFe-GoM is largely\ndecayed. However, our approach without pose priors does not suffer from the bad poses.\n4 Experiments\n4.1 Experimental setup\nDatasets.We train our model on THuman2.0 [ 35], THuman2.1 [ 35], and HuGe100K [ 37]. For\nevaluation purposes, we adopt THuman2.0, HuGe100K and XHuman [ 21]. We follow GHG\u2019s\nsplit [ 10] on THuman2.0. On HuGe100K, we use the scripts provided by IDOL [ 37] and split each\ndirectory into 10 validation subjects, 50 test subjects and the rest for training. Please see the appendix\nfor details.\nBaselines.For novel view synthesis from sparse view images we compare to GoMAvatar [ 27], 3DGS-\nAvatar [ 15], iHuman [ 13], NHP [ 8], NIA [ 9], GHG [ 10], and LIFe-GoM [ 28] on THuman2.0. We use\nthree input images in this task. Note that all baselines use camera poses and human poses as input\nfor reconstruction in test-time, while we do not. Among the baselines, GoMAvatar, 3DGS-Avatar\nand iHuman require to be optimized for each subject, while others and our method are generalizable.\nWe evaluate two test-time settings: 1) Use of human poses predicted by the state-of-the-art SMPL-X\nprediction model MultiHMR [ 1] for test-time reconstruction, which is a realistic real-world setup; 2)\nUse of ground truth human poses provided by the dataset for test-time reconstruction, which is not\na realistic real-world setup, but nonetheless insightful. The target rendering poses in test-time are\nalways ground-truths from THuman2.0. We also evaluate on the XHuman dataset for cross-domain\nnovel view synthesis and novel pose synthesis, which we defer to the appendix due to the page\nlimit. For the task of novel view synthesis from a single image, we compare to IDOL [ 37] and\nLHM [ 16] on HuGe100K. Like our approach, both methods eliminate the need for input poses during\nthe reconstruction phase at test-time. We train IDOL and our method with the same training set on\nHuGe100K. Since LHM does not open-source the training script and the training dataset, we are\nunable to compare fairly. Hence, we directly assess the released checkpoints on HuGe100K\u2019s test set.\nEvaluation metrics.We assess PSNR, LPIPS\u2217, and FID following GHG [ 10] on THuman2.0 [ 35],\nXHuman [ 21], and HuGe100K [ 37]. We additionally compare the reconstruction time, i.e., the time\nspent to compute the 3D representation given input images.\n4.2 Comparisons with baselines\nNovel view synthesis from sparse images.We evaluate novel view synthesis from sparse images on\nthe THuman2.0 dataset in Tab. 1, using three input views. Other than input images and subject masks,\nfor reconstruction at test-time, GHG and LIFe-GoM also use the predicted SMPL-X poses from\nMultiHMR [ 1], the state-of-the-art SMPL-X predictor. For rendering, all methods use the ground-\n6\nTable 1:Comparisons to novel view synthesis from sparse input images on THuman2.0.All the\nbaselines take thepredictedposes in reconstruction in test-time from MultiHMR [ 1] as inputs. We\ncompare in the setting of no test-time optimization, only optimizing the camera poses, and optimizing\nboth camera poses and human poses in test time. Our approach, without pose priors, does not suffer\nfrom the errors in the predicted poses. Therefore, ours significantly outperforms the baselines.\nw/o test-time optim. w/ test-time cam pose optim. w/ test-time all pose optim.\nPSNR\u2191LPIPS\u2217\u2193FID\u2193 PSNR\u2191LPIPS\u2217\u2193FID\u2193 PSNR\u2191LPIPS\u2217\u2193FID\u2193\nGHG 16.96 185.67 71.46 19.50 160.52 70.37 - - -\nLIFe-GoM 19.70 146.19 63.34 20.52 142.53 62.78 22.59 130.71 60.74\nOurs 22.49 105.45 42.19 22.94 103.94 42.25 25.33 92.32 39.66\nTable 2:Comparisons on novel view synthesis from sparse input images in reconstruction in\ntest-time on THuman2.0.All the baselines take theground-truthposes as inputs, which is an\nunrealistic setting in real-world applications. We use three images as inputs. We compare to baselines\nw/o test-time pose optimization in the first block and w/ test-time pose optimization in the second\nblock, where we mark the methods with stars. Our approach, without any pose priors, achieves\ncomparable PSNR and better LPIPS\u2217and FID than baselines with ground-truth input poses.\nMethod PSNR\u2191LPIPS\u2217\u2193FID\u2193\nw/o test-time\noptimizationGoMAvatar [27] 23.05 133.98 87.51\n3DGS-Avatar [15] 21.25 160.48 157.21\niHuman [13] 22.77 131.67 101.70\nNHP [8] 23.32 184.69 136.56\nNIA [9] 23.20 181.82 127.30\nGHG [10] 21.90 133.41 61.67\nLIFe-GoM [28] 24.65110.82 51.27\nOurs 22.49105.45 42.19\nw/ test-time\noptimizationLIFe-GoM* [28] 25.87108.67 50.78\nOurs* 25.3392.32 39.66\ntruth camera poses and human poses provided by the dataset. Due to potential misalignments in scale\nand human pose between the predicted poses (used during reconstruction) and the ground-truth poses\n(used for rendering), we further conduct test-time pose optimization. Specifically, we optimize the\ntarget renderingcamera and human poses while keeping the reconstructed representation fixed. This\nprocedure is only used for evaluation purposes. Notably, since GHG is designed solely for novel view\nsynthesis and lacks support for animation, it cannot accommodate changes in human pose during\ntest-time optimization. Accordingly, we report three sets of numbers: w/o test-time pose optimization,\nw/ test-time camera pose optimization, and w/ test-time camera and human pose optimization. Here,\n\u201cpose\u201d refers to the target pose used in rendering. Our approach consistently outperforms GHG and\nLIFe-GoM by a margin in all evaluation protocols. We decrease LPIPS\u2217by over 35 points and FID\nby over 20 points compared to LIFe-GoM. We further compare with the baselines qualitatively in\nFig. 3. When the input poses are insufficiently accurate, the baseline methods struggle to reconstruct\nhigh-fidelity details, particularly in fine-grained regions such as the face and feet. In contrast, our\napproach remains unaffected.\nTo complete the quantitative comparison, we also report the standard evaluation setting on THu-\nman2.0 [ 10,28] in Tab. 2: poses for reconstruction and rendering are both ground truths provided by\nthe dataset. Note that this setting is unrealistic in real-world applications, since the ground-truth input\nposes are not available for reconstruction at test-time and must be predicted by off-the-shelf tools.\nEven without any pose priors as inputs, our approach improves upon methods that use ground-truth\nposes in LPIPS\u2217and FID. Our PSNR is slightly worse than the baselines. This is because PSNR\nfocuses more on pixel-level accuracy. The pose priors provide better alignment between the canonical\nreconstruction and the target poses (same as input poses). Test-time optimization for target poses in\nrendering can resolve the potential ambiguity in our approach, as shown in the bottom part of Tab. 2.\nThe qualitative comparisons in Fig. 3, shows that our method can 1) capture details better than the\nbaselines, e.g., the prints on the back of the second person; and, aided by the two-branch design, 2)\nimproves inpainting of unseen regions, e.g., the pants in the first example.\n7\nTable 3:Comparisons on novel view synthesis from a single image on HuGe100K.All methods\ndo not take input poses in the reconstruction phase. Our approach achieves better PSNR, LPIPS\u2217and\nFID compared to IDOL. We are not able to fairly compare to LHM due to missing training scripts of\nLHM. However, our reconstruction is much faster than LHM.\nMethod PSNR\u2191LPIPS\u2217\u2193FID\u2193Reconstruction time\u2193\nLHM-500M [16] 17.48 129.63 25.65 2.69s\nLHM-1B [16] 17.48 128.85 24.72 7.41s\nIDOL [37] 20.89 111.68 16.91311.89ms\nOurs 23.15 90.63 15.56321.58ms\nIn terms of the reconstruction time, our method needs 1.32s on an NVIDIA A100 when processing\nthree input images of resolution 1024\u00d71024 . This is slightly slower than LIFe-GoM\u2019s 908ms, but\nstill faster than the fastest per-scene optimization approach, iHuman, which requires more than 7s.\nGround truthLHM-1BIDOLOurs\nFigure 4:Comparisons on novel view synthesis\nfrom a single image on HuGe100K.Our model\ndetails better than IDOL and LHM. Meanwhile, it\ncan also reconstruct the challenging clothes, such\nas long dresses.Novel view synthesis from a single image.For\nnovel view synthesis from a single image we\ncompare to two recent works, IDOL [ 37] and\nLHM [ 16]. All three methods are similar in\nthree aspects: 1) They do not require poses as\ninput for reconstruction; 2) They need SMPL-\nX shape and pose for rendering; 3) The recon-\nstructions are in a pre-defined canonical pose.\nWe split HuGe100K [ 37] into training, valida-\ntion and test set using the scripts provided by\nIDOL. We report the performance on the test\nset in Tab. 3 and qualitative results in Fig. 4.\nWe use the same training setting for IDOL and\nour method. Since LHM training code wasn\u2019t\navailable prior to the submission, we are unable\nto conduct a fair comparison. Therefore, we\ndirectly apply the pretrained weights provided\nby LHM to HuGe100K\u2019s test set. Our method\nimproves upon IDOL, especially in PSNR and\nLPIPS\u2217. Benefitting from the image branch, our method captures observed details better. To verify,\nwe assess results on HuGe100K data separately for all-observed views (front-facing) and for views\nwith little/no overlap with the input image (three back views, camera rotated by at least 120 degree)\nin Tab. 4. The larger gain in all-observed views than views with little/no overlap with the input image\n(in LPIPS*, -38.72 vs. -19.37 over IDOL, -56.35 vs. -37.84 over LHM) highlights efficacy of our\nimage branch in capturing observed details. We also measure the reconstruction time, i.e., the time\ntaken to compute the canonical representation. Our method uses 321.58ms, taking a single image of\nresolution896\u00d7640as input. This is similar to IDOL, and much faster than LHM.\nWe further conduct the experiment of the single-view setting on THuman2.0 in Tab. 5. We use white\nbackground following IDOL\u2019s default setting. Same as on HuGe100K, we train our approach and\nIDOL on THuman2.0 while not finetuning LHM. Our approach significantly outperforms IDOL and\nLHM on PSNR, SSIM and LPIPS*.\n4.3 Ablation studies\nWe demonstrate the effectiveness of the key design choices. We use THuman2.1 as the training set\nand 100 held-out subjects from THuman2.0 as the test set in this section.\nAblations on the template branch and image branches.Our model consists of two branches: a\nsingle template branch that injects prior knowledge about the human and inpaints the missing parts,\nandNimage branches that output pixel-aligned Gaussians to ensure the fine-grained details observed\nin the input images are represented. We demonstrate the importance of both types of branches in\nTab. 6 and Fig. 5(left). When input images are very sparse, e.g., one single input image ( N= 1 ), the\ntemplate branch plays a vital role in inpainting large unseen regions. This is shown in the first block\nof Tab. 6: the model with only image branches performs the worst, especially in LPIPS\u2217and FID, as\nit fails to inpaint the missing regions; the model with only a template branch yields results similar to\n8\nTable 4:Comparisons on novel view synthesis from single image dividing views to all-observed\nviews and views with little/no overlap with the input image on HuGe100K.We split the test views\nby their overlap with the input image. We evaluate approaches on all-observed views (front-facing,\nsame view as the input images) and views with little/no observation (three back views, camera rotated\nby at least 120 degree from the input views). Compared to the baselines, our approach gains larger in\nall-observed views than views with little/no overlap with the input image highlights efficacy of our\nimage branch in capturing observed details.\nAll-observed views Views with little/no observation\nMethod PSNR\u2191LPIPS\u2217\u2193FID\u2193 PSNR\u2191LPIPS\u2217\u2193FID\u2193\nLHM-1B [16] 18.56 105.86 23.25 17.17 138.34 37.27\nIDOL [37] 23.16 88.23 20.30 20.38 119.87 26.99\nOurs 26.64 49.51 14.81 22.53 100.50 24.14\nTable 5:Comparisons on novel view synthesis from a single image on THuman2.0.All methods\ndo not take input poses in the reconstruction phase. Our approach achieves better PSNR, LPIPS\u2217and\nFID compared to IDOL.\nMethod PSNR\u2191LPIPS\u2217\u2193FID\u2193\nLHM-1B [16] 22.03 70.31 63.34\nIDOL [37] 23.47 66.62 83.37\nOurs 24.64 49.69 34.82\nthe model with all branches. When we increase the number of input images to 3, the image branches\nmodel most of the regions in better detail. Using the template branch only fails to capture the fine\ntextures, hence performing the worst in FID. The model with both types of branches attains the best\noverall results across different numbers of inputs.\nAblations on auxiliary losses LprojandLlbsin Eq. (4).We report the quantitative results in Tab. 7.\nThe projection loss Lprojencourages the image branches to predict pixel-aligned Gaussians so that\nthe observed details are modeled. Without the project loss, the image branches do not output any\nvisible Gaussians. Only the coarse template Gaussians are used in the rendering, which results in\nmissing fine-grained textures, as shown in Fig. 5(middle).\nOur approach reconstructs the avatar in the canonical T-pose, different from the poses in the input\nimages. Together with the predicted linear blend skinning weights, this reconstruction can be easily\nanimated to novel poses. Removing Llbsdoes not affect novel view synthesis, but the image branches\nsometimes reconstruct in the pose shown by the input images instead of the canonical T-pose. Further,\nthe model cannot learn appropriate LBS weights, affecting novel pose synthesis. We visualize the\nlearned LBS weights and the canonical space of the corresponding image branch in Fig. 5(right).\nLchamfer in Eq. (4)speeds up convergence of the image branches at the beginning of the training, but\nwe do not find it useful in the final model.\n4.4 Zero-shot downstream tasks\nPart segmentationInput imagePose estimationOne stageTwostages\nFigure 6:Zero-shot downstream\ntasks.Our model can be adapted to\npart segmentation and pose estimation\nwithout finetuning.In addition to novel view and pose synthesis, our model can\npotentially generalize to downstream tasks in a zero-shot\nmanner\u2014that is, without being explicitly trained for any of\nthem.\nPart segmentation.We predict the pixel-aligned linear\nblend skinning weights together with other Gaussian prim-\nitives in the image branches. The weights can be converted\nto segmentation masks, which indicate the body part that\na pixel belongs to. In the second column of Fig. 6, we\nshowcase the parts in different colors.\n9\nTable 6:Ablations on the template branch and image\nbranches.Taking N= 1 orN= 3 input images, we train\nand evaluate our approach with the template branch only,\nimage branches only and both types of branches. Using\nboth types of branches offers the best performance across\ndifferent numbers of input images.\nNinputs Branch types PSNR\u2191LPIPS\u2217\u2193FID\u2193\n1Template branch only 21.36 121.46 56.01\nImage branches only 21.51 134.96 66.53\nAll branches 21.41 124.36 57.78\n3Template branch only 22.13 108.85 47.60\nImage branches only 22.03 109.23 42.05\nAll branches 22.23 106.98 42.18Table 7:Ablations on auxiliary losses\nLprojandLlbs.We compare three train-\ning losses: (a) w/o Lproj, (b) w/o Llbs\nand (c) w/ all losses. Without Lproj, the\nimage branches cannot predict pixel-\naligned Gaussians. Without Llbs, the\nimage branches fail to reconstruct in\nthe canonical T-pose.\nLproj Llbs PSNR\u2191LPIPS\u2217\u2193FID\u2193\n\u2717 \u2713 22.09 110.37 48.98\n\u2713 \u2717 22.18 107.28 43.17\n\u2713 \u2713 22.23 106.98 42.18\nGround truthAll branchesImage branchonlyTemplatebranchonly\nGround truthw/o \ud835\udc3f!\"#$w/ \ud835\udc3f!\"#$\nInputw/o \ud835\udc3f%&'w/ \ud835\udc3f%&'\nLBSweightsCanonical renderingLBSweightsCanonical rendering\nFigure 5:Ablation studies. Left:Ablations on the template branch and image branches. Taking\na single image as input, template branch only cannot model fine details, such as the prints on the\nT-shirts (orange boxes). Image branches only miss unseen regions (green boxes). Using both branches\noffers the best overall quality.Middle:Ablation on Lproj. Without Lproj, only the template Gaussians\nare effective in the rendering, leading to blurry results.Right:Ablation on Llbs. Without supervised\nwith the pseudo LBS weights, the image branch fails to reconstruct in the canonical T-pose and to\npredict the correct LBS weights.\nHuman pose estimation.In the image branches, we also predict pixel-aligned 3D points in the\ncanonical T-pose. This naturally provides correspondences between the 3D canonical points and\nthe 2D pixel coordinates. Using correspondences, we can optimize the body poses, as shown in the\nthird column of Fig. 6 (\u201cPose estimation (one stage)\u201d). Since there can be multiple valid poses that\nsatisfy the correspondences, we then adopt a second stage for pose optimization, where we optimize\nthe poses via photometric losses, i.e., we treat Pinitialized from the first stage as the optimizable\nparameters in Eq. (1). The results are shown in the last column of Fig. 6 (\u201cPose estimation (two\nstages)\u201d). We highlight the improved poses with orange boxes.\n5 Conclusions\nWe propose NoPo-Avatar, a novel model that reconstructs the canonical representation from sparse\ninput images, while not using any human poses. Our model consists of two branches: a template\nbranch that injects prior knowledge about human shapes and inpaints the missing parts, and image\nbranches that predict pixel-aligned Gaussians to depict the fine-grained details. Our approach\nachieves comparable results to state-of-the-arts which use ground-truth pose priors during test-time\nreconstruction, and significantly outperforms those that use predicted pose priors during test-time\nreconstruction.\nBroader Impact.The proposed method can benefit AR/VR applications. Concerns remain regarding\nthe identity and authenticity of reconstructed avatars. To address these issues, we advocate for the\nuse of strict licensing agreements and responsible usage practices for the proposed methods.\nAcknowledgements.Work supported in part by NSF grants 2008387, 2045586, 2106825, NIFA\naward 2020-67021-32799, and OAC 2320345.\n10\nReferences\n[1]F. Baradel, M. Armando, S. Galaaoui, R. Br\u00e9gier, P. Weinzaepfel, G. Rogez, and T. Lucas. Multi-hmr:\nMulti-person whole-body human mesh recovery in a single shot. InECCV, 2024.\n[2]J. Chen, C. Li, J. Zhang, H. Chen, B. Huang, and G. H. Lee. Generalizable human gaussians from\nsingle-view image.arXiv, 2024.\n[3]Z. Chen, J. Yang, and H. Yang. Pref3r: Pose-free feed-forward 3d gaussian splatting from variable-length\nimage sequence.arXiv, 2024.\n[4]S. Hu, F. Hong, L. Pan, H. Mei, L. Yang, and Z. Liu. Sherf: Generalizable human nerf from a single image.\nInICCV, 2023.\n[5]S. Hu, T. Narihira, K. Fukuda, R. Sawata, T. Shibuya, and Y . Mitsufuji. Humangif: Single-view human\ndiffusion with generative prior.arXiv, 2025.\n[6]A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg,\nW.-Y . Lo, P. Dollar, and R. Girshick. Segment anything. InICCV, 2023.\n[7]N. Kolotouros, T. Alldieck, E. Corona, E. G. Bazavan, and C. Sminchisescu. Instant 3d human avatar\ngeneration using image diffusion models. InECCV, 2024.\n[8]Y . Kwon, D. Kim, D. Ceylan, and H. Fuchs. Neural human performer: Learning generalizable radiance\nfields for human performance rendering.NIPS, 2021.\n[9]Y . Kwon, D. Kim, D. Ceylan, and H. Fuchs. Neural image-based avatars: Generalizable radiance fields for\nhuman avatar modeling.ICLR, 2023.\n[10] Y . Kwon, B. Fang, Y . Lu, H. Dong, C. Zhang, F. V . Carrasco, A. Mosella-Montoro, J. Xu, S. Takagi,\nD. Kim, et al. Generalizable human gaussians for sparse view synthesis.ECCV, 2024.\n[11] M. Loper, N. Mahmood, J. Romero, G. Pons-Moll, and M. J. Black. SMPL: A skinned multi-person linear\nmodel.ACM TOG, 2015.\n[12] P. Pan, Z. Su, C. Lin, Z. Fan, Y . Zhang, Z. Li, T. Shen, Y . Mu, and Y . Liu. Humansplat: Generalizable\nsingle-image human gaussian splatting with structure priors.arXiv, 2024.\n[13] P. Paudel, A. Khanal, A. Chhatkuli, D. P. Paudel, and J. Tandukar. ihuman: Instant animatable digital\nhumans from monocular videos.arXiv, 2024.\n[14] G. Pavlakos, V . Choutas, N. Ghorbani, T. Bolkart, A. A. A. Osman, D. Tzionas, and M. J. Black. Expressive\nbody capture: 3D hands, face, and body from a single image. InCVPR, 2019.\n[15] Z. Qian, S. Wang, M. Mihajlovic, A. Geiger, and S. Tang. 3dgs-avatar: Animatable avatars via deformable\n3d gaussian splatting. InCVPR, 2024.\n[16] L. Qiu, X. Gu, P. Li, Q. Zuo, W. Shen, J. Zhang, K. Qiu, W. Yuan, G. Chen, Z. Dong, and L. Bo. Lhm:\nLarge animatable human reconstruction model from a single image in seconds.arXiv, 2025.\n[17] R. Ranftl, A. Bochkovskiy, and V . Koltun. Vision transformers for dense prediction. InICCV, 2021.\n[18] S. Serengil and A. Ozpinar. A benchmark of facial recognition pipelines and co-usability performances of\nmodules.Journal of Information Technologies, 2024.\n[19] S. I. Serengil and A. Ozpinar. Lightface: A hybrid deep face recognition framework. InASYU, 2020.\n[20] S. I. Serengil and A. Ozpinar. Hyperextended lightface: A facial attribute analysis framework. InICEET,\n2021.\n[21] K. Shen, C. Guo, M. Kaufmann, J. Zarate, J. Valentin, J. Song, and O. Hilliges. X-avatar: Expressive\nhuman avatars. InCVPR, 2023.\n[22] S. Szymanowicz, C. Rupprecht, and A. Vedaldi. Splatter image: Ultra-fast single-view 3d reconstruction.\nInCVPR, 2024.\n[23] Z. Tang, Y . Fan, D. Wang, H. Xu, R. Ranjan, A. Schwing, and Z. Yan. MV-DUSt3R+: Single-Stage Scene\nReconstruction from Sparse Views In 2 Seconds. InCVPR, 2025.\n[24] Z. Tang, Y . Fan, D. Wang, H. Xu, R. Ranjan, A. Schwing, and Z. Yan. Mv-dust3r+: Single-stage scene\nreconstruction from sparse views in 2 seconds. InCVPR, 205.\n11\n[25] B. Wang, R. Ouyang, X. Wang, Z. Zhu, G. Zhao, C. Ni, G. Huang, L. Liu, and X. Wang. Humandreamer-x:\nPhotorealistic single-image human avatars reconstruction via gaussian restoration.arXiv, 2025.\n[26] S. Wang, V . Leroy, Y . Cabon, B. Chidlovskii, and J. Revaud. Dust3r: Geometric 3d vision made easy. In\nCVPR, 2024.\n[27] J. Wen, X. Zhao, Z. Ren, A. Schwing, and S. Wang. GoMAvatar: Efficient Animatable Human Modeling\nfrom Monocular Video Using Gaussians-on-Mesh. InCVPR, 2024.\n[28] J. Wen, A. G. Schwing, and S. Wang. Life-gom: Generalizable human rendering with learned iterative\nfeedback over multi-resolution gaussians-on-mesh. InICLR, 2025.\n[29] Z. Weng, J. Liu, H. Tan, Z. Xu, Y . Zhou, S. Yeung-Levy, and J. Yang. Single-view 3d human digitalization\nwith large reconstruction models.arXiv, 2024.\n[30] J. Xiao, Q. Zhang, Y . Nie, L. Zhu, and W.-S. Zheng. Rogsplat: Learning robust generalizable human\ngaussian splatting from sparse multi-view images. InCVPR, 2025.\n[31] K. Xiong, Y . Feng, Q. Zhang, J. Jiao, Y . Zhao, Z. Liang, H. Gao, and R. Wang. Mvd-hugas: Human\ngaussians from a single image via 3d human multi-view diffusion prior.arXiv, 2025.\n[32] Y . Xue, X. Xie, R. Marin, and G. Pons-Moll. Human 3diffusion: Realistic avatar creation via explicit 3d\nconsistent diffusion models.arXiv, 2024.\n[33] Y . Yang, F. Liu, Y . Lu, Q. Zhao, P. Wu, W. Zhai, R. Yi, Y . Cao, L. Ma, Z.-J. Zha, and J. Dong. Sigman:\nScaling 3d human gaussian generation with millions of assets.arXiv, 2025.\n[34] B. Ye, S. Liu, H. Xu, X. Li, M. Pollefeys, M.-H. Yang, and S. Peng. No pose, no problem: Surprisingly\nsimple 3d gaussian splats from sparse unposed images. InICLR, 2025.\n[35] T. Yu, Z. Zheng, K. Guo, P. Liu, Q. Dai, and Y . Liu. Function4d: Real-time human volumetric capture\nfrom very sparse consumer rgbd sensors. InCVPR, 2021.\n[36] S. Zheng, B. Zhou, R. Shao, B. Liu, S. Zhang, L. Nie, and Y . Liu. Gps-gaussian: Generalizable pixel-wise\n3d gaussian splatting for real-time human novel view synthesis. InCVPR, 2024.\n[37] Y . Zhuang, J. Lv, H. Wen, Q. Shuai, A. Zeng, H. Zhu, S. Chen, Y . Yang, X. Cao, and W. Liu. Idol: Instant\nphotorealistic 3d human creation from a single image. InCVPR, 2025.\n[38] A. Zubekhin, H. Zhu, P. Gotardo, T. Beeler, M. Habermann, and C. Theobalt. Giga: Generalizable sparse\nimage-driven gaussian avatars.arXiv, 2025.\n12\nNeurIPS Paper Checklist\n1.Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper\u2019s contributions and scope?\nAnswer: [Yes]\nJustification: The claims in the abstract and introduction match the main contributions.\nGuidelines:\n\u2022The answer NA means that the abstract and introduction do not include the claims\nmade in the paper.\n\u2022The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. A No or\nNA answer to this question will not be perceived well by the reviewers.\n\u2022The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings.\n\u2022It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper.\n2.Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\nJustification: We discuss the limitations and possible solutions in the supplementary due to\npage limits.\nGuidelines:\n\u2022The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper.\n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.\n\u2022The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally). The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be.\n\u2022The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. In general, empirical results often\ndepend on implicit assumptions, which should be articulated.\n\u2022The authors should reflect on the factors that influence the performance of the approach.\nFor example, a facial recognition algorithm may perform poorly when image resolution\nis low or images are taken in low lighting. Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon.\n\u2022The authors should discuss the computational efficiency of the proposed algorithms\nand how they scale with dataset size.\n\u2022If applicable, the authors should discuss possible limitations of their approach to\naddress problems of privacy and fairness.\n\u2022While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that aren\u2019t acknowledged in the paper. The authors should use their best\njudgment and recognize that individual actions in favor of transparency play an impor-\ntant role in developing norms that preserve the integrity of the community. Reviewers\nwill be specifically instructed to not penalize honesty concerning limitations.\n3.Theory assumptions and proofs\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof?\nAnswer: [NA]\n13\nJustification: The paper does not include theoretical results.\nGuidelines:\n\u2022 The answer NA means that the paper does not include theoretical results.\n\u2022All the theorems, formulas, and proofs in the paper should be numbered and cross-\nreferenced.\n\u2022All assumptions should be clearly stated or referenced in the statement of any theorems.\n\u2022The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition.\n\u2022Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material.\n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.\n4.Experimental result reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)?\nAnswer: [Yes]\nJustification: We provide essential implementation details in the main paper and the supple-\nmentary.\nGuidelines:\n\u2022 The answer NA means that the paper does not include experiments.\n\u2022If the paper includes experiments, a No answer to this question will not be perceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhether the code and data are provided or not.\n\u2022If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or verifiable.\n\u2022Depending on the contribution, reproducibility can be accomplished in various ways.\nFor example, if the contribution is a novel architecture, describing the architecture fully\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\nbe necessary to either make it possible for others to replicate the model with the same\ndataset, or provide access to the model. In general. releasing code and data is often\none good way to accomplish this, but reproducibility can also be provided via detailed\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\nof a large language model), releasing of a model checkpoint, or other means that are\nappropriate to the research performed.\n\u2022While NeurIPS does not require releasing code, the conference does require all submis-\nsions to provide some reasonable avenue for reproducibility, which may depend on the\nnature of the contribution. For example\n(a)If the contribution is primarily a new algorithm, the paper should make it clear how\nto reproduce that algorithm.\n(b)If the contribution is primarily a new model architecture, the paper should describe\nthe architecture clearly and fully.\n(c)If the contribution is a new model (e.g., a large language model), then there should\neither be a way to access this model for reproducing the results or a way to reproduce\nthe model (e.g., with an open-source dataset or instructions for how to construct\nthe dataset).\n(d)We recognize that reproducibility may be tricky in some cases, in which case\nauthors are welcome to describe the particular way they provide for reproducibility.\nIn the case of closed-source models, it may be that access to the model is limited in\nsome way (e.g., to registered users), but it should be possible for other researchers\nto have some path to reproducing or verifying the results.\n5.Open access to data and code\nQuestion: Does the paper provide open access to the data and code, with sufficient instruc-\ntions to faithfully reproduce the main experimental results, as described in supplemental\nmaterial?\n14\nAnswer: [No]\nJustification: We will release the codes and model weights upon acceptance.\nGuidelines:\n\u2022 The answer NA means that paper does not include experiments requiring code.\n\u2022Please see the NeurIPS code and data submission guidelines ( https://nips.cc/\npublic/guides/CodeSubmissionPolicy) for more details.\n\u2022While we encourage the release of code and data, we understand that this might not be\npossible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not\nincluding code, unless this is central to the contribution (e.g., for a new open-source\nbenchmark).\n\u2022The instructions should contain the exact command and environment needed to run to\nreproduce the results. See the NeurIPS code and data submission guidelines ( https:\n//nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n\u2022The authors should provide instructions on data access and preparation, including how\nto access the raw data, preprocessed data, intermediate data, and generated data, etc.\n\u2022The authors should provide scripts to reproduce all experimental results for the new\nproposed method and baselines. If only a subset of experiments are reproducible, they\nshould state which ones are omitted from the script and why.\n\u2022At submission time, to preserve anonymity, the authors should release anonymized\nversions (if applicable).\n\u2022Providing as much information as possible in supplemental material (appended to the\npaper) is recommended, but including URLs to data and code is permitted.\n6.Experimental setting/details\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\nAnswer: [Yes]\nJustification: We describe the detailed experimental settings in the main paper and the\nsupplementary.\nGuidelines:\n\u2022 The answer NA means that the paper does not include experiments.\n\u2022The experimental setting should be presented in the core of the paper to a level of detail\nthat is necessary to appreciate the results and make sense of them.\n\u2022The full details can be provided either with the code, in appendix, or as supplemental\nmaterial.\n7.Experiment statistical significance\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\ninformation about the statistical significance of the experiments?\nAnswer: [No]\nJustification: Error bars are not reported because it would be too computationally expensive.\nGuidelines:\n\u2022 The answer NA means that the paper does not include experiments.\n\u2022The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\ndence intervals, or statistical significance tests, at least for the experiments that support\nthe main claims of the paper.\n\u2022The factors of variability that the error bars are capturing should be clearly stated (for\nexample, train/test split, initialization, random drawing of some parameter, or overall\nrun with given experimental conditions).\n\u2022The method for calculating the error bars should be explained (closed form formula,\ncall to a library function, bootstrap, etc.)\n\u2022 The assumptions made should be given (e.g., Normally distributed errors).\n15\n\u2022It should be clear whether the error bar is the standard deviation or the standard error\nof the mean.\n\u2022It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\nof Normality of errors is not verified.\n\u2022For asymmetric distributions, the authors should be careful not to show in tables or\nfigures symmetric error bars that would yield results that are out of range (e.g. negative\nerror rates).\n\u2022If error bars are reported in tables or plots, The authors should explain in the text how\nthey were calculated and reference the corresponding figures or tables in the text.\n8.Experiments compute resources\nQuestion: For each experiment, does the paper provide sufficient information on the com-\nputer resources (type of compute workers, memory, time of execution) needed to reproduce\nthe experiments?\nAnswer: [Yes]\nJustification: We provide the training details in the appendix.\nGuidelines:\n\u2022 The answer NA means that the paper does not include experiments.\n\u2022The paper should indicate the type of compute workers CPU or GPU, internal cluster,\nor cloud provider, including relevant memory and storage.\n\u2022The paper should provide the amount of compute required for each of the individual\nexperimental runs as well as estimate the total compute.\n\u2022The paper should disclose whether the full research project required more compute\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\ndidn\u2019t make it into the paper).\n9.Code of ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethicshttps://neurips.cc/public/EthicsGuidelines?\nAnswer: [Yes]\nJustification: We follow the Code of Ethics.\nGuidelines:\n\u2022The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n\u2022If the authors answer No, they should explain the special circumstances that require a\ndeviation from the Code of Ethics.\n\u2022The authors should make sure to preserve anonymity (e.g., if there is a special consid-\neration due to laws or regulations in their jurisdiction).\n10.Broader impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed?\nAnswer: [Yes]\nJustification: We discuss the broader impacts in the paper.\nGuidelines:\n\u2022 The answer NA means that there is no societal impact of the work performed.\n\u2022If the authors answer NA or No, they should explain why their work has no societal\nimpact or why the paper does not address societal impact.\n\u2022Examples of negative societal impacts include potential malicious or unintended uses\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\ngroups), privacy considerations, and security considerations.\n16\n\u2022The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments. However, if there is a direct path to\nany negative applications, the authors should point it out. For example, it is legitimate\nto point out that an improvement in the quality of generative models could be used to\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\nthat a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster.\n\u2022The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology.\n\u2022If there are negative societal impacts, the authors could also discuss possible mitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\nfeedback over time, improving the efficiency and accessibility of ML).\n11.Safeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)?\nAnswer: [NA]\nJustification: The paper poses no such risks.\nGuidelines:\n\u2022 The answer NA means that the paper poses no such risks.\n\u2022Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage guidelines or restrictions to access the model or implementing\nsafety filters.\n\u2022Datasets that have been scraped from the Internet could pose safety risks. The authors\nshould describe how they avoided releasing unsafe images.\n\u2022We recognize that providing effective safeguards is challenging, and many papers do\nnot require this, but we encourage authors to take this into account and make a best\nfaith effort.\n12.Licenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected?\nAnswer: [Yes]\nJustification: We properly respected and credited the assets.\nGuidelines:\n\u2022 The answer NA means that the paper does not use existing assets.\n\u2022 The authors should cite the original paper that produced the code package or dataset.\n\u2022The authors should state which version of the asset is used and, if possible, include a\nURL.\n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.\n\u2022For scraped data from a particular source (e.g., website), the copyright and terms of\nservice of that source should be provided.\n\u2022If assets are released, the license, copyright information, and terms of use in the\npackage should be provided. For popular datasets, paperswithcode.com/datasets\nhas curated licenses for some datasets. Their licensing guide can help determine the\nlicense of a dataset.\n\u2022For existing datasets that are re-packaged, both the original license and the license of\nthe derived asset (if it has changed) should be provided.\n17\n\u2022If this information is not available online, the authors are encouraged to reach out to\nthe asset\u2019s creators.\n13.New assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets?\nAnswer: [NA]\nJustification: The paper does not release new assets.\nGuidelines:\n\u2022 The answer NA means that the paper does not release new assets.\n\u2022Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates. This includes details about training, license,\nlimitations, etc.\n\u2022The paper should discuss whether and how consent was obtained from people whose\nasset is used.\n\u2022At submission time, remember to anonymize your assets (if applicable). You can either\ncreate an anonymized URL or include an anonymized zip file.\n14.Crowdsourcing and research with human subjects\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)?\nAnswer: [NA]\nJustification: The paper does not involve crowdsourcing nor research with human subjects.\nGuidelines:\n\u2022The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n\u2022Including this information in the supplemental material is fine, but if the main contribu-\ntion of the paper involves human subjects, then as much detail as possible should be\nincluded in the main paper.\n\u2022According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector.\n15.Institutional review board (IRB) approvals or equivalent for research with human\nsubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained?\nAnswer: [NA]\nJustification: The paper does not involve crowdsourcing nor research with human subjects.\nGuidelines:\n\u2022The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n\u2022Depending on the country in which research is conducted, IRB approval (or equivalent)\nmay be required for any human subjects research. If you obtained IRB approval, you\nshould clearly state this in the paper.\n\u2022We recognize that the procedures for this may vary significantly between institutions\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\nguidelines for their institution.\n\u2022For initial submissions, do not include any information that would break anonymity (if\napplicable), such as the institution conducting the review.\n16.Declaration of LLM usage\n18\nQuestion: Does the paper describe the usage of LLMs if it is an important, original, or\nnon-standard component of the core methods in this research? Note that if the LLM is used\nonly for writing, editing, or formatting purposes and does not impact the core methodology,\nscientific rigorousness, or originality of the research, declaration is not required.\nAnswer: [NA]\nJustification: The core method development in this research does not involve LLMs as any\nimportant, original, or non-standard components.\nGuidelines:\n\u2022The answer NA means that the core method development in this research does not\ninvolve LLMs as any important, original, or non-standard components.\n\u2022Please refer to our LLM policy ( https://neurips.cc/Conferences/2025/LLM )\nfor what should or should not be described.\n19\nAppendix \u2014 NoPo-Avatar: Generalizable and Animatable Avatars from\nSparse Inputs without Human Poses\nThe appendix is structured as follows:\n\u2022 We provide implementation details in Sec. A.\n\u2022 We elaborate on the baseline setup and experimental details in Sec. B.\n\u2022Due to space limitations in the main paper, we clarify the experimental settings and provide\nadditional results in Sec. C.\n\u2022 We discuss the limitations and future works in Sec. D.\nWe also provide a webpage index.html in the supplementary. It lists videos for freeview rendering\ncomparisons to baselines, novel pose synthesis, and cross-domain generalization.\nA Implementation Details\nA.1 Articulation\nWe describe in detail how we articulate the canonical representation into pose P= (\u03b2,\u03b8) , where\n\u03b2is the shape and \u03b8is the pose represented by the orientation of each joint. We assume that the\ncanonical representation Gfollows the skeleton of the average template shape in T-pose, i.e., the\nskeleton acquired by setting the shape parameters and pose parameters to all zeros in SMPL-X. When\nwarping with pose P, we first reshape the canonical representation consuming the target shape \u03b2,\nand then rotate the reshaped Gaussians in T-pose by the pose\u03b8.\nStep 1: Reshaping by \u03b2.People are of various heights and bone lengths. To reshape the canonical\nrepresentation, we first compute the skeleton of the average shape and the skeleton of shape \u03b2, both\nin T-pose. Then we calculate the transformation between each pair of corresponding bones in two\nskeletons, denoted as {si, Ri, ti}O\ni=1. Here, si\u2208R accounts for the scaling factor of the bone length,\nandRi\u2208SO(3) andti\u2208R3are the rotation and translation, respectively. Odenotes the number\nof bones. In Eq. (3), we define the LBS weights as w\u2217\n.={w\u2217\n.i}O\ni=1\u2208RO. We then transform the\nGaussian\u2019s mean\u00b5\u2217\n., and rescale and rotate a Gaussian\u2019s covariance:\n\u00b5\u2217\u03b2\n.=OX\ni=1w\u2217\n.i(siRi\u00b5\u2217\n.+ti),\u03a3\u2217\u03b2\n.= OX\ni=1w\u2217\n.i(siRi)!T\nR\u2217T\n.S\u2217T\n.S\u2217\n.R\u2217\n. OX\ni=1w\u2217\n.i(siRi)!\n.(5)\nR\u2217T\n.S\u2217T\n.S\u2217\n.R\u2217\n.is the original covariance matrix, where R\u2217\n.andS\u2217\n.are the matrix form of the predicted\nrotation and scale of the Gaussian. We rescale the covariance matrices accordingly to avoid artifacts\ndue to reshaping.\nStep 2: Rotating by \u03b8.\u00b5\u2217\u03b2\n.and\u03a3\u2217\u03b2\n.are still in T-pose. We then warp it by pose \u03b8, which represents\nthe orientations of eachjoint. This step is a standard linear blend skinning. However, we need to\nconvert the predicted LBS weights associated with bones w\u2217\n.\u2208ROto weights associated with joints.\nAs the skeleton is defined in a tree structure, each bone connects a parent joint and a child joint. We\nsimply assign the LBS weights associated with the bone to its parent joint. We illustrate this choice\nin Fig. 7.\nAlternatively, the reconstruction module could directly predict the canonical representation in the\ntarget shape, thereby eliminating the need for Step 1 in the articulation. However, we find that\ndecoupling the shape from the canonical representation and retaining Step 1 is empirically important.\nDue to the decoupling, the canonical representation Gfrom the reconstruction stage follows a\nfixed skeleton, i.e., the skeleton of the average template shape in T-pose, which alleviates the scale\nambiguity inherent in this task. We show the artifacts without decoupling the shape from Gand Step\n1 in Fig. 8(middle) and the improved rendering quality with our design choice in Fig. 8(right).\nAt test-time, we use the predicted or user-specified shape and pose as \u03b2and\u03b8, similar to IDOL [ 37]\nand LHM [ 16]. These parameters are only used in the articulation and rendering stage, not the\nreconstruction stage. Importantly, our approach remains pose-free in the reconstruction stage.\n20\nLBS weights assigned to bonesParentjointParentjointChildjointChildjointChildjointChildjointChildjoint\nLBS weights assigned to jointsFigure 7:Converting LBS weights assigned to bones to LBS weights assigned to joints.We\nillustrate the LBS weights around the pelvis root. Our model predicts the LBS weights corresponding\nto the three bones connected to the pelvis root. The pelvis root serves as the parent joint of three child\njoints. The LBS weights of the three bones (left) are aggregated and reassigned to the pelvis root\n(right).\nGround-truthw/o decoupling shape from canonical spaceDecoupling shape from canonical space\nFigure 8:Decoupling the shape from the canonical representation.Without the decoupling\n(middle), the model sometimes fails to reconstruct the thin structures, e.g., hands. Decoupling the\nshape from the canonical representation eases the reconstruction ambiguity.\nA.2 Training and Inference Details\nThe learnable embedding in the template encoder FT\n0is in the shape of 16\u00d716\u00d71024 . We use\n\u03b1lpips= 0.05,\u03b1 chamfer = 0.1,\u03b1 proj= 1.0, and\u03b1 lbs= 0.01in Eq. (4).\nThe model is pretrained from NoPoSplat [ 34]. To reduce training time, we first train using a low-\nresolution template and images. The model is first trained at a resolution of 256\u00d7256 for 300K\niterations, then upsampled to 512\u00d7512 for another 300K iterations, and finally fine-tuned at full\nresolution for an additional 50K iterations. We train for 80K iterations in the full resolution of\n896\u00d7640on HuGe100K.\nWe use a batch size of 4 for all experiments on THuman2.0, THuman2.1, and THuman2.1 +\nHuGe100K. The training process takes roughly 12 days, 4 days for each stage. For the comparison\nto LHM and IDOL on HuGe100K, we use a batch size of 16 in the first two stages and 8 in the last\nstage.\nWe use two NVIDIA L40S for training at a resolution of 256\u00d7256 , four NVIDIA L40S for\n512\u00d7512 resolution training, and four NVIDIA H200 for the last stage, i.e., for full resolution\ntraining. Inference, including both reconstruction and rendering, requires approximately 24GB of\nGPU memory for three input images at a resolution of 1024\u00d71024 , and around 11GB for a single\nimage at896\u00d7640.\nA.3 Zero-shot Human Pose Estimation\nWe show that our model can perform human pose estimation via analysis-by-synthesis in a zero-shot\nmanner in Sec. 4.4. For this, we use a two-stage optimization process.\n21\nStage 1: Optimization with projection losses.Recall that the image branches in our model predict\npixel-aligned Gaussians, which means that we obtain one Gaussian with mean \u00b5I\nn,ijand LBS weights\nwI\nn,ijfor each pixel (i, j) in the n-th image branch. We optimize for the human shape and pose\nPby minimizing\r\r(i, j)\u2212Project\u0000\nLBS(\u00b5I\nn,ij,wI\nn,ij,P);K,E\u0001\r\r2\n2summed over all foreground\npixels, where LBS(\u00b5;w,P) warps \u00b5via linear blend skinning with weights wunder pose P, and\nProject(\u00b5;K,E) projects the 3D point \u00b5to 2D using intrinsics Kand extrinsics E. We set Eto\nthe identity matrix and letKhave fixed focal lengths and principal points.\nStage 2: Optimization with photometric losses.There can be multiple valid poses that satisfy\nthe correspondences. Meanwhile, the reconstruction from image branches is sometimes unreliable\nfor fine-grained structures. We therefore optimize with a photometric loss in the second stage.\nSpecifically, we treat Pas the learnable parameter, initialized by the result of the first stage. We warp\nthe Gaussians by Pand fixed LBS weights, render the images, and compute the photometric loss\nbetween the rendered images and the ground-truth, which is the image we estimate the pose from in\npose estimation.\nAs is shown in Sec. 4.4, optimizing with projection losses solely already leads to good results. This is\na side benefit from our design choice of predicting pixel-aligned Gaussians from the image branches.\nAdding the second stage which optimizes with photometric losses further improves the estimation.\nNote that optimizing with photometric losses alone does not lead to meaningful results, since it\nrequires a good initialization.\nB Experimental Details\nB.1 Datasets\nTHuman2.0.THuman2.0 provides 526 3D scans as well as the corresponding SMPL-X parameters.\nWe follow GHG [ 10] to render them into 64 multiview images in the resolution of 1024\u00d71024 and\nsplit them into 426 subjects for training and 100 for evaluation. THuman2.0 uses a special license\nagreement1, which we follow.\nTHuman2.1.THuman2.1 extends THuman2.0 to \u223c2500 scans. Note that the 526 scans from\nTHuman2.0 are included in THuman2.1. The newly added subjects are combined with THuman2.0\u2019s\n426 training subjects as the new training set. The license agreement is the same as THuman2.0.\nXHuman.XHuman consists of 20 subjects. It provides the 3D scans and corresponding SMPL-X\nposes of multiple motion sequences for each subject. We follow LIFe-GoM [ 28] to evaluate the\ncross-domain generalization in novel view synthesis on XHuman. The pose estimator MultiHMR\nfails in one sample so that we cannot get the estimated input poses for the reconstruction phase of\nGHG and LIFe-GoM. So we remove that sample from the test set. We additionally evaluate novel\npose synthesis on the XHuman dataset. For each subject, images from the first frame of a sequence\nare used as input. The two sequences designated as test sequences are selected as target poses for\nnovel pose synthesis. For each target pose, we render images from three different views. XHuman\nuses a special license agreement2, which we follow.\nHuGe100K.HuGe100K is a synthetic dataset that contains more than 100K subjects. For each\nsubject, it provides a video of 360-degree freeview rendering, camera poses of each frame and\nthe SMPL-X parameters. We use SAM [ 6] to acquire the subject masks. The dataset groups the\nsubjects into directories. We follow the scripts provided by IDOL [ 37] and split each directory into 10\nvalidation subjects, 50 test subjects and the rest for training. We use frame 19 as the input view and\nsample 6 views as the target views for evaluation. HuGe100K uses DeepFashion\u2019s license3, which\nwe follow.\nB.2 Pose Estimation\nThe baselines, including GHG [ 10] and LIFe-GoM [ 28], require camera poses and human poses of\nthe input images as input for the reconstruction phase. As mentioned in the paper, the ground-truth\n1https://github.com/ytrock/THuman2.0-Dataset?tab=readme-ov-file#agreement\n2https://xhumans.ait.ethz.ch\n3https://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html\n22\nposes are typically not available in real applications. Instead, the poses have to be estimated by\noff-the-shelf tools prior to reconstruction.\nWe choose MultiHMR [ 1] as the pose estimator for all our experiments requiring estimated poses.\nMultiHMR is the state-of-the-art end-to-end pose estimator. Different from RoGSplat [ 30], we do\nnot use EasyMoCap4as the pose estimator, since EasyMoCap requires calibrated camera poses,\nwhich are unavailable in real settings. Complex pose estimation pipelines such as the one used in\nExAvatar [ 21] are also not preferred. Though accurate, they take over 20 minutes to acquire the poses\nfrom a monocular video, which is impractical in real-world applications.\nMultiHMR predicts per-image poses. Since LIFe-GoM can take images with different poses as\ninput, we feed the poses independently predicted from each view as the input poses to LIFe-GoM.\nGHG, however, must take multiview images with the same pose. Therefore, we use the human poses\npredicted from one view (most frontal view) as the input human pose and register the other views.\nB.3 Additional Information on Test-time Pose Optimization\nWe perform test-time optimization of both camera pose and human pose in Tab. 1 and Tab. 2. In\nTab. 1, we use predicted poses for GHG and LIFe-GoM during test-time reconstruction. During\nrendering, we always use the ground-truths from the dataset as the target poses for evaluation. The\nposes in the reconstruction stage may not align with the poses in the rendering stage. For example,\nthe predicted poses assume a tall person photographed by a faraway camera, while the ground-truth\ntarget poses assume a shorter person captured by a closer camera. We want to rule out the potential\ndrop in the evaluation metrics due to such misalignment. Though not taking any poses in the test-time\nreconstruction, our approach may also have the same misalignment between the reconstruction and\nthe rendering poses. Therefore, we optimize the target camera poses and human posesonly for\nevaluation purposes. We similarly apply test-time optimization to our method, as shown in Tab. 2,\nand also apply it to LIFe-GoM using ground-truth input poses for a fair comparison.\nC Addtional Experiments\nWe present additional experiments, including cross-domain generalization results, a quantitative\nanalysis of how different types of noise in input poses affect baseline performance during test-\ntime reconstruction, and evidence that injecting noise during training does not necessarily enhance\nrobustness to noisy poses at test time.Additional qualitative video comparisons are provided in\nthe supplementary material atindex.html.\nC.1 Cross-dataset Evaluation With Sparse Images as Input\nWe show results of experiments with different training settings in Tab. 8. We choose three training\nsettings of different scales: 1) THuman2.0 which has 426 subjects; 2) THuman2.1 which has 2345\nsubjects, around 5 \u00d7the scale of THuman2.0; 3) THuman2.1 + HuGe100K which has over 100K\nsubjects, over 200 \u00d7the scale of THuman2.0. Meanwhile, we evaluate on three settings: 1) In-domain\nevaluation on novel view synthesis: The test set is 100 subjects in THuman2.0. We call it in-domain\nsince the training and test sets are both from THuman2.0. 2) Cross-domain evaluation on novel view\nsynthesis: The test set consists of subjects from XHuman, which differ from the training set. 3)\nCross-domain evaluation on novel pose synthesis: The test set is also from XHuman, but we evaluate\nnovel poses.\nFor both in-domain and cross-domain evaluation, novel view synthesis or novel pose synthesis, ours\nconsistently outperforms LIFe-GoM with predicted input poses.\nMore importantly, we find that our method benefits from scaling up, especially in the cross-domain\nevaluation. When switching the training data from THuman2.0 to THuman2.1, our approach improves\nby 0.6 in PSNR for novel view synthesis and by 0.4 for novel pose synthesis in the cross-domain\nevaluation. Similar improvements can also be observed in LPIPS* and FID scores: LPIPS* improves\nfrom 129.04 to 119.84 and FID improves from 47.58 to 41.55. However, LIFe-GoM, which incor-\nporates more hand-crafted inductive biases, does not exhibit such a scaling ability. Equipped with\nan even larger dataset, such as HuGe100K, we also observe an improvement in PSNR. But LPIPS*\n4https://github.com/zju3dv/EasyMocap\n23\nGround truthLIFe-GoMOursTHuman2.0THuman2.1THuman2.0 + HuGe100KTraining set:THuman2.0THuman2.1THuman2.0 + HuGe100KFigure 9:Novel pose synthesis and cross-domain generalization from sparse input images using\ndifferent training set sizes.Our approach scales with the size of the training set, achieving improved\nidentity recovery, whereas LIFe-GoM does not exhibit this behavior.\nTable 8:Comparison on in-domain evaluation and cross-domain generalization using different\ntraining set sizes.We evaluate in-domain novel view synthesis on THuman2.0 and cross-domain\nnovel view synthesis and novel pose synthesis on XHuman. The input poses to LIFe-GoM are\npredicted. Our approach improves with larger-scale training data, while LIFe-GoM does not. The\nlines labeled with * indicate results w/ test-time pose optimization. We optimize both camera poses\nand human poses to eliminate the misalignment between the poses used during the reconstruction\nand the target poses used for rendering. In all settings, ours consistently outperforms LIFe-GoM.\nTraining set THuman2.0 THuman2.1 THuman2.1 + IDOL\nPSNR\u2191LPIPS*\u2193FID\u2193 PSNR\u2191LPIPS*\u2193FID\u2193 PSNR\u2191LPIPS*\u2193FID\u2193\nIn-domain evaluation on THuman2.0, novel view synthesis\nLIFe-GoM 19.70 146.19 63.34 19.41 147.75 62.11 19.50 154.21 67.99\nLIFe-GoM* 23.01 129.56 60.68 22.86 128.34 58.96 22.65 139.30 65.77\nOurs 22.49 105.45 42.19 22.23 106.98 42.18 22.41 114.14 48.69\nOurs* 25.33 92.32 39.66 25.94 87.94 38.45 25.73 97.39 46.07\nCross-domain evaluation on XHuman, novel view synthesis\nLIFe-GoM 20.94 136.42 56.02 20.50 141.24 59.94 20.60 153.43 69.54\nLIFe-GoM* 24.39 116.25 52.19 24.22 117.07 54.43 23.91 132.16 64.47\nOurs 20.96 129.04 47.58 21.56 119.84 41.55 22.52 114.95 43.59\nOurs* 25.05 100.86 41.82 26.23 91.17 37.41 26.70 92.18 40.05\nCross-domain evaluation on XHuman, novel pose synthesis\nLIFe-GoM 20.95 132.86 45.22 20.49 137.62 47.69 20.56 147.52 55.05\nLIFe-GoM* 23.98 116.42 42.64 23.70 117.81 44.37 23.56 129.09 52.01\nOurs 20.93 130.89 38.65 21.36 124.96 34.67 21.74 124.91 38.27\nOurs* 24.79 103.09 34.68 25.48 97.81 31.79 25.76 99.12 35.13\nand FID do not improve further. This is likely because HuGe100K synthesizes multiview images of\navatars using diffusion models. The multiview consistency is hence not guaranteed. Inconsistency in\nthe training data may induce blurred rendering, which eventually affects perceptual-based evaluation\nmetrics. We showcase the scaling ability qualitatively in Fig. 9. Since THuman2.0 and THuman2.1\nonly contain Asian people, our approach may not generalize well to other ethnicities. Our approach\nbenefits from the increasing diversity in HuGe100K and reconstructs better in the cross-domain data.\nLIFe-GoM, due to hand-crafted inductive biases in its architecture, works better when trained with a\nsmall-scale dataset.\nC.2 Comparison of Different Noise Levels in the Input Poses During Test-time Reconstruction\nIn Fig. 1(a), we show that the baselines including GHG and LIFe-GoM are sensitive to noise in input\nposes during test-time reconstruction. We achieve this by injecting Gaussian noise or by predicting\nthe input poses. We provide the quantitative results of injecting Gaussian noise or using predicted\nposes in Tab. 9, matching the results in Fig. 1(a). We also provide a qualitative comparison with\nLIFe-GoM under noisy input poses in Fig. 10. As the noise level of the input poses increases during\ntest-time reconstruction, LIFe-GoM\u2019s performance degrades noticeably. In contrast, our method,\nwhich does not rely on pose priors, remains robust and unaffected by such noise.\n24\nTable 9:Comparison of different noise levels in input poses during test-time reconstruction.\nGHG and LIFe-GoM take input camera and human poses during test-time reconstruction. We\nevaluate their robustness to noisy input poses by injecting Gaussian noise into ground-truth poses or\nby predicting the input poses. For each Gaussian noise level, we average over 5 runs and report the\nmeans and standard deviations. Our method, which does not take any pose priors, outputs the same\nresult, regardless of the noise in input poses. The numbers reported in this table are the same as those\nin Fig. 1(a).\nNoise level No noise std=0.01\nMethod PSNR\u2191LPIPS*\u2193FID\u2193 PSNR\u2191LPIPS*\u2193FID\u2193\nGHG 21.90 133.41 61.67 21.28\u00b10.04 136.71\u00b10.25 62.06\u00b10.24\nLIFe-GoM 24.65 110.82 51.27 22.95\u00b10.03 118.34\u00b10.21 52.87\u00b10.08\nOurs 22.51 105.85 42.37 22.51 105.85 42.37\nNoise level std=0.03 std=0.05\nMethod PSNR\u2191LPIPS*\u2193FID\u2193 PSNR\u2191LPIPS*\u2193FID\u2193\nGHG 19.73\u00b10.08 150.08\u00b10.54 64.99\u00b10.31 18.55\u00b10.09 163.39\u00b10.70 68.45\u00b10.75\nLIFe-GoM 20.90\u00b10.07 133.07\u00b10.35 57.28\u00b10.26 19.73\u00b10.08 144.13\u00b10.47 63.60\u00b10.76\nOurs 22.51 105.85 42.37 22.51 105.85 42.37\nNoise level pred\nMethod PSNR\u2191LPIPS*\u2193FID\u2193\nGHG 16.96 185.67 71.46\nLIFe-GoM 19.70 146.19 63.34\nOurs 22.51 105.85 42.37\nInput pose:Ground truthLIFe-GoMOursGround-truthstd=0.01std=0.03std=0.05PredN/A\nFigure 10:Comparison of different noise levels in input poses during test-time reconstruction.\nWe qualitatively demonstrate the performance degradation of LIFe-GoM under noisy input poses\nduring test-time reconstruction. To simulate noisy conditions, we inject Gaussian noise with varying\nstandard deviations or use predicted poses as inputs. In contrast, our method\u2014relying on no pose\npriors\u2014remains unaffected by pose noise.\nC.3 Training LIFe-GoM With Noisy Inputs\nWe train both GHG and LIFe-GoM using ground-truth input poses, but employ predicted poses\nduring test-time reconstruction in the experiments discussed in the main paper and above. This raises\na natural question: is the performance drop caused by the gap in input pose quality between training\nand testing? In this section, we demonstrate that training with noisy input poses does not improve\nthe models\u2019 robustness to pose inaccuracies. In fact, training with accurate ground-truth poses often\nyields the best performance across all levels of input pose noise during test-time reconstruction.\nTaking LIFe-GoM as an example, we train it using 1) ground-truth input poses, 2) input poses with\nsynthetic Gaussian noise of std=0.01, 3) input poses with synthetic Gaussian noise of std=0.03, 4)\n25\nTable 10:Training LIFe-GoM with noisy inputs.We train LIFe-GoM using ground-truth input\nposes, poses with synthetic Gaussian noise, and predicted poses (pred) during the reconstruction\nstage. Training with ground-truth poses often yields the best performance across varying levels of\npose noise at test time. This indicates that the performance drop observed when using predicted\nposes during reconstruction is not due to a training\u2013testing gap and cannot be mitigated simply by\nintroducing noise during training.\nPSNR\u2191LPIPS\u2217\u2193FID\u2193 PSNR\u2191LPIPS\u2217\u2193FID\u2193 PSNR\u2191LPIPS\u2217\u2193FID\u2193\nTest input noise\nNo noise std=0.01 std=0.03\nTraining\ninput\nnoiseNo noise 24.13 110.12 49.65 22.95 118.34 52.87 20.90 133.07 57.28\nstd=0.01 24.11 114.97 52.87 22.74 121.70 53.85 20.67 136.17 57.36\nstd=0.03 23.80 118.92 55.60 22.58 124.89 56.33 20.54 139.03 59.26\nstd=0.05 23.38 122.18 57.55 22.32 127.71 58.57 20.41 141.24 61.39\nPred 23.38 121.66 57.34 22.32 127.44 58.22 20.65 139.44 62.38\nTest input noise\nstd=0.05 Pred\nTraining\ninput\nnoiseNo noise 19.73 144.13 63.60 19.70 146.19 63.34\nstd=0.01 19.48 146.97 62.45 19.63 148.11 64.02\nstd=0.03 19.18 151.75 62.58 19.63 150.64 66.46\nstd=0.05 19.10 153.60 64.42 19.54 152.07 68.48\nPred 19.59 148.79 66.80 19.78 149.49 65.77\nTable 11:Robustness to noisy training poses.We train our approach with noisy training poses. We\nadd synthetic Gaussian noise to the training poses.\nNoise in training poses PSNR\u2191LPIPS\u2217\u2193FID\u2193\nNo noise 22.49 105.45 42.19\nstd=0.1 22.59 110.12 49.39\nstd=0.3 20.58 138.67 73.20\ninput poses with synthetic Gaussian noise of std=0.05 and 5) predicted poses from MultiHMR, and\nevaluate each in all levels of noise. The results are shown in Tab. 10. The more noise we add to the\ninput poses during training, the worse the performance we get across all levels of input pose noise\nduring test-time reconstruction. Hence, the performance drop observed when using predicted poses\nduring reconstruction cannot be mitigated simply by introducing noise during training. Our approach,\nwhich takes no pose priors, is an option to eliminate the need for accurate camera and human poses\nduring test time. It is more suitable for real-world applications.\nC.4 Sensitivity to noise in training poses\nWe use the ground-truth poses provided by the dataset duringtraining. To analyze the robustness to\nposes during training, we add synthetic Gaussian noise. We report the quantitative results in Tab. 11\nWe find that our approach is robust to some noise, e.g., Gaussian noise of std=0.1: On THuman2.0, it\nachieves a PSNR/LPIPS*/FID of 22.59/110.12/49.39, compared to 22.49/105.45/42.19 with ground-\ntruth training poses reported in Tab. 2. If we further perturb the ground-truth poses with more\nsignificant Gaussian noise of std=0.3, the PSNR/LPIP*/FID drops to 20.58/138.67/73.20. We do not\nthink robustness to the target poses in training is a big concern. Collecting high-quality training data\nis a one-time effort. Once trained, the model does not rely on poses for reconstruction at inference\ntime. This is different from pose-dependent methods, which still require high-quality input poses\nduring inference for reconstruction.\nC.5 Validating the identity shift\nWe assess identity shift quantitatively via the facial verification tool DeepFace [ 19,20,18], on\nHuGe100K. It verifies if two images show the same person. We think this is a good surrogate for\nidentity shift. We adopt all 400 front-facing HuGe100K images and report the verified-rate, i.e., the\npercentage of \u201crendered image\u201d-\u201cground-truth image\u201d-pairs recognized as the same person, and the\ncosine distance between the rendered face and the ground-truth image in VGG-Face\u2019s feature space\n26\nTable 12:Validating the identity shift on HuGe100K.To quantify the identity shift, we report\nthe verified-rate and the cosine distance between the rendered image and the ground-truth image in\nVGG-Face\u2019s feature space.\nNoise in training poses Verified-rate\u2191Cosine distance\u2193\nLHM-1B 95.75% 0.30\nIDOL 93.75% 0.45\nOurs 95.25% 0.32\nAn example of inconsistent data in HuGe100K \nSemi-transparent regions in renderingRendered front viewRendered back view\nFigure 11:Limitations. Left:Incorrect hand geometry.Middle:When trained on HuGe100K,\nour model sometimes produces semi-transparent regions. We suspect the reason is that the data in\nHuGe100K lack multiview consistency.Right:Taking the front view as the only input view, our\nmodel renders a sharp front view. But the back view is blurry due to inpainting.\nin Tab. 12. Our method without dedicated face module achieves a verified-rate of 95.25% and an\naverage cosine distance of 0.32. LHM-1B has 95.75%/0.30. Note, LHM uses a dedicated face image\ncrop as additional input and a feature pyramid for improved facial modeling. Both ours and LHM\noutperform IDOL\u2019s 93.75%/0.45. We think this shows that our model does not suffer from a severe\nidentity shift as results are comparable to methods with dedicated face module.\nD Limitations\nFailure in modeling expressions and hands.Our model currently does not support expression\nretargeting. In most of the input images, hands occupy very few pixels or are heavily occluded.\nTherefore, predicting the LBS weights and corresponding 3D locations in the canonical T-pose for\neach pixel in the image branches is sometimes challenging. So the hands are not as sharp as other\nregions, as is shown in Fig. 11(left). Similar issues happen in LHM. A possible solution is to train\ntwo separate models, especially for faces and hands, respectively, compromising the training time\nand reconstruction speed.\nSensitivity to inconsistent training data.Our model sometimes predicts blurry results in unseen\nregions and semi-transparent regions on the boundary when trained on HuGe100K. Notably, such\nissues do not happen when training on THuman2.0. We suspect that this is because the HuGe100K\ndata is synthesized by diffusion models. It hence sometimes lacks multiview consistency. We\nshowcase a HuGe100K human subject which is inconsistent in different views in Fig. 11(middle). As\na regression-based method, ours is prone to output blurry results, semi-transparent regions, or small\nfloaters on the boundary. A more consistent and higher-quality training set will resolve this issue.\nBlurry inpainting.Even though our model improves upon LIFe-GoM in inpainting small unseen\nregions, our model, as a regression-based method, struggles to hallucinate sharp and high-frequency\ndetails in large unseen regions, e.g., the dotted pattern on the shirt in Fig. 11(right). Generative\nmodels are generally more suitable for hallucinating high-frequency details, which is important for\nreconstruction and rendering from a single image.\n27",
      "summary": "NoPo-Avatar is a novel approach for reconstructing animatable 3D human avatars from sparse images without requiring human pose inputs. This method eliminates reliance on pose estimates, making avatar reconstruction more robust and broadly applicable.",
      "key_contributions": [
        "Introduces NoPo-Avatar, a pose-free method for 3D avatar reconstruction from sparse images.",
        "Demonstrates that reliance on pose estimates degrades reconstruction quality when poses are noisy.",
        "Shows superior performance over existing baselines in practical settings without ground-truth poses.",
        "Validates the method's effectiveness across multiple challenging datasets."
      ],
      "methodology": "NoPo-Avatar reconstructs 3D avatars directly from one or more input images, removing the need for pose or camera pose information at test-time.",
      "results": "NoPo-Avatar outperforms state-of-the-art methods in real-world scenarios without ground-truth poses and achieves comparable results in controlled lab settings with ground-truth poses.",
      "keywords": [
        "3D avatar reconstruction",
        "animatable avatars",
        "pose-free",
        "sparse images",
        "human modeling",
        "robustness",
        "generalization"
      ],
      "status": "new",
      "notes": null,
      "created_at": "2025-11-23T20:14:06.245458",
      "needs_llm_processing": false
    },
    "5": {
      "id": "2511.16671v1",
      "title": "Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation",
      "authors": [
        {
          "name": "Ziyu Guo",
          "affiliation": null
        },
        {
          "name": "Renrui Zhang",
          "affiliation": null
        },
        {
          "name": "Hongyu Li",
          "affiliation": null
        },
        {
          "name": "Manyuan Zhang",
          "affiliation": null
        },
        {
          "name": "Xinyan Chen",
          "affiliation": null
        },
        {
          "name": "Sifan Wang",
          "affiliation": null
        },
        {
          "name": "Yan Feng",
          "affiliation": null
        },
        {
          "name": "Peng Pei",
          "affiliation": null
        },
        {
          "name": "Pheng-Ann Heng",
          "affiliation": null
        }
      ],
      "abstract": "Recent advances in visual generation have increasingly explored the integration of reasoning capabilities. They incorporate textual reasoning, i.e., think, either before (as pre-planning) or after (as post-refinement) the generation process, yet they lack on-the-fly multimodal interaction during the generation itself. In this preliminary study, we introduce Thinking-while-Generating (TwiG), the first interleaved framework that enables co-evolving textual reasoning throughout the visual generation process. As visual content is progressively generating, textual reasoning is interleaved to both guide upcoming local regions and reflect on previously synthesized ones. This dynamic interplay produces more context-aware and semantically rich visual outputs. To unveil the potential of this framework, we investigate three candidate strategies, zero-shot prompting, supervised fine-tuning (SFT) on our curated TwiG-50K dataset, and reinforcement learning (RL) via a customized TwiG-GRPO strategy, each offering unique insights into the dynamics of interleaved reasoning. We hope this work inspires further research into interleaving textual reasoning for enhanced visual generation. Code will be released at: https://github.com/ZiyuGuo99/Thinking-while-Generating.",
      "arxiv_id": "2511.16671v1",
      "arxiv_url": "http://arxiv.org/abs/2511.16671v1",
      "pdf_url": "https://arxiv.org/pdf/2511.16671v1",
      "published": "2025-11-20T18:59:52Z",
      "updated": "2025-11-20T18:59:52Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "full_text": "Thinking-while-Generating:\nInterleaving Textual Reasoning throughout Visual Generation\nZiyu Guo\u22171, Renrui Zhang\u2020\u22172, Hongyu Li\u22173, Manyuan Zhang\u20203, Xinyan Chen2\nSifan Wang, Yan Feng3, Peng Pei3, Pheng-Ann Heng1\nCUHK1IMIXR &2MMLab3Meituan\nProject Page:https://think-while-gen.github.io\nAbstract\nRecent advances in visual generation have increasingly ex-\nplored the integration of reasoning capabilities. They in-\ncorporate textual reasoning, i.e., think, either before (as\npre-planning) or after (as post-refinement) the generation\nprocess, yet they lack on-the-fly multimodal interaction dur-\ning the generation itself. In this preliminary study, we in-\ntroduceThinking-while-Generating(TWIG),the firstin-\nterleaved framework that enables co-evolving textual rea-\nsoning throughout the visual generation process. As vi-\nsual content is progressively generating, textual reason-\ning is interleaved to both guide upcoming local regions\nand reflect on previously synthesized ones. This dynamic\ninterplay produces more context-aware and semantically\nrich visual outputs. To unveil the potential of this frame-\nwork, we investigate three candidate strategies, zero-shot\nprompting, supervised fine-tuning (SFT) on our curated\nTWIG-50K dataset, and reinforcement learning (RL) via\na customizedTWIG-GRPO strategy, each offering unique\ninsights into the dynamics of interleaved reasoning. We\nhope this work inspires further research into interleaving\ntextual reasoning for enhanced visual generation. Code will\nbe released at: https://github.com/ZiyuGuo99/\nThinking-while-Generating.\n1. Introduction\nVisual generation have developed rapidly with diffusion [ 37,\n39,40] and autoregressive [ 7,45,52] models, enabling\nhigh-fidelity synthesis across diverse domains [ 32,35,56].\nDespite impressive visual quality, today\u2019s generators often\nstruggle with long-horizon composition, multi-entity rela-\ntions, and adherence to nuanced textual instructions. Starting\nfrom\u2018Generation with CoT\u2019[ 17,46], a growing line of work\n\u2217Equal Contribution\u2020Project Lead\nThe glowing sunset sky and distant \ncastle on the horizon.\nThe castle\u2019s \nreflection \non the lake and \nthe unicorn \nstands in light.\nFlowers glowing under \nthe warm evening light.\n\u3001\nThe soft rippling \nreflection of \nthe unicorn in \ngolden water.\nThinking -while -Generating\nInterleave\nInterleave\nInterleave\nInterleaveFigure 1.Interleaving Textual Reasoning throughout Visual\nGeneration.Inspired by the image-interleaved reasoning in textual\nresponses [ 8,33,43,59], we reverse the modality flow and weave\ntextual thoughts into the unfolding canvas, delivering on-the-fly\nguidance and reflection throughout synthesis.\nexploresreasoningas a remedy, typically injecting chain-of-\nthoughts in the language modality to assist visual synthesis.\nExisting CoT-based approaches can be grouped bywhere\nthe textual reasoning is applied, as compared in Figure 2:\n\u2022Think before generation as a pre-planning aid.Meth-\nods [ 11,23,27] first produce a structured or free-form\nplan, e.g., detailed captions, scene layouts, or object at-\ntributes and relations, and then condition the image genera-\ntor on this plan. This improves global coherence and entity\nplacement, but the plan is fixed once generation begins,\nlimiting nuanced guidance and mid-course correction.\n1arXiv:2511.16671v1  [cs.CV]  20 Nov 2025\nAs a Pre -planning Aid(a) Think -before-Generation\nThink\n(b) Think -after-Generation(c) Thinking -while-Generating (Ours)\nGenerate\nGenerateThink\nAs a Post -refinement StageReflectGenerate\nGenerate\nGenerateThink\u2026\u2026Text Prompt\nA white  cat \nsunbathing  on \na windowsill  \nsurrounded  \nby plantsA fluffy \nwhite cat is \npeacefully \nsunbathing on \u2026by \nsoft green plants.\nThe cat\u2019s \nfur and \nsome leaves \nappear slightly \nunnatural.Sunlit \nwindow with \nfloating dust and \nplant leaves visible.\nTerracotta \npots holding \nlush green \nhouseplants in \nsoft warm light.\nFluffy \nwhite cat \npeacefully resting, \neyes closed, fur \nglowing softly.\nText Prompt\nA white  cat \nsunbathing  on \na windowsill  \nsurrounded  \nby plants\nText Prompt\nA white  cat \nsunbathing  on \na windowsill  \nsurrounded  \nby plants\nReflect\nReflectThink\nThinkAs an Interleaved, \non-the-fly Guidance\nFigure 2.Comparison ofWherethe Textual Reasoning is Applied in Visual Generation:(a)Think-before-Generation[ 11,23,27] injects\na pre-planning thought prior to the synthesis, limiting fine-grained control and later correction; (b)Think-after-Generation[ 17,26,36] verifies\nand revise the image once it is complete, lacking nuanced, timely adjustment with extra inference cost; (c) OurThinking-while-Generating\ninterleaves thoughts and reflections throughout the synthesis, providing on-the-fly. co-evolving guidance.\n\u2022Think after generation as a post-refinement stage.Meth-\nods [ 17,26,61] synthesize the entire image first, and then\nelicit textual feedback via self-critique or external verifiers,\niteratively revising the visual errors. These approaches\nhelp with local fixes and attribute binding, but reasoning\nis only loosely coupled to the synthesis trajectory with-\nout fine-grained, timely revision and, importantly, incur\nadditional, costly extra inference rounds.\nGiven these limitations in visual generation, we note a\ncomplementary trend in visual understanding: recent large\nmultimodal models (LMMs) [ 9,13,25,57,59] perform\nimage-text interleaved reasoning, adaptively weaving inter-\nmediate visual evidence (e.g., detected objects, zoomed-in\nregions, or tagged images) into textual CoTs to improve in-\nterpretation and analysis. Inspired by this paradigm, we pose\na natural question: as illustrated in Figure 1,Can we invert\nthe flow and interleave text into the intermediate visual gen-\neration process, providing on-the-fly, co-evolving reasoning\nthat guides synthesis as it unfolds?\nIn this preliminary study, we presentthe firstinterleaved\nframework for visual generation that keeps textual reasoning\nin the loop, termed asThinking-while-Generating(TWIG),\nas compared in Figure 2 (c). As our approach is compatible\nwith multiple models and task settings, for clarity and future\nextensibility, we adopt the unified understanding-generation\nLMM (ULM) [ 7,50,52,60] with autoregressive generationparadigms, e.g., Janus-Pro [ 7], and experiment on text-to-\nimage scenarios in our study.\nGiven a text prompt, the model first interprets the instruc-\ntion and plans an optimal interleave schedule, i.e., how many\nsteps to use and how to partition the canvas into local regions\nfor progressive synthesis. While generating each region, the\nmodel conducts on-the-fly textual reasoning and grounds its\nthoughts in the current partial visual state. This interleaved\nthinkstep serves two roles:(i)it produces nuanced guidance\nfor the upcoming synthesis, and(ii)it critiques and reflects\non the previously generated content. In this way, textual\nreasoning co-evolves with the visual modality, providing\ndetailed, step-by-step directives. The image can be dynam-\nically revised and precisely steered as it unfolds within a\nsingle generative trajectory.\nWe consider three candidate routes forThinking-while-\nGenerating, and investigate which, if any, proves effective:\n\u2022Can zero-shot prompting alone achieve the goal?We craft\ninterleave-aware prompts to directly elicit global plans and\nreasoning thoughts. This route reveals the latent capacity\nof ULMs to self-organize interleaved reasoning without\nparameter updates, but can suffer instability.\n\u2022Does supervised fine-tuning (SFT) benefit the perfor-\nmance?We categorize the understanding and generation\nprocess into nine subtasks, and curate a dataset, TWIG-\n2\n50K, for fine-tuning ULM, aiming to improve instruction\nadherence and reduce visual hallucination.\n\u2022Will reinforcement learning (RL) further unlock its po-\ntential?We optimize the interleaved reasoning policy\nof ULM via a customized GRPO [ 42] algorithm, TWIG-\nGRPO, to push the performance boundary, investigating\ndifferent RL approaches and reward designs.\nOur experiments indicate that the ULM itself exhibits\nstrong zero-shot capability forThinking-while-Generating.\nWith carefully designed prompts, it substantially improves\nJanus-Pro on T2I-CompBench(++) [ 19,20] without addi-\ntional training. Building on this, SFT with TWIG-50K pro-\nvides further modest yet consistent gains, leading to more\nstable behavior compared with the zero-shot baseline. Fi-\nnally, optimization with our TWIG-GRPO algorithm yields\nconsiderable improvements, underscoring the value of RL\nfor decidingwhento think,whatto say, andhowto refine.\nTaken together, these findings, though preliminary, are in-\nformative: they demonstrate the feasibility of interleaving\ntextual reasoning during generation, and highlight this direc-\ntion as a promising avenue for advancing visual synthesis.\nIt is worth noting that two relevant concurrent works,\nIRG [ 21] and Uni-CoT [ 36], attempt to \u2018interleave\u2019 rea-\nsoning with generation, but still treat the visual systhesis\nprocess as a monolithic block, like a combination ofthink-\nbefore-generationandthink-after-generation. They are well-\nperformed with unique insights, but not truly interleaving\nreasoning within the generative process itself, limiting the\ngranularity and controllability.\n2.Thinking-while-Generating\nIn Section 2.1, we first introduce the design scope and appli-\ncability ofThinking-while-Generating. Then, in Section 2.2,\nwe present its overall pipeline and core components of the\nframework in detail.\n2.1. Scope and Applicability\nAiming for generalization and extensibility,Thinking-while-\nGeneratingis conceptually compatible with diverse settings\nalong the following three axes:\n\u2022System Architecture.The framework can be instanti-\nated either as (i) a pipeline that couples a text-to-image\nmodel [ 2,10,37] with an LMM [ 1,29,54], where the\nLMM specializes in producing interleaved reasoning for\nthe text-to-image outputs; or (ii) a ULM [ 7,52,60] that\nperforms textual reasoning and visual generation within a\nsingle backbone.\n\u2022Generation Paradigm.The framework is applicable for\nvisual generation with diffusion [ 12,30,38], discrete dif-\nfusion [ 4,52,53], and autoregressive models [ 7,44,45].\nFor continuous diffusion models, textual thoughts are in-terleaved at selected denoising steps; for discrete diffusion\nand autoregressive models, thoughts are inserted between\nsegments of visual tokens to guide upcoming spans.\n\u2022Task Scenarios.The framework applies beyond T2I, e.g.,\nimage-to-image [ 3,18,56], text-to-video [ 14,16,47], text-\nto-3D [ 15,28,35], and related generative tasks: as long as\nan LMM (or ULM) can provide reasoning thoughts for the\ntarget modality, they can be interleaved to steer generation.\nAs a preliminary study, we adopt asingleULM with an\nautoregressive generation paradigm (e.g., Janus-Pro [ 7]) for\nclarity of exposition, promising headroom, and end-to-end\ntraining efficiency. We denote its understanding forward\npass byULM uand the generation forward pass byULM g.\n2.2. Framework Overview\nFigure 3 presents the overallThinking-while-Generating\n(TWIG) framework, which interleaves textual reasoning with\nvisual generation through three schemes:whento think,what\nto say, andhowto refine.\nWhento Think (Scheduling).Given an input prompt T,\nULM ufirst determines an interleaved reasoning schedule,\ndenoted asS={V k}K\nk=1, according to:\nS= ULM u(T),\nwhere each Vkdenotes a target visual region at which rea-\nsoning is applied (e.g., token spans in autoregressive and\ndiscrete diffusion models, or timestep windows in continu-\nous diffusion models). This decouples the generation process\ninto smaller, more controllable sub-tasks guided by the inter-\nleaved textual reasoning. Scheduling can be static (fixed K,\nuniform spacing) or adaptive (variable K, content-dependent\nVk). In Section 3.1, we investigate different schedules and\nfind that a static schedule with K= 3 performs the best,\nbased on the heuristic that most images consist of three\nsemantic components: upper background, central content,\nand lower background. Additionally, current capabilities\nofULM uare limited in reliably generating well-structured\nadaptive schedules, which remains a future work.\nWhatto Say (Reasoning Content).At each scheduled rea-\nsoning point, ULM uprovides a textual thought \u03c4kintended\nto guide the generation of the visual region Vk. This thought\nserves as a localized sub-prompt exclusively targeted at Vk,\noffering finer-grained guidance and alignment than prior\nthink-before-generationapproaches. The generation of \u03c4kis\nconditioned on three elements, i.e., the input prompt T, the\nprevious thoughts {\u03c4j}j<k, and the visual content generated\nfor prior regions{V j}j<k, formulated as:\n\u03c4k= ULM u(T,{\u03c4 j}j<k,{Vj}j<k).\n3\n\u2264\ud835\udf03>\ud835\udf03A cozy wooden cabin beside a calm \nlake at sunrise, snow -covered pines \nand mountains glowing warmly.\nStep 1 Step KText Prompt \ud835\udc47\nStep 2Soft pastel  sunrise  above  \nrugged  snowy  mountains,  \nwarm  golden  light spreading  \nacross  quiet  winter  sky.Dense  pine forest  coated  in \nsnow,  a cozy cabin  with \nchimney  smoke  rising  into \ncold morning  air.\nULMu\n ULMu\nULMg\n ULMg\nULMu\nThink\ud835\udf0f1\nGenerate \ud835\udcb11Think\ud835\udf0f\ud835\udc3eThinkWhen to Think\nWhat to Say\nHow toRefine\nScheduling \ud835\udc46\nGenerate \ud835\udcb12 Generate \ud835\udcb1KThink\ud835\udf0f2\nReflectScore \ud835\udc5f1\nULMu\nReflectScore \ud835\udc5f2\nULMu\n\u2264\ud835\udf03>\ud835\udf03\nMirror -still lake reflecting  \nmountains,  trees,  and cabin,  \ncreating  a peaceful  and \nsymmetrical  winter  scene .\nULMu\nULMg\nReflectScore \ud835\udc5fK\nULMu\n\u2264\ud835\udf03>\ud835\udf03\nFinal\nImage\n...\n\u0ddd\ud835\udf0f1 \u0ddd\ud835\udf0f2 \u0dde\ud835\udf0fK\u2026\nFigure 3.Overall Pipeline ofThinking-while-Generating.The framework comprises three components:When to Thinkfor globally\ndetermining the interleaved generation schedule;What to Sayfor producing the step-by-step textual thought as fine-grained guidance; and\nHow to Refinefor a region-level reflection on the current canvas with optional corrective updates. ULM uandULM gdenote to apply a\nsingle ULM for understanding and generation, respectively.\nThis allows \u03c4kto incorporate accumulated contextual infor-\nmation and to plan appropriately for the next visual segment.\nSubsequently, ULM gsynthesizes the target region Vk, con-\nditioned on all reasoning thoughts and the visual content\nproduced up to by:\nVk= ULM g({\u03c4j}j\u2264k,{Vj}j<k).\nIt is important to note that ULM gis only required to pos-\nsess text-to-image capabilities, no need for image-to-image\nfunctionality. This is because the visual context {Vj}j<k\nis not provided as image input to the model. Instead, we\ndirectly extend the textual pre-context from {\u03c4j}j<k to\n{\u03c4j}j\u2264kat the beginning of the token sequence, while pre-\nserving the generated visual content{V j}j<kunchanged at\nthe end of the sequence. This modification preserves the\nautoregressive generation process within a single trajectory,\nwithout introducing discontinuities or new generation rounds,\nas illustrated in Figure 4 (a).Howto Refine (Reflection).After generating each visual\nregionVk, we allow ULM uto perform an immediate, region-\nlevel revision step that couples visual critique and an optional\ncorrection process. This enables finer-grained corrections\nwhile significantly reducing computational cost compared to\npriorthink-after-generationapproaches that conduct global\npost-revision. Before producing the next reasoning thought\n\u03c4k+1,ULM ufirst generates a reflection tuple ck= (r k,\u02c6\u03c4k),\ngiven all the generated textual and visual contents as:\nck= ULM u(T,{\u03c4 j}j\u2264k,{Vj}j\u2264k),\nwhere rk\u2208[0,100] is an integer representing the critic\nscore assigned to the current region Vk, and \u02c6\u03c4kis a revised\nsub-caption intended for potential correction. The score rk\nevaluates the semantic alignment and visual coherence of Vk\nwith respect to its guiding prompt \u03c4k. Ifrkexceeds a prede-\nfined threshold \u03b8, the model proceeds directly to generate the\nnext reasoning thought without revision. Otherwise, a local\n4\nvvv\ud835\udf0f1 \ud835\udcb11Textual Tokens Visual Tokens\n(a)Insert the 2nd thought\nGenerate\n\ud835\udf0f1\n\ud835\udf0f1\ud835\udf0f2 \ud835\udcb11\n\ud835\udf0f2 \ud835\udcb11\ud835\udcb12\n\ud835\udcb12\n\ud835\udf0f1 \u0ddd\ud835\udf0f2 \ud835\udcb11\u0de2\ud835\udcb12(b) Replace the reflectionRe-GenerateFigure 4.Illustration of Interleaved Token Sequence:(a) InWhat\nto Say, the textual pre-context extends from {\u03c4j}j<kto{\u03c4j}j\u2264k\n(K= 2 ), guiding the generation of the next Vkwhile leaving the\nearlier {Vj}j<kuntouched; (b) InHow to Refine, the thought \u03c4kis\nrevised to \u02c6\u03c4k, and only the local region \u02c6Vkis re-generated to replace\nVk. Neither operation requires the ULM to possess image-to-image\ncapabilities, and both preserve a single text-to-image generation\ntrajectory without launching a fresh pass or full re-generation.\nreflection is triggered to refine only the current sub-region,\nguided by\u02c6\u03c4 k, as defined by:\n\u02c6Vk= ULM g({\u03c4j}j<k,\u02c6\u03c4k,{Vj}j<k).\nThis localized corrective mechanism mitigates the accumula-\ntion of visual misalignments with timely revision. Likewise,\nas presented in Figure 4 (b), we directly update the textual\npre-context from \u03c4kto the revised \u02c6\u03c4k, and re-generate only\nthe local part \u02c6Vkto replace Vkat the end of the token se-\nquence, which also preserves a single trajectory without\nrequiring the costly full re-generation.\nIn sum,Thinking-while-Generating(i) first schedules a\nnumber Kof interleaved reasoning points (when); then for\neachk= 1, . . . , K , (ii) produces a textual thought that lo-\ncally steers the next visual update (what); and (iii) performs\na region-level reflection with optional correction (how). The\nloop of (ii) and (iii) preserves a single generative trajectory,\nenabling on-the-fly guidance and precise local revision.\n3. Implementation Exploration\nIn this section, we implement three candidate approaches for\nThinking-while-Generating: zero-shot prompting (3.1), su-\npervised fine-tuning (3.2), and reinforcement learning (3.3).\nWe present experimental results that highlight their respec-\ntive strengths. Please refer to detailed experimental settings\nand visualizations in the Supplementary Material.Table 1.Zero-shot ExperimentsofThinking-while-Generatingon\nT2I-CompBench [ 19]. We denote our zero-shot model as TWIG-\nZS, and mark the improvement over the baseline, Janus-Pro-7B [ 7].\nPanels(a),(b),(c), and(d)present four ablation studies.\nSettingAttribute Binding Object RelationshipComplex\u2191\nColor\u2191Shape\u2191Texture\u2191Spatial\u2191Non-Spatial\u2191\nv.s. Baseline\nJanus-Pro-7B [7] 63.59 35.28 49.36 20.61 30.85 35.59\nTWIG-ZS 73.11 41.55 64.77 21.98 30.90 48.16\nImprove +9.52 +6.27 +15.41 +1.37 +0.05 +12.57\n(a) Where the Textual Reasoning is Applied\nThink-before-Gen.65.12 36.20 51.05 20.88 30.82 41.75\nThink-after-Gen.64.72 37.95 50.62 21.05 30.87 42.28\nThinking-while-Gen.73.11 41.55 64.77 21.98 30.90 48.16\n(b) Interleaved Reasoning Step\nK= 272.79 42.26 64.64 21.97 30.89 49.71\nK= 373.11 41.55 64.77 21.98 30.90 48.16\nK= 472.95 41.90 64.70 22.03 31.10 48.90\n(c) How to PartitionV kin Space\nUniform Spacing 73.11 41.55 64.77 21.98 30.90 48.16\nAdaptive Spacing 72.43 40.88 63.92 21.67 30.88 47.39\n(d) Whether to Perform Reflection\nw/o Reflection 73.11 41.55 64.77 21.98 30.90 48.16\n1-round Reflection 73.90 46.02 66.10 24.50 30.81 51.97\n2-round Reflection 73.68 45.72 66.02 24.42 30.88 51.65\n3.1. Zero-shot Prompting\nPrompt Customization.To elicit satisfactory zero-shot\nThinking-while-Generating, we meticulously design a series\nof interleave-aware prompts for ULM, corresponding to the\nthree components described in Section 2.2. Please refer to\nthe final prompt templates in the Supplementary Material.\n\u2022Forwhento think, we prompt the model to adopt a global\nview, sketching the image\u2019s high-level semantics and struc-\nture step by step from the input prompt. For an adaptive\nschedule, we additionally prompt the model to output the\nrelative ratios of visual parts across the canvas.\n\u2022Forwhatto say, we guide the model to focus strictly on the\nlocal region currently being generated while maintaining\ncoherence with previously generated visual and textual\ncontext. We discourage any spatial-anchor tokens; the\nmodel should produce only the descriptive content.\n\u2022Forhowto refine, we prompt the model to provide a critic\nscore evaluating along five criteria (color accuracy, object\ncompleteness, detail richness, spatial relationships, and\nvisual coherence), ensuring a consistent standard across\ncases. The template enforces that any revision is local and\ndoes not contradict validated prior regions.\nExperiments and Analysis.In Table 1 (top), we present\nthe performance of our zero-shot model, TWIG-ZS. We ob-\nserve that our carefully designed prompts yield surprisingly\nstrong improvements over the baseline, significantly sur-\npassing Janus-Pro-7B [ 7] across multiple dimensions. This\nhighlights the potential of our framework and its natural\napplicability within current ULMs, making the zero-shot\nvariant a strong foundation for subsequent SFT and RL. By\n5\nTable 2.SFT ExperimentsofThinking-while-Generatingon T2I-\nCompBench [ 19]. We denote our fine-tuned model as TWIG-SFT,\nand mark the improvement over TWIG-ZS. Panel(a)ablates the\nvarying proportions of thinking (T), generation (G), and reflection\n(R) data in TWIG-50K. Panel(b)reports the standard deviation\n(Std) across random seeds to assess stability.\nModel / SettingData Attribute Binding Object RelationshipComplex\u2191\nT/G/R Color\u2191Shape\u2191Texture\u2191Spatial\u2191Non-Spatial\u2191\nv.s. Baseline\nJanus-Pro-7B [7] \u2013 63.59 35.28 49.36 20.61 30.85 35.59\nTWIG-ZS \u2013 73.11 41.55 64.77 21.98 30.90 48.16\nTWIG-SFT 74.58 52.42 67.95 27.02 31.24 53.41\nImprove \u2013 +1.47 +10.87 +3.18 +5.04 +0.34 +5.25\n(a) Effect of Training Data Composition\nThink-heavy 73.38 50.92 66.47 26.08 30.97 51.86\nGen-heavy 74.12 51.77 67.28 26.58 31.09 52.83\nThink-Gen-equal 74.58 52.42 67.95 27.02 31.24 53.41\nReflect-lite 72.76 49.75 65.93 26.36 30.92 51.17\nReflect-heavy 71.88 48.98 65.05 25.62 30.84 50.27\n(b) Stability across 5 Random Seeds\nTWIG-ZSStd\u2193 \u2013 0.82 0.70 0.76 0.45 0.38 0.91\nTWIG-SFTStd\u2193 0.65 0.59 0.61 0.40 0.36 0.80\ndefault, we adopt an interleaved schedule with K= 3 and\nuniform spacing, and permit at most one round of reflection.\nWe conduct four ablations:\n\u2022Ablation(a):Thinking-while-GeneratingversusThink-\nbefore/after-Generationunder identical zero-shot settings.\nInterleaving provides nuanced, on-the-fly guidance rather\nthan only pre-planning or post-refinement, and consis-\ntently outperforms the alternatives.\n\u2022Ablation(b):Number of interleaved reasoning steps under\na uniform schedule. We find K= 3 is optimal, aligning\nwith the heuristic that many images decompose into three\nsemantic components: upper background, central content,\nand lower background.\n\u2022Ablation(c):Adaptive scheduling of interleaved spacing.\nDespite exploring multiple prompting strategies, current\nULMs struggle to reliably follow such instructions, leading\nto unstable or poorly structured adaptive schedules.\n\u2022Ablation(d):Effectiveness of reflection during reasoning.\nA single reflection round corrects misalignments and im-\nproves performance across aspects; however, conducting\ntwo rounds brings no further gains, likely limited by the\ncritique\u2013and\u2013revision capacity of zero-shot ULMs.\n3.2. Supervised Fine-tuning\nSFT Task Formulation.Building on the zero-shot base-\nline, we investigate whether SFT can enhance the capabili-\nties. We decompose theThinking-while-Generatingprocess\ninto nine supervised tasks that mirror the inference loop,\nusing a fixed number of three reasoning steps. These com-\nprise three thinking targets for ULM u(upper/central/lower\nthoughts), three reflection targets for ULM u(three scores\nwith revised thoughts), and three generation targets for\nULM g(three visual regions). This enables the model toTable 3.RL ExperimentsofThinking-while-Generatingon T2I-\nCompBench [ 19]. We denote our reinforced model with GRPO [ 41]\nas TWIG-RL, and mark the improvement over the TWIG-SFT.\nPanels(a)and(b)present the results of two ablation studies.\nSettingAttribute Binding Object RelationshipComplex\u2191\nColor\u2191Shape\u2191Texture\u2191Spatial\u2191Non-Spatial\u2191\nv.s. Baseline\nJanus-Pro-7B [7] 63.59 35.28 49.36 20.61 30.85 35.59\nTWIG-ZS 73.11 41.55 64.77 21.98 30.90 48.16\nTWIG-SFT 74.58 52.42 67.95 27.02 31.24 53.41\nTWIG-RL 82.49 61.28 73.19 34.06 31.99 54.45\nImprove +7.91 +8.86 +5.24 +7.04 +0.75 +1.04\n(a)TWIG-GRPO Strategy\nULM g-GRPO 80.12 59.87 72.01 32.47 31.30 54.02\nULM u-GRPO 78.36 57.94 70.68 30.93 31.27 53.76\nTWIG-GRPO 82.49 61.28 73.19 34.06 31.99 54.45\n(b) Reward Model Ensemble\nHuman Preference 79.83 60.97 71.35 20.68 30.53 52.87\n+Object Grounding 80.44 60.01 73.79 25.84 31.15 54.03\n++VQA Consistency 80.87 59.29 74.26 30.05 31.41 53.64\n+++LMM Alignment 82.49 61.28 73.19 34.06 31.99 54.45\nlearn structured reasoning, localized reflection, and region-\nwise generation in an interleaved, context-aware manner.\nTWIG-50K Dataset.To support the task formulation, we\ncurate a high-quality dataset termed TWIG-50K. The con-\nstruction process comprises multiple stages of synthetic su-\npervision using advanced commercial models.\n\u2022Forwhatto say ( \u223c17K, three tasks), we source 5.5K text\nprompts from the training split of T2I-CompBench [ 19],\nand adopt GPT-4o [ 22] to generate stepwise sub-captions\nthat segment the image into three coherent parts (up-\nper background, central content, lower background).\nThese sub-captions are concatenated and fed to GPT-4o-\nImage [ 22] to synthesize images that are semantically con-\nsistent with the specified divisions. We then filter low-\nquality instances and organize them into interleaved for-\nmats aligned with theThinking-while-Generatingprotocol.\nNote that, since the reasoning step count is fixed to three,\nwe do not collect supervision data forwhento think.\n\u2022Forhowto refine ( \u223c17K, three tasks), building on the\ninterleaved samples above, we construct three visual un-\nderstanding tasks focused on critique and revision. GPT-4o\nis prompted to evaluate each region by assigning a critic\nscore along five criteria (the same as zero-shot settings)\nand to provide a revised sub-caption that addresses defi-\nciencies identified by the critique. If the original image\nattains a high score, the revised thought simply repeats, a\ncase that may not trigger re-generation during inference.\n\u2022To enhance the generation capability of ULM g(\u223c16K,\nthree tasks), we construct interleaved visual generation\ndata from the image\u2013sub-caption pairs obtained in the\nwhen/whatstage. Each training instance conditions\nthe generation of region Vkon cumulative reasoning\nthoughts {\u03c4j}j\u2264kand previously generated visual con-\ntents{Vj}j<k. Note that this remains text-to-image su-\n6\nTable 4.Performance Comparison on T2I-CompBench++ [20].The best and the second-best scores are highlighted.\nModelAttribute Binding Object RelationshipNumeracy\u2191Complex\u2191\nColor\u2191Shape\u2191Texture\u21912D-Spatial\u21913D-Spatial\u2191Non-Spatial\u2191\nCurrent Generative Models\nShow-o [52] 56 41 46 20 - 30 - 29\nSD-XL-base-1.0 [34] 58.79 46.87 52.99 21.31 35.66 31.19 49.91 32.37\nAttend-and-Excite [5] 64.00 45.17 59.63 14.55 32.22 31.09 47.73 34.01\nPixArt-\u03b1[6] 66.90 49.27 64.77 20.64 - 31.97 - 34.33\nGoT [11] 65.51 50.08 58.36 24.57 31.13 - 37.54\nShow-o + PARM [17] 75 56 66 29 - 31 - 37\nFLUX.1 [24] 74.07 57.18 69.22 28.63 38.66 31.27 61.85 37.03\nEmu3 [49] 75.44 57.06 71.64 - - - - -\nT2I-R1 [23] 81.30 58.52 72.43 33.78 - 30.90 60.97 39.93\nThinking-while-Generating\nJanus-Pro-7B [7] (Baseline) 63.59 35.28 49.36 20.61 32.94 30.85 41.32 35.59\nTWIG-ZS 73.11 41.55 64.77 21.98 33.68 30.90 36.58 48.16\nTWIG-SFT 74.58 52.42 67.95 27.02 35.57 31.24 51.70 53.41\nTWIG-RL 82.49 61.28 73.19 34.06 38.87 31.99 61.93 53.56\npervision to preserve a single generation trajectory (not\nimage-to-image), augmented with a visual pre-context.\nExperiments and Analysis.In Table 2 (top), we present\nthe performance of our fine-tuned model, TWIG-SFT. Rela-\ntive to the zero-shot baseline (TWIG-ZS), SFT delivers mod-\nest and reliable gains across benchmarks, with the largest\nimprovements onShapeandSpatialcategories. This demon-\nstrates the effectiveness of our fine-tuning recipe and the\ncurated TWIG-50K dataset. By default, we inherit the opti-\nmal model settings from TWIG-ZS, and adopt a balanced\ndata mixture with equal thinking and generation tasks. We\nfurther provide two analyses:\n\u2022Ablation(a):Effect of data composition from TWIG-50K.\nBalancing thinking (T) and generation (G) provides the\nbest trade-off and strengthensThinking-while-Generating\nfrom both sides. However, adding reflection data (R) de-\ngrades the results, where the thoughts become longer and\nover-corrections appear more frequently. This suggests\nthat, TWIG-ZS already exposes most of the model\u2019s re-\nflection proficiency, and oversupplyingRdiverts capacity\naway from learning stableTandGbehaviors. Although\nthe reflection subset cannot contribute here, we hope it will\nfacilitate future research on critique-and-revise training.\n\u2022Comparison(b):Inference stability across five random\nseeds. We report the standard deviation (Std) over differ-\nent runs, and observe that SFT notably tightens disper-\nsion compared to TWIG-ZS, indicating more predictable\nbehavior. Qualitatively, SFT shortens verbose thoughts,\ncurbs hallucinations, improves attribute persistence across\nadjacent regions, and reduces spurious reflection triggers\nnear the decision threshold.3.3. Reinforcement Learning\nTWIG-GRPO Strategy.To further advance performance,\nwe employ RL to enhance the interleaved reasoning. Specifi-\ncally, we adopt the GRPO algorithm [ 41] with the training\nprompts from T2I-CompBench, and tailor it to ourThinking-\nwhile-Generatingframework. Within this setup, the ULM\nperforms multiple forward passes within a single rollout\nduring GRPO training. A key design question is which com-\nponents should be reinforced through the reward mechanism:\nall stages, or only the understanding or generation phases?\nWe propose to reinforce all of them simultaneously through\nour TWIG-GRPO strategy. Concretely, we compute a single\nreward based on the final generated image and the input\nprompt, and utilize it as a shared reward to optimize the\npolicies of every thinking, generation, and reflection pass\njointly. This approach not only simplifies implementation\n(no need to compute rewards for each local visual subtask),\nbut also enables consistent reinforcement across ULM uand\nULM g, allowing global information to flow across differ-\nent paths and thereby enhancing the overall synergy of the\nTWIG framework.\nReward Model Design.Since a high-quality image must\nsatisfy multiple aspects (overall aesthetics, object attributes\nand relationships), we explore to combine complementary\nreward models for joint optimization and mitigating reward\nhacking [ 23]: (i) human preference scores (HPS v2 [ 51]), (ii)\nobject grounding scores (GroundingDINO [ 31]), (iii) VQA\nconsistency scores (GIT [ 48]), and (iv) LMM alignment\nscores (the fine-tuned ORM [ 17]). We utilize an unweighted\naverage of the four reward model, and this simple strategy ef-\nfectively leverages our framework\u2019s generality for RL gains.\n7\nA round doughnut and a square napkin.\n A candle hidden by a bag.\nThe glowing moon rose above the distant hill and the calm sea. A blue apple and a green vase.\nBaselineText Prompt \ud835\udc47\nBaseline\nText Prompt \ud835\udc47 Text Prompt \ud835\udc47Text Prompt \ud835\udc47Figure 5.Qualitative Comparison of TWIG Variants:the baseline (Janus-Pro-7B [ 7]), TWIG-ZS, -SFT, and -RL. Our method\ndemonstrates progressive improvements in compositional fidelity, object counting, and visual realism.\nThe head of a brown \ncow is starting to \nappear at the top of \nthe scene under a \nblue sky.A brown cow\u2019s full head, \nwith open eyes, curved \nhorns and wide ears, is \nclearly visible  against a \nclean blue sky.The lower area shows \nthe toy and bottle on \na light surface with \nsoft, even tones.The lower area shows \nclearer shadows on the \nsurface and a more \ndefined base beneath \nthe toy.Three swans \nglide across a \ncalm blue lake \nunder a clear sky.Exactly three swans move \nacross the lake with a \nsharper horizon and \nsmoother water surface.A blue backpack and a brown cow.\nOn the \nhillside, the \nperson stands \non the grass.The person stands on a hillside \nwith the balloon on the right, \nboth casting consistent shadows \nthat match the sunlight.\nReflectThree swans. A plastic toy and a glass bottle . A balloon on the right of a person.\n\u2713 \u2717Text Prompt \ud835\udc47\n Text Prompt \ud835\udc47 Text Prompt \ud835\udc47 Text Prompt \ud835\udc47\nReflectReflectReflect\n\u2713 \u2717\n\u2713 \u2717 \u2713 \u2717\u0ddd\ud835\udf0f1\n\ud835\udcb11\ud835\udcb13\n\u0de2\ud835\udcb11\u0ddd\ud835\udf0f3 \ud835\udf0f3 \ud835\udf0f1\n\u0de2\ud835\udcb13\u0ddd\ud835\udf0f2\n\ud835\udcb12\u0de2\ud835\udcb12\ud835\udf0f2\n\ud835\udcb13\u0ddd\ud835\udf0f3 \ud835\udf0f3\n\u0de2\ud835\udcb13\nBefore Reflection After ReflectionBefore Reflection After Reflection\nBefore Reflection After Reflection\nBefore Reflection After Reflection\nFigure 6.The Reflection Capacity of TWIG-RL. The reflection within ourThinking-while-Generatingrefines both semantic and visual\nconsistency, e.g., improving spatial alignment, shadow coherence, and overall realism across diverse prompts.\nExperiments and Analysis.In Table 3 (top), we present\nthe performance of our reinforced model, TWIG-RL. Com-\npared with TWIG-SFT, the initialization point, RL de-\nlivers substantial gains, e.g., exceeding +5%, across the\nthreeAttribute Bindingcategories and theSpatialcategory.\nThis highlights the remaining headroom of theThink-while-\nGeneratingparadigm once a policy is guided in a right\ndirection with an appropriate GRPO strategy and reward\nensemble designs. In Table 4, we report the three TWIG\napproaches in comparison with current generative models\non T2I-CompBench++ [ 20]. Our method offers a flexible\ntrade-off between implementation efficiency (ZS) and com-\npetitive performance (RL), allowing practitioners to balance\nthe cost and quality according to deployment needs. Further-\nmore, in Figures 5, 6, and 7, we present three visualizations,\ni.e., illustrating the improvements across different variants,\nthe reflection capability, and the image-text interleaved rea-\nsoning process, respectively, which highlight the qualitative\neffectiveness of our methods.\u2022Ablation(a):Different strategies for GRPO algorithms.\nOur TWIG-GRPO jointly reinforces all (up to nine) local\nvisual subtasks within a single rollout. We investigate to\nseparately optimize the understanding-related tasks (think-\ning and reflection) and the generation-related tasks, each\nusing the shared reward to update ULM uandULM g, re-\nspectively. As compared, the separate enhancements fail\nto surpass the joint strategy, highlighting their comple-\nmentary nature and mutual reinforcement. Only when\ncombined under the full TWIG-GRPO strategy can the RL\npotential of the interleaved reasoning be fully realized.\n\u2022Ablation(b):Ensemble of multiple reward models. We\nbegin with a single HPS v2, and progressively incorporate\nother three rewards. HPS v2 primarily improves global\naesthetics and stylistic coherence; GroundingDINO tight-\nens entity presence and localization; GIT curbs instruc-\ntion violations and strengthens attribute consistency; the\nfine-tuned ORM improves holistic text\u2013image alignment.\nAdding components steadily improves performance, and\nthe ensemble of four achieves the best overall balance.\n8\nThe elephant\u2019s  head and upper  back against  a \nsmooth,  neutral  background . The top part of the \nbright  red suitcase  and its extended  handle  \nappear  beside  the elephant\u2019s  head.The elephant\u2019s  shoulders,  torso,  and upper  parts  \nof its legs are visible,  with its trunk  hanging  down  \nin front  of the suitcase . The central  body of the \nred suitcase  is appearing,  showing  its curved,  \nglossy  surface .The elephant\u2019s  feet planted  on the ground  and \nthe very bottom  of its legs, as well as the base \nof the suitcase  with its small  wheels\u2026 A flat, \nlight-colored  floor  is visible  here,  along  with the \nfaint  line where  the floor  meets  the background .\nA brown elephant and a red suitcase.Text Prompt \ud835\udc47\nThe duck\u2019s  head and neck rising  above  bright  blue \nwater,  with some  green  foliage  ... Light  reflects  \nsoftly  on the water,  giving  the scene  a calm \natmosphere .The duck\u2019s  full body is visible,  floating  on the \npond with gentle  ripples  forming  around  it. \nVibrant  flowers  and leaves  frame  the edges  of \nthe water,  \u2026The duck\u2019s  reflection  becomes  clear  in the \ndeeper  blue water,  along  with the circular  ripples  \nspreading  outward . More  greenery  appears  at the \ncorner,  grounding  the scene  and emphasizing  the \npond\u2019s  stillness .A yellow duck and a blue pond.Text Prompt \ud835\udc47\nThe wooden  ceiling  beams  and the dark window  \nframes,  with the deep blue night  sky visible  \noutside . The warm  interior  light contrasts  \nsharply  with the cold darkness  beyond  the glass .The room\u2019s  wooden  walls and large  windows  come  \ninto view,  with several  cushioned  chairs  arranged  \naround  the fire... The firelight  reflects  softly  on \nthe wood  and upholstery,  giving  the space  a \ncomforting,  amber  glow.Bright  flames  rise from  the fireplace  in the \nforeground,  casting  flickering  highlights  on the \nsoft,  textured  carpet . Additional  seating  and a \nsmall  table  appear  near the hearth,  reinforcing  \nthe inviting,  relaxed  atmosphere  of the room .The flickering fire lit up the cozy room and the \ndark night sky.Text Prompt \ud835\udc47\nA soft,  green  field fading  gently  into the \ndistance,  with the upper  portion  of a bright  \nyellow  sunflower ... Its petals  stand  out clearly  \nagainst  the blurred  background .The full sunflower  head becomes  prominent,  \ncentered  above  a slender  stem ... The leaves  \nextend  outward  on both sides,  catching  the light \nand adding  a crisp  contrast  against  the \nsurrounding  green .The stem  continues  into the dense,  vivid grass,  \nwhich  grows  taller  and darker  toward  the bottom  \nedge. Shadows  and highlights  on the blades  \ncreate  a textured,  lively  foreground  that grounds  \nthe sunflower  firmly  in its environment .The bright yellow sunflower stood tall next to the \nsoft green grass.Text Prompt \ud835\udc47\nThe cow\u2019s  head appears  from  the opening  of a \nlarge  beige  sack,  with both ears pushed  outward  \nby the fabric  around  it. The edges  of the sack \nframe  its face clearly,  making  the cow\u2019s  \nexpression  immediately  noticeable .The cow\u2019s  face and upper  body rest snugly  inside  \nthe sack,  surrounded  by folds  of wrinkled  cloth ... \nIts eyes are open and attentive,  and the smooth  \nfur contrasts  with the rough,  creased  canvas .The rounded  base of the sack sits on the ground,  \nits fabric  gathered  and folded  as it supports  the \ncow inside . A patch  of grass  beneath  it shows  \nthat the scene  takes  place  outdoors .A cow on the bottom of a bag.Text Prompt \ud835\udc47\nA single  glowing  light bulb hangs  against  a nearly  \nblack  background,  its warm  filament  clearly  \nvisible . The faint  illumination  spreads  outward  \njust enough  to reveal  the edges  of two open door \nframes  on either  side.The dim room  behind  the bulb becomes  partially  \nvisible,  with soft reflections  and hints  of \ntextured  walls... The warm  glow creates  a small  \npool of light around  the bulb,  leaving  much  of the \nspace  in shadow .A portion  of the dark wooden  floor  appears,  along  \nwith the edge of a chair  or furniture  shape  \nemerging  from  the gloom . The shadows  grow \ndeeper  here,  \u2026The flickering light bulb brightened the dark \nroom and the dim hallway.Text Prompt \ud835\udc47\ud835\udf0f1 \ud835\udf0f2 \ud835\udf0f3\n\ud835\udf0f1 \ud835\udf0f2 \ud835\udf0f3\n\ud835\udf0f1 \ud835\udf0f2 \ud835\udf0f3\n\ud835\udf0f1 \ud835\udf0f2 \ud835\udf0f3\n\ud835\udf0f1 \ud835\udf0f2 \ud835\udf0f3\n\ud835\udf0f1 \ud835\udf0f2 \ud835\udf0f3\ud835\udcb11\n\ud835\udcb12\n\ud835\udcb13\n\ud835\udcb11\n\ud835\udcb12\n\ud835\udcb13\n\ud835\udcb11\n\ud835\udcb12\n\ud835\udcb13\n\ud835\udcb11\n\ud835\udcb12\n\ud835\udcb13\n\ud835\udcb11\n\ud835\udcb12\n\ud835\udcb13\n\ud835\udcb11\n\ud835\udcb12\n\ud835\udcb13Figure 7.Thinking-while-GeneratingProcess of TWIG-RL.Each example showcases how the model iteratively interleaves its textual\nreasoning and visual outputs, progressively improving compositional accuracy, spatial alignment, and scene coherence.\n9\n4. Conclusion\nIn this paper, we introduce theThinking-while-Generating\n(TWIG) paradigm, an interleaved framework that keeps tex-\ntual reasoning in the loop during visual generation. Starting\nfrom carefully designed zero-shot prompts, then enhancing\nwith SFT, and finally optimizing a policy via RL, our TWIG\nmodel learns to think, generate, and reflect within a single\nvisual generation trajectory. We hope this paradigm may\ninspire future research to fully investigate the potential of\ninterleaved visual generation schemes.\nLimitations.Given the incapacity of current ULMs, our\n\u2018whento think\u2019 utilizes a fixed three-step schedule, which is\ngeneral but not optimal. As more capable models emerge,\nlearning fully adaptive schedules is a promising next step.\nSecond, our RL setup employs the original GRPO, already\nstrong, but may be further enhanced by recent variants [ 55,\n58]. Finally, extending TWIG to video, 3D, or image-to-\nimage tasks presents another compelling avenue.\nReferences\n[1]Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,\nIlge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko\nAltenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4\ntechnical report.arXiv preprint arXiv:2303.08774, 2023. 3\n[2]James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng\nWang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce\nLee, Yufei Guo, et al. Improving image generation with\nbetter captions.Computer Science. https://cdn. openai.\ncom/papers/dall-e-3. pdf, 2(3):8, 2023. 3\n[3]Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nInProceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 18392\u201318402, 2023. 3\n[4]Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T\nFreeman. Maskgit: Masked generative image transformer. In\nProceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 11315\u201311325, 2022. 3\n[5]Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel\nCohen-Or. Attend-and-excite: Attention-based semantic guid-\nance for text-to-image diffusion models.ACM Transactions\non Graphics (TOG), 42(4):1\u201310, 2023. 7\n[6]Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze\nXie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo,\nHuchuan Lu, and Zhenguo Li. Pixart- \u03b1: Fast training of\ndiffusion transformer for photorealistic text-to-image synthe-\nsis, 2023. 7\n[7]Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen\nLiu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro:\nUnified multimodal understanding and generation with data\nand model scaling.arXiv preprint arXiv:2501.17811, 2025.\n1, 2, 3, 5, 6, 7, 8\n[8]Xinyan Chen, Renrui Zhang, Dongzhi Jiang, Aojun Zhou,\nShilin Yan, Weifeng Lin, and Hongsheng Li. Mint-cot: En-abling interleaved visual tokens in mathematical chain-of-\nthought reasoning.arXiv preprint arXiv:2506.05331, 2025.\n1\n[9]Chengqi Duan, Kaiyue Sun, Rongyao Fang, Manyuan Zhang,\nYan Feng, Ying Luo, Yufang Liu, Ke Wang, Peng Pei, Xun-\nliang Cai, et al. Codeplot-cot: Mathematical visual reason-\ning by thinking with code-driven images.arXiv preprint\narXiv:2510.11718, 2025. 2\n[10] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim En-\ntezari, Jonas M \u00a8uller, Harry Saini, Yam Levi, Dominik Lorenz,\nAxel Sauer, Frederic Boesel, et al. Scaling rectified flow trans-\nformers for high-resolution image synthesis. InForty-first\ninternational conference on machine learning, 2024. 3\n[11] Rongyao Fang, Chengqi Duan, Kun Wang, Linjiang Huang,\nHao Li, Shilin Yan, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng\nDai, et al. Got: Unleashing reasoning capability of multi-\nmodal large language model for visual generation and editing.\narXiv preprint arXiv:2503.10639, 2025. 1, 2, 7\n[12] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun\nAkula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang,\nand William Yang Wang. Training-free structured diffusion\nguidance for compositional text-to-image synthesis.arXiv\npreprint arXiv:2212.05032, 2022. 3\n[13] Jun Gao, Yongqi Li, Ziqiang Cao, and Wenjie Li. Interleaved-\nmodal chain-of-thought. InProceedings of the Computer\nVision and Pattern Recognition Conference, pages 19520\u2013\n19529, 2025. 2\n[14] Google DeepMind. Veo-3 technical report. Technical report,\nGoogle DeepMind, 2025. 3\n[15] Ziyu Guo*, Renrui Zhang*, Xiangyang Zhu, Yiwen Tang,\nXianzheng Ma, Jiaming Han, Kexin Chen, Peng Gao, Xi-\nanzhi Li, Hongsheng Li, et al. Point-bind & point-llm:\nAligning point cloud with multi-modality for 3d understand-\ning, generation, and instruction following.arXiv preprint\narXiv:2309.00615, 2023. 3\n[16] Ziyu Guo, Xinyan Chen, Renrui Zhang, Ruichuan An, Yu Qi,\nDongzhi Jiang, Xiangtai Li, Manyuan Zhang, Hongsheng Li,\nand Pheng-Ann Heng. Are video models ready as zero-shot\nreasoners? an empirical study with the mme-cof benchmark.\narXiv preprint arXiv:2510.26802, 2025. 3\n[17] Ziyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao,\nRui Huang, Haoquan Zhang, Manyuan Zhang, Jiaming Liu,\nShanghang Zhang, Peng Gao, et al. Can we generate images\nwith cot? let\u2019s verify and reinforce image generation step by\nstep.arXiv preprint arXiv:2501.13926, 2025. 1, 2, 7\n[18] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-\nage editing with cross attention control.arXiv preprint\narXiv:2208.01626, 2022. 3\n[19] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui\nLiu. T2i-compbench: A comprehensive benchmark for open-\nworld compositional text-to-image generation.Advances in\nNeural Information Processing Systems, 36:78723\u201378747,\n2023. 3, 5, 6\n[20] Kaiyi Huang, Chengqi Duan, Kaiyue Sun, Enze Xie, Zhen-\nguo Li, and Xihui Liu. T2i-compbench++: An enhanced and\ncomprehensive benchmark for compositional text-to-image\n10\ngeneration.IEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence, 2025. 3, 7, 8\n[21] Wenxuan Huang, Shuang Chen, Zheyong Xie, Shaosheng\nCao, Shixiang Tang, Yufan Shen, Qingyu Yin, Wenbo Hu,\nXiaoman Wang, Yuntian Tang, et al. Interleaving rea-\nsoning for better text-to-image generation.arXiv preprint\narXiv:2509.06945, 2025. 3\n[22] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman,\nAditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda,\nAlan Hayes, Alec Radford, et al. Gpt-4o system card.arXiv\npreprint arXiv:2410.21276, 2024. 6\n[23] Dongzhi Jiang, Ziyu Guo, Renrui Zhang, Zhuofan Zong,\nHao Li, Le Zhuo, Shilin Yan, Pheng-Ann Heng, and Hong-\nsheng Li. T2i-r1: Reinforcing image generation with col-\nlaborative semantic-level and token-level cot.arXiv preprint\narXiv:2505.00703, 2025. 1, 2, 7\n[24] Black Forest Labs. Flux. https://github.com/\nblack-forest-labs/flux, 2024. 7\n[25] Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia,\nShaoguang Mao, Li Dong, Ivan Vuli \u00b4c, and Furu Wei. Imag-\nine while reasoning in space: Multimodal visualization-of-\nthought.arXiv preprint arXiv:2501.07542, 2025. 2\n[26] Shufan Li, Konstantinos Kallidromitis, Akash Gokul, Arsh\nKoneru, Yusuke Kato, Kazuki Kozuka, and Aditya Grover.\nReflect-dit: Inference-time scaling for text-to-image diffu-\nsion transformers via in-context reflection.arXiv preprint\narXiv:2503.12271, 2025. 2\n[27] Jiaqi Liao, Zhengyuan Yang, Linjie Li, Dianqi Li, Kevin Lin,\nYu Cheng, and Lijuan Wang. Imagegen-cot: Enhancing text-\nto-image in-context learning with chain-of-thought reasoning.\narXiv preprint arXiv:2503.19312, 2025. 1, 2\n[28] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-\nYu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-\nto-3d content creation. InProceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pages\n300\u2013309, 2023. 3\n[29] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. InNeurIPS, 2023. 3\n[30] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and\nJoshua B Tenenbaum. Compositional visual generation with\ncomposable diffusion models. InEuropean conference on\ncomputer vision, pages 423\u2013439. Springer, 2022. 3\n[31] Siyi Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang,\nJie Yang, Chun yue Li, Jianwei Yang, Hang Su, Jun-Juan\nZhu, and Lei Zhang. Grounding dino: Marrying dino with\ngrounded pre-training for open-set object detection.ArXiv,\nabs/2303.05499, 2023. 7\n[32] OpenAI. Sora 2 system card. Technical report, OpenAI, 2025.\n1\n[33] OpenAI. Openai o3 and o4-mini system card. Technical\nreport, OpenAI, 2025. 1\n[34] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann,\nTim Dockhorn, Jonas M \u00a8uller, Joe Penna, and Robin Rombach.\nSdxl: Improving latent diffusion models for high-resolution\nimage synthesis.arXiv preprint arXiv:2307.01952, 2023. 7[35] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall.\nDreamfusion: Text-to-3d using 2d diffusion.arXiv preprint\narXiv:2209.14988, 2022. 1, 3\n[36] Luozheng Qin, Jia Gong, Yuqing Sun, Tianjiao Li, Mengping\nYang, Xiaomeng Yang, Chao Qu, Zhiyu Tan, and Hao Li.\nUni-cot: Towards unified chain-of-thought reasoning across\ntext and vision.arXiv preprint arXiv:2508.05606, 2025. 2, 3\n[37] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image genera-\ntion with clip latents.arXiv preprint arXiv:2204.06125, 2022.\n1, 3\n[38] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj \u00a8orn Ommer. High-resolution image\nsynthesis with latent diffusion models. InProceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 3\n[39] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj \u00a8orn Ommer. High-resolution image\nsynthesis with latent diffusion models. InProceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 10684\u201310695, 2022. 1\n[40] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay\nWhang, Emily Denton, Seyed Kamyar Seyed Ghasemipour,\nRaphael Gontijo Lopes, Jie Hou, Alexander Kolesnikov, et al.\nPhotorealistic text-to-image diffusion models with deep lan-\nguage understanding. InAdvances in Neural Information\nProcessing Systems (NeurIPS), 2022. 1\n[41] Zhenming Shao, Jiayi Gu, Ziyang Wang, Liang Ding, Yi\nWang, Yao Zhang, Shuming Tang, et al. Deepseekmath: Push-\ning the limits of mathematical reasoning in open language\nmodels.arXiv preprint arXiv:2402.03300, 2024. Introduces\nGroup Relative Policy Optimization (GRPO) used for RL\ntraining. 6, 7\n[42] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao\nSong, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li,\nYang Wu, et al. Deepseekmath: Pushing the limits of mathe-\nmatical reasoning in open language models.arXiv preprint\narXiv:2402.03300, 2024. 3\n[43] Zhaochen Su, Peng Xia, Hangyu Guo, Zhenhua Liu, Yan Ma,\nXiaoye Qu, Jiaqi Liu, Yanshu Li, Kaide Zeng, Zhengyuan\nYang, et al. Thinking with images for multimodal reasoning:\nFoundations, methods, and future frontiers.arXiv preprint\narXiv:2506.23918, 2025. 1\n[44] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue\nPeng, Ping Luo, and Zehuan Yuan. Autoregressive model\nbeats diffusion: Llama for scalable image generation.arXiv\npreprint arXiv:2406.06525, 2024. 3\n[45] Chameleon Team. Chameleon: Mixed-modal early-fusion\nfoundation models.arXiv preprint arXiv:2405.09818, 2024.\n1, 3\n[46] Chengzhuo Tong, Ziyu Guo, Renrui Zhang, Wenyu Shan,\nXinyu Wei, Zhenghao Xing, Hongsheng Li, and Pheng-Ann\nHeng. Delving into rl for image generation with cot: A study\non dpo vs. grpo.arXiv preprint arXiv:2505.17017, 2025. 1\n[47] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao,\nChen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao\nYang, et al. Wan: Open and advanced large-scale video\ngenerative models.arXiv preprint arXiv:2503.20314, 2025. 3\n11\n[48] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li,\nKevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang.\nGit: A generative image-to-text transformer for vision and\nlanguage.arXiv preprint arXiv:2205.14100, 2022. 7\n[49] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun,\nYufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen\nLi, Qiying Yu, et al. Emu3: Next-token prediction is all you\nneed.arXiv preprint arXiv:2409.18869, 2024. 7\n[50] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma,\nXingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai\nYu, Chong Ruan, et al. Janus: Decoupling visual encoding\nfor unified multimodal understanding and generation.arXiv\npreprint arXiv:2410.13848, 2024. 2\n[51] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng\nZhu, Rui Zhao, and Hongsheng Li. Human preference score\nv2: A solid benchmark for evaluating human preferences of\ntext-to-image synthesis.arXiv preprint arXiv:2306.09341,\n2023. 7\n[52] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang,\nWeihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen,\nZhenheng Yang, and Mike Zheng Shou. Show-o: One single\ntransformer to unify multimodal understanding and genera-\ntion.arXiv preprint arXiv:2408.12528, 2024. 1, 2, 3, 7\n[53] Jinheng Xie, Zhenheng Yang, and Mike Zheng Shou. Show-\no2: Improved native unified multimodal models.arXiv\npreprint arXiv:2506.15564, 2025. 3\n[54] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan\nHui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang,\nChenxu Lv, et al. Qwen3 technical report.arXiv preprint\narXiv:2505.09388, 2025. 3\n[55] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xi-\naochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu,\nLingjun Liu, et al. Dapo: An open-source llm reinforcement\nlearning system at scale.arXiv preprint arXiv:2503.14476,\n2025. 10\n[56] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models. In\nProceedings of the IEEE/CVF international conference on\ncomputer vision, pages 3836\u20133847, 2023. 1, 3\n[57] Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Yichi Zhang, Ziyu\nGuo, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei,\nShanghang Zhang, et al. Mavis: Mathematical visual instruc-\ntion tuning.arXiv preprint arXiv:2407.08739, 2024. 2\n[58] Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen,\nBowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men,\nAn Yang, et al. Group sequence policy optimization.arXiv\npreprint arXiv:2507.18071, 2025. 10\n[59] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao,\nGuohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes:\nIncentivizing\u201d thinking with images\u201d via reinforcement learn-\ning.arXiv preprint arXiv:2505.14362, 2025. 1, 2\n[60] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michi-\nhiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma,\nLuke Zettlemoyer, and Omer Levy. Transfusion: Predict the\nnext token and diffuse images with one multi-modal model.\narXiv preprint arXiv:2408.11039, 2024. 2, 3[61] Le Zhuo, Liangbing Zhao, Sayak Paul, Yue Liao, Renrui\nZhang, Yi Xin, Peng Gao, Mohamed Elhoseiny, and Hong-\nsheng Li. From reflection to perfection: Scaling inference-\ntime optimization for text-to-image diffusion models via re-\nflection tuning. InProceedings of the IEEE/CVF International\nConference on Computer Vision, pages 15329\u201315339, 2025.\n2\n12",
      "summary": "This paper introduces Thinking-while-Generating (TwiG), a novel framework that interleaves textual reasoning throughout the visual generation process, enabling dynamic multimodal interaction. TwiG guides and reflects on visual content as it is generated, resulting in more context-aware and semantically rich outputs.",
      "key_contributions": [
        "Proposes the first interleaved framework (TwiG) for co-evolving textual reasoning during visual generation.",
        "Demonstrates dynamic interplay between textual reasoning and visual synthesis to enhance output quality.",
        "Investigates three strategies for interleaved reasoning: zero-shot prompting, supervised fine-tuning on the TwiG-50K dataset, and reinforcement learning via TwiG-GRPO.",
        "Curates and utilizes a new dataset (TwiG-50K) for supervised training."
      ],
      "methodology": "The authors interleave textual reasoning steps within the visual generation process, evaluating three approaches\u2014zero-shot prompting, supervised fine-tuning, and reinforcement learning\u2014to guide and reflect on visual outputs as they are synthesized.",
      "results": "The interleaved reasoning framework produces visual outputs that are more context-aware and semantically rich compared to traditional pre- or post-generation reasoning methods.",
      "keywords": [
        "visual generation",
        "textual reasoning",
        "multimodal interaction",
        "interleaved framework",
        "reinforcement learning",
        "supervised fine-tuning",
        "zero-shot prompting"
      ],
      "status": "new",
      "notes": null,
      "created_at": "2025-11-23T20:14:09.461895",
      "needs_llm_processing": false
    },
    "6": {
      "id": "2511.16670v1",
      "title": "Learning to Think Fast and Slow for Visual Language Models",
      "authors": [
        {
          "name": "Chenyu Lin",
          "affiliation": null
        },
        {
          "name": "Cheng Chi",
          "affiliation": null
        },
        {
          "name": "Jinlin Wu",
          "affiliation": null
        },
        {
          "name": "Sharon Li",
          "affiliation": null
        },
        {
          "name": "Kaiyang Zhou",
          "affiliation": null
        }
      ],
      "abstract": "When confronted with complex problems, we tend to think slowly; conversely, for simple questions, we think quickly. Such a two-system thinking mechanism allows us to efficiently allocate cognitive resources, enabling quick decision-making for straightforward issues while reserving deeper analytical thinking for more intricate challenges. However, existing reasoning-oriented visual language models (VLMs), whether trained with explicit chain-of-thought annotations or rule-based RL rewards, mainly pursue lengthy, detailed reasoning chains, which often lead to excessive computational costs. In this work, we propose a simple RL approach, which enables VLMs to automatically switch between fast and slow thinking modes depending on task difficulty. The approach consists of two stages: in the first stage, we label data as either requiring fast thinking or slow thinking based on the model output length, which is inspired by the observation that pre-trained VLMs typically produce answers of varying lengths for different types of questions; in the second stage, we train the model using GRPO along with the thinking mode labels to develop dual-mode thinking. Despite its simplicity, our model, named DualMindVLM, significantly outperforms the base model and achieves performance on par with state-of-the-art visual reasoning models, while maintaining exceptionally high token efficiency.",
      "arxiv_id": "2511.16670v1",
      "arxiv_url": "http://arxiv.org/abs/2511.16670v1",
      "pdf_url": "https://arxiv.org/pdf/2511.16670v1",
      "published": "2025-11-20T18:59:48Z",
      "updated": "2025-11-20T18:59:48Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "full_text": "Learning to Think Fast and Slow for Visual Language Models\nChenyu Lin1Cheng Chi2BJinlin Wu3Sharon Li4Kaiyang Zhou1B\n1Hong Kong Baptist University2Beijing Academy of Artificial Intelligence\n3Institude of Automation, CAS4University of Wisconsin-Madison\nhttps://github.com/maifoundations/DualMindVLM\nAbstract\nWhen confronted with complex problems, we tend to\nthink slowly; conversely, for simple questions, we think\nquickly. Such a two-system thinking mechanism allows us\nto efficiently allocate cognitive resources, enabling quick\ndecision-making for straightforward issues while reserv-\ning deeper analytical thinking for more intricate chal-\nlenges. However, existing reasoning-oriented visual lan-\nguage models (VLMs), whether trained with explicit chain-\nof-thought annotations or rule-based RL rewards, mainly\npursue lengthy, detailed reasoning chains, which often lead\nto excessive computational costs. In this work, we propose\na simple RL approach, which enables VLMs to automati-\ncally switch between fast and slow thinking modes depend-\ning on task difficulty. The approach consists of two stages:\nin the first stage, we label data as either requiring fast\nthinking or slow thinking based on the model output length,\nwhich is inspired by the observation that pre-trained VLMs\ntypically produce answers of varying lengths for different\ntypes of questions; in the second stage, we train the model\nusing GRPO along with the thinking mode labels to de-\nvelop dual-mode thinking. Despite its simplicity, our model,\nnamedDualMindVLM, significantly outperforms the base\nmodel and achieves performance on par with state-of-the-\nart visual reasoning models, while maintaining exception-\nally high token efficiency.\n1. Introduction\nHuman cognition is widely recognized to operate through\ntwo thinking systems\u2014System 1 and System 2 [6, 7, 11].\nSystem 1 enables fast, automatic responses to routine or\nsimple scenarios, while System 2 engages in slow, delib-\nerate reasoning for intricate or unknown challenges. Re-\nmarkably, the human brain can efficiently integrate multi-\nmodal information, such as visual or linguistic cues, and\nBCorresponding authors.dynamically switch between these two modes of thinking\ndepending on the context. This synergy between intuitive\nperception and analytical reasoning across diverse sensory\ninputs offers valuable insights for designing more cogni-\ntively aligned visual language models (VLMs).\nCurrent research on visual reasoning models primarily\nemphasizes step-by-step reasoning [4, 5, 9, 27, 34, 37\u201339,\n42\u201344], encouraging behaviors such as detailed image de-\nscription or reflective reasoning to elongate their reasoning\nchains. However, existing approaches ignore the human-\nlike dual-mode thinking mechanism, causing excessive rea-\nsoning on simple problems and thus leading to redundant\ntoken usage. As shown in Figure 1, the model trained with\nGroup Relative Policy Optimization (GRPO) [26], which\nexhibits the System-2-like reasoning behavior, produces\nsubstantially longer reasoning chains compared to the base\nmodel. While such step-by-step reasoning benefits chal-\nlenging problems like math (Figure 1 right), it incurs unnec-\nessary computational overhead on simpler ones, e.g., recog-\nnizing the emoji in Figure 1 left is straightforward but the\nGRPO model spends excessive tokens to produce the an-\nswer.\nIn this work, we introduceDualMindVLM, a dual-\nmode thinking VLM that can automatically switch between\nfast and slow thinking modes based on the difficulty level\nof the problem. DualMindVLM is learned using a simple\nRL approach based on question-answer pairs. The approach\nconsists of two stages, as illustrated in Figure 3. The first\nstage assigns each sample a thinking mode label, which\nindicates whether the model should activate fast thinking\nor slow thinking. We use the model\u2019s output length as a\nproxy for problem difficulty: shorter outputs indicate eas-\nier problems and are labeled as fast-thinking cases, whereas\nlonger outputs correspond to harder problems and are la-\nbeled as slow-thinking cases. The second stage aims to de-\nvelop dual-mode thinking in the model through RL: for easy\nquestions, the model receives higher rewards for using fast\nthinking, whereas for hard questions, the model is incen-\ntivized to activate slow thinking.\n1arXiv:2511.16670v1  [cs.CV]  20 Nov 2025\nWhat emotion does this emoji expr ess?\nOptions: A: happy , B: sad, C: excited,\nD: angry\nThe emoji in the image shows a face with\nclosed eyes and a slight frown, which typically\nrepresents a sad or disappointed emotion.\n\\n\\n\\\\boxed{ B: sad }\nTo determine the emotion expressed by the\nemoji, let's analyze its features:\n1. The emoji has closed eyes, which often\nindicate a state of rest, sleep, or perhaps\n...   Therefore, the correct answer is \\\\boxed{ B}As shown in the figur e, the two chords AB and CD in the cir cle\nintersect at E, \u2220 D = 35\u00b0, \u2220 AEC = 105\u00b0, then \u2220 C = ()\nOptions: A:60 \u00b0, B: 70 \u00b0, C:80 \u00b0,  D:85 \u00b0 \nBase Model\n34 Tokens\nGRPO\n126 Tokens\nDualMindVLM\nShort Thinking: The emoji has closed eyes\nand a downturned mouth, which are typical\nexpressions of sadness. \\\\boxed{ B}26 TokensTo solve for \u2220 C, we need to use the properties of angles in a circle, speci-\nfically the fact that the sum of angles in a triangle is 180\u00b0 ...\nStep 1: Identify the given information and the relationships between the angles.\n\u2220D = 35\u00b0, \u2220 AEC = 105\u00b0 ...\nStep 2: ...\nTherefore, the correct answer is  \\\\boxed{B}GRPO\n556 Tokens\nDualMindVLM\nLong Thinking: To solve this problem, we need to use the properties of\nangles ... **Identify the given information: ...  **Understand the relationship\nbetween the angles:** ...  Thus, the correct answer is \\\\boxed{B}.280 TokensTo solve for the measure of \u2220 C in the given circle, we need to use the\nproperties of angles in a circle, specifically the fact that the measure of an angle\nformed by ... Here are the steps: ...\nSo, the measure of \u2220 C is \\\\boxed{75\u00b0}Base Model\n361 TokensEasy Question Hard QuestionFigure 1.Comparison among the base model, the GRPO model and our DualMindVLM. For simple queries, the GRPO model tends\nto produce unnecessarily long responses, leading to additional computational overhead for questions that the base model can already handle\nefficiently. In contrast, our model adaptively balances response length by maintaining concise answers for simple queries and engaging in\ndetailed reasoning for complex ones through two automatically selected modes of thinking.\n/uni00000018/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013 /uni00000016/uni00000013/uni00000013\n/uni00000037/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057/uni00000056/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni00000026/uni00000058/uni00000050/uni00000058/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000027/uni00000058/uni00000044/uni0000004f/uni00000030/uni0000004c/uni00000051/uni00000047/uni00000039/uni0000002f/uni00000030/uni00000010/uni0000001a/uni00000025\n/uni00000032/uni00000053/uni00000048/uni00000051/uni00000039/uni0000002f/uni00000037/uni0000004b/uni0000004c/uni00000051/uni0000004e/uni00000048/uni00000055/uni00000010/uni0000001a/uni00000025\n/uni00000039/uni0000002f/uni00000010/uni00000035/uni00000048/uni00000057/uni0000004b/uni0000004c/uni00000051/uni0000004e/uni00000048/uni00000055/uni00000010/uni0000001a/uni00000025\n/uni00000034/uni0000005a/uni00000048/uni00000051/uni00000010/uni00000015/uni00000011/uni00000018/uni00000010/uni00000039/uni0000002f/uni00000010/uni0000001a/uni00000025/uni00000003/uni0000000b/uni00000025/uni00000044/uni00000056/uni00000048/uni0000000c\nFigure 2.Accuracy vs. token budgets. Under the same token\nbudget, DualMindVLM performs favorably against other models.\nTo demonstrate computational efficiency, we present the\ncumulative accuracy of DualMindVLM and some leading\nVLMs on the MMStar benchmark [3] under varying token\nbudgets in Figure 2. It is clear that existing System-2 rea-\nsoning models require substantially more tokens to reach\ndecent accuracy whereas DualMindVLM shows superior\ntoken efficiency. Furthermore, we conduct extensive experi-\nments on a wide range of multimodal benchmarks spanning\nmathematics [20, 35], science [12, 18], and general visualunderstanding problems [3, 14]. The results show that Dual-\nMindVLM consistently delivers highly competitive perfor-\nmance compared to state-of-the-art reasoning VLMs while\nmaintaining exceptionally high token efficiency.\nIn summary, our main contributions are threefold: 1) We\nreveal the overthinking problem in state-of-the-art System-\n2-like visual reasoning models; 2) We propose a simple RL\nframework that can turn a VLM into a System 1+2 thinking\nmachine using simple question-answer pairs; 3) Extensive\nexperiments are conducted on six multimodal benchmarks\nto demonstrate the effectiveness of DualMindVLM. Code\nand models will be made publicly available to facilitate fu-\nture research.\n2. Related Work\nVisual reasoning.Driven by the recent advances in reason-\ning capabilities of LLMs [8, 10, 31], the vision community\nhas increasingly focused on equipping VLMs with step-\nby-step reasoning abilities. Early efforts [39, 43] concen-\ntrate on constructing high-quality chain-of-thought datasets\nand teaching models to follow predefined reasoning pat-\nterns through supervised fine-tuning (SFT). With the intro-\nduction of GRPO [26], researchers have begun exploring\nreinforcement learning (RL)\u2013based methods that leverage\nverifiable reward signals to elicit the inherent reasoning ca-\n2\nBase Model RolloutsAverage Length\nComputation\nSlow ThinkingFast Thinking(a) Thinking Mode Auto-Labeling\n(b) Learning Dual-Mode Thinking\nPolicy ModelReward\nComputationGroup Advantage\nComputationfree-form\npolicy updateAverage Accuracy\nComputation\nRolloutsfilter  out 0/1 samples to avoid zer o gradient\n< 100 Tokens\n> 200 Tokens\nw/ prefix...\n......\n...Figure 3.Overview of DualMindVLM. (a) For each VQA pair, we annotate its thinking mode based on the base model\u2019s response length\nand discard samples for which all responses are correct or incorrect (to avoid zero relative advantage in GRPO training). (b) During GRPO,\nthe thinking mode label is used to guide the generation of a group of candidate responses, while the other group of responses are generated\nusing the model\u2019s own judgment. A group-wise advantage is computed using all candidate responses to update the model.\npabilities of VLMs. Several studies [4, 5, 9, 30, 42, 44, 45]\nadopt a two-stage SFT+RL paradigm, where SFT serves as\na strong initialization or provides guidance for subsequent\nRL optimization. In contrast, other works [23, 34, 36, 38]\npursue RL-only strategies, aiming to encourage deliberate,\nslow-thinking behavior through detailed descriptions or re-\nflective reasoning. However, these methods have largely\noverlooked that not all tasks require step-by-step reasoning,\nleading to unnecessary computational overhead on simpler\nproblems.\nEfficient reasoning.Improving the efficiency of reasoning\nmodels has recently attracted growing interest in language\ntasks. Chain-of-Draft [40] encourages models to gener-\nate concise intermediate steps, while DAST [28] and Ada-\nCoT [16] employ RL to penalize unnecessarily long reason-\ning trajectories. Some approaches [1, 41] train unified mod-\nels that support multiple reasoning modes, yet users must\nstill manually select the appropriate mode. In the multi-\nmodal domain, however, the ability to reason both effec-\ntively and efficiently\u2014by automatically adapting reasoning\nmodes to task complexity\u2014remains largely under-explored.\n3. Methodology\nExisting visual reasoning methods primarily focus on Sys-\ntem 2 thinking, i.e., generating detailed chain-of-thought\nreasoning, while overlooking the development of System 1\nthinking, leading to unnecessary token redundancy for sim-\nple queries. To fill the gap, we propose DualMindVLM, a\ndual-mode thinking model that is trained using RL and sim-\nple visual question-answer pairs.\nAs shown in Figure 3, the overall training pipeline of\nDualMindVLM consists of two stages. The first stage,\n/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni0000004f/uni00000003/uni00000009/uni00000003/uni00000032/uni00000026/uni00000035 /uni00000026/uni00000052/uni00000058/uni00000051/uni00000057/uni0000004c/uni00000051/uni0000004a /uni00000026/uni0000004b/uni00000044/uni00000055/uni00000057/uni00000003/uni00000009/uni00000003/uni00000027/uni0000004c/uni00000044/uni0000004a/uni00000055/uni00000044/uni00000050 /uni00000030/uni00000044/uni00000057/uni0000004b/uni00000013/uni00000015/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni00000035/uni00000048/uni00000056/uni00000053/uni00000052/uni00000051/uni00000056/uni00000048/uni00000003/uni0000002f/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004bFigure 4. Average response lengths of a pre-trained general-\npurpose VLM across a variety of VQA tasks. The simpler the\nquestion, the shorter the response. The harder the question, the\nlonger the response. These insights are indicative of task diffi-\nculty.\nthinking mode auto-labeling, aims to partition the train-\ning data into two subsets, one for developing slow thinking\nwhile the other for stimulating fast thinking. The second\nstage,learning dual-mode thinking, leverages the thinking\nmode labels obtained in the first stage to learn dual-mode\nthinking behaviors. Specifically, the model generates two\ngroups of rollouts. One group of rollouts is guided by a\nthinking mode-specific prefix, i.e., slow thinking for hard\nquestions and fast thinking for easy ones. The other group\nof rollouts is generated in free-form, i.e., the model uses\nits own judgment to decide which thinking mode to acti-\nvate. By jointly optimizing these two groups of rollouts, the\nmodel can gradually develop the ability to switch between\nslow thinking and fast thinking depending on task difficulty.\nBelow we detail the designs of these two stages.\n3\n3.1. Thinking Mode Auto-Labeling\nTo develop dual-mode thinking, it is intuitive to label ques-\ntions according to their required thinking effort. Such\nsupervision helps the model learn to respond quickly to\neasy problems while engaging in step-by-step reasoning on\nharder ones. Although one can obtain these annotations us-\ning third-party models (e.g., GPT-4o) or human evaluators,\nthis approach introduces considerable monetary and labor-\nrelated costs. In this work, we propose a more straightfor-\nward and cost-effective method for obtaining thinking mode\nlabels by utilizing the model itself.\nInsights in model output lengths.We observe that pre-\ntrained general-purpose VLMs typically produce answers\nof varying lengths for different types of problems. Specif-\nically, we measure the average response length and accu-\nracy of the popular Qwen2.5-VL-7B model [2] across a va-\nriety of VQA tasks. As shown in Figure 4, for simple ques-\ntions like recognition and counting, the response is typically\nshort; for more complex problems like chart understanding\nand math, the response is mostly long and includes more\nelaborate reasoning chains.\nThe labeling process.Based on these insights, we assign\nto each question a label, i.e., fast thinking (easy) or slow\nthinking (hard). As illustrated in Figure 3(a), we prompt\nthe base model to generate a number of rollouts (e.g., 8)\nper training sample and determine the label based on the\naverage response length: if the average length is below\n100 tokens, the data is labeled as fast thinking; if the av-\nerage length exceeds 200 tokens, the data is labeled as slow\nthinking. Those questions with response length falling in\nbetween 100 and 200 tokens are discarded to ensure clear\nseparation between the two modes. To mitigate the prob-\nlem of vanishing advantages [23, 34], we exclude samples\nfor which the average model accuracy is 0 or 1, as such\ncases do not have any relative advantage. This labeling pro-\ncess naturally aligns with the subsequent RL training, as the\nsampling model is more likely to produce responses with\nlengths consistent with the assigned labels.\n3.2. Learning Dual-Mode Thinking\nThe goal of this stage is to develop dual-mode thinking abil-\nities via RL. The main idea is to use the thinking mode la-\nbels obtained above to guide the rollouts of the model: half\nwith a thinking mode-specific prefix and half in free-form.\nGRPO [26] is used to calculate the reward and update the\nmodel. See Figure 3(b) for illustration.\nDual-mode thinking prompt.As shown in Figure 5,\nour system prompt asks the model to output a spe-\ncific thinking mode prefix before answering a ques-\ntion. Specifically, for simple questions, the model is\nencouraged to generate the fast thinking prefix,pfast=\n\"Short Thinking:\". For more challenging problems,System Prompt\nYou are a Vision-Language Model answering questions\nabout images. Follow these rules strictly:\n1. Judge the length of reasoning needed.\n- Short: start with\"Short Thinking:\".\n- Long: start with\"Long Thinking:\".\n2.\"Short Thinking:\"give a concise thinking process\nwhich is sufficient to answer the question, then provide the\nfinal answer.\n3.\"Long Thinking:\"give a structured reasoning pro-\ncess of the question and the image, including question\nanalysis, visual details description, self-verification and\nthen provide the final answer.\n4. The final answer MUST BE put in\\boxed{}.\nFigure 5. System prompt for dual-mode RL training.\nthe model is expected to produce the slow thinking prefix,\npslow=\"Long Thinking:\". The prefix acts as a con-\ntrol signal for switching between the two thinking modes.\nGiven the nature of next-token prediction [24], the model\nis steered to produce fast responses when the prefix shows\nshort thinking, and long reasoning chains when the prefix\nindicates long thinking. The design also offers flexibility in\ndeployment: the user can either specify a preferred thinking\nmode by inserting the corresponding prefix into the prompt\nor just let the model automatically choose a thinking mode\nbased on task difficulty.\nHybrid group response sampling.Since the base model\nhas not been trained to follow the dual-mode thinking\nparadigm, it struggles to generate the desired output format\nat the beginning of RL training. In particular, the model of-\nten fails to generate the thinking mode prefix or produces\nanswers inconsistent with the chosen prefix, e.g., the model\nmay produce a long answer for the short thinking prefix.\nThis problem leads to unstable training. To address this\nproblem, we introduce hybrid group response sampling. For\neach question, half of the sampled responses are forced to\nbegin with the prefix corresponding to the annotated think-\ning mode. For instance, if the question was labeled as fast\nthinking, we manually insert the \u201cshort thinking\u201d prefix to\nthe end of the system prompt for this subgroup to encour-\nage the model to perform fast thinking. The other half are\ngenerated freely, i.e., the model relies on its own judgment\nto decide whether to activate slow thinking or fast think-\ning. This design provides clear advantage signals to help\nthe model quickly acquire the ability to use appropriate pre-\nfixes.\nFormally, given an inputx= (I, Q)whereIdenotes an\nimage andQthe query, the sampling model\u03c0 \u03b8oldgenerates\ntotallyncandidate responses, which are divided into two\nsubgroups: the free-form subgroup{y i}m\ni=1and the prefix-\n4\nconditioned subgroup{\u02c6y i}n\ni=m+1 . For the latter, each re-\nsponse\u02c6y icontains a manually-inserted prefix. Below we\ndiscuss the reward computation using only the notationy\nfor clarity.\nEach responsey iis evaluated by a reward function con-\nsisting of an accuracy rewardr aand a format rewardr f.\nThe accuracy reward equals to 1 if the predicted answer\nis correct and 0 otherwise. The format reward evaluates\nwhether the correct thinking mode prefixp\u2217is generated:\nrf(yi) =\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f31,ifprefix(y i) =p\u2217,\n0.5,ifprefix(y i)\u0338=p\u2217\nandprefix(y i)\u2208 {pfast, pslow},\n0,otherwise,(1)\nwhereprefix(y i)denotes the prefix extracted from the\ngenerated responsey i.\nThe final reward for each response is computed as:\nr(yi) =r f(yi) +r a(yi).(2)\nWe then calculate the relative advantage for each candi-\ndate response as:\nAi=r(y i)\u2212mean(r(y 1), r(y 2), ..., r(y n)),(3)\nwhere we omit the normalization of variance to elimi-\nnate bias towards overly simple or difficult samples [15].\nNote that the advantage is calculated using all candidate re-\nsponses.\nThe policy model\u03c0 \u03b8is optimized using the GRPO ob-\njective with a KL penalty:\nJGRPO(\u03b8) =1\nnnX\ni=1\"\nmin \n\u03c0\u03b8(yi|x)\n\u03c0\u03b8old(yi|x)Ai,\nclip\u0012\u03c0\u03b8(yi|x)\n\u03c0\u03b8old(yi|x),1\u2212\u03f5,1 +\u03f5\u0013\nAi!\n+\u03b2D KL(\u03c0\u03b8|\u03c0ref)#\n.\n(4)\nwhere\u03f5and\u03b2are both hyper-parameters.\u03f5controls the tol-\nerance for policy deviation, while\u03b2determines the strength\nof the KL penalty, preventing the policy from drifting too\nfar from the reference model\u03c0 ref.\n4. Experiments\n4.1. Experimental Setup\nTraining data.We combine multiple public datasets cov-\nering general visual understanding [18, 25, 29], spatial rea-\nsoning [13], chart and document understanding [12, 17, 19,21, 22], and mathematical reasoning [23?]. After apply-\ning the thinking mode labeling process, we end up with\na dataset containing 37,506 visual question-answer pairs,\namong which 18,778 are slow-thinking samples and 18,728\nare fast-thinking samples. The detailed composition of the\ntraining dataset is provided in the supplementary.\nBenchmarks.We evaluate our approach on a wide range\nof multimodal benchmarks. For mathematical reasoning,\nwe choose MathVista [20] (Testmini) and MathVision [35]\n(Test). For general visual understanding, we evaluate on\nMMStar [3] and MMbench (EN) [14]. For scientific QA,\nwe use ScienceQA [18] and AI2D [12].\nImplementation details.We adopt Qwen2.5-VL-7B [2] as\nour base model. Training is performed using the TRL [33]\nframework. During inference rollouts, we samplen= 8\ncompletions per question. We set the learning rate to1\u00d7\n10\u22126, rollout batch size to 256, KL coefficient to1\u00d710\u22123,\nand maximum generation length to 2,048 tokens.\n4.2. Main Results\nTable 1 presents a detailed comparison of our DualMind-\nVLM against state-of-the-art visual reasoning models of\nsimilar sizes. Note that all models except LLaV A-CoT\nand R1-VL are based on the same model, i.e., Qwen2.5-\nVL. Overall, DualMindVLM achieves state-of-the-art per-\nformance while exhibiting exceptionally high token effi-\nciency.\nComparison with the base model.Compared with the\nbase model Qwen2.5-VL, DualMindVLM obtains signifi-\ncant improvement in accuracy onallbenchmarks. Specif-\nically, DualMindVLM improves the accuracy by +7.4%\non MathVista, +5.1% on MathVision, +1.4% on MMStar,\n+5.3% on MMBench, +3.2% on ScienceQA, and +3.0% on\nAI2D. It is also worth mentioning that DualMindVLM\u2019s av-\nerage output length is shorter than the base model across all\nbenchmarks. These results strongly demonstrate the effec-\ntiveness and efficiency of our model.\nComparison with leading reasoning models.We com-\npare DualMindVLM with the latest state-of-the-art reason-\ning models, including VL-Rethinker [34], ThinkLite [36],\nMM-Eureka [23], OpenVLThinker [4], R1-VL [44], R1-\nOnevision [42], and LLaV A-CoT [39]. In terms of accuracy,\nDualMindVLM beats the best-performing rivals on four\nout of six benchmarks, namely MathVista, MMStar, Sci-\nenceQA, and AI2D. On MathVision and MMBench, Dual-\nMindVLM\u2019s performance is close to state-of-the-art. In\nterms of token usage, DualMindVLM outperforms the rea-\nsoning models on all benchmarks except on MathVision\nwhere OpenVLThinker produces the least tokens. Com-\npared with the best-performing rival on each benchmark,\nDualMindVLM reduces token usage by 40% on average.\n5\nTable 1.Comparison of DualMindVLM with state-of-the-art visual reasoning models. For each benchmark, we report accuracy (acc,\n%) and average response length (len, #tokens). The best result is highlighted inbold. DualMindVLM strikes the best balance between\naccuracy and token efficiency among all models.\nModel Size StrategyMathVista MathVision MMStar MMBench ScienceQA AI2D Avg\nacc\u2191len\u2193acc\u2191len\u2193acc\u2191len\u2193acc\u2191len\u2193acc\u2191len\u2193acc\u2191len\u2193acc\u2191len\u2193\nQwen2.5-VL 7B - 68.2 205 25.1 511 63.9 155 83.0 73 84.0 156 80.8 145 67.5 208\nLLaV A-CoT 11B SFT 54.8 350 - - 57.6 464 - - - - - - - -\nR1-Onevision 7B SFT+RL 64.1 279 29.9 560 - - - - - - - - - -\nR1-VL 7B SFT+RL 63.5 263 24.7 363 60.0 221 - - - - - - - -\nOpenVLThinker 7B SFT+RL 72.3 242 25.932663.3 200 87.5 177 82.2 171 83.2 160 69.1 213\nMM-Eureka 7B RL 73.0 252 26.9 612 64.1 246 87.3 159 83.5 202 83.5 207 69.7 280\nThinkLite 7B RL 75.1 247 28.5 599 65.0 17588.7113 - - 83.6 168 - -\nVL-Rethinker 7B RL 74.9 26832.3566 64.9 231 87.6 201 85.5 205 82.4 226 71.3 283\nDualMindVLM 7B RL 75.6 184 30.2 446 65.3 121 88.3 69 87.2 98 83.8 104 71.7 170\nTable 2. Ablation study on key components of DualMindVLM.\nModelMathVista MathVision\nacc\u2191len\u2193acc\u2191len\u2193\nQwen2.5-VL 68.2 205 25.1 511\nDualMindVLM 75.6 184 30.2 446\nw/o auto-labeling 72.6 120 28.5 332\nw/o dual-mode RL 75.0 271 28.9 584\n/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013\n/uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000036/uni00000057/uni00000048/uni00000053/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000029/uni00000044/uni00000056/uni00000057/uni00000003/uni00000037/uni0000004b/uni0000004c/uni00000051/uni0000004e/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000035/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000027/uni00000058/uni00000044/uni0000004f/uni00000030/uni0000004c/uni00000051/uni00000047/uni00000039/uni0000002f/uni00000030\n/uni0000005a/uni00000012/uni00000052/uni00000003/uni00000024/uni00000058/uni00000057/uni00000052/uni00000010/uni0000002f/uni00000044/uni00000045/uni00000048/uni0000004f/uni0000004c/uni00000051/uni0000004a\nFigure 6.Fast thinking ratios recorded during training.With-\nout auto-labeling, the model quickly collapses to the fast think-\ning mode only, whereas the complete model keeps the ratio well-\nbalanced at around 50%.\nOverall, DualMindVLM achieves the best balance between\naccuracy and token efficiency.\n4.3. Ablation Study\nEffect of thinking mode auto-labeling.Recall that our ap-\nproach consists of two stages: thinking mode auto-labeling\nand dual-mode RL (see Figure 3). We first evaluate the role\nof auto-labeling. By removing the auto-labeling stage\u2014\nmeaning that we loss the thinking mode labels\u2014we rely\nonly on the the dual-mode system prompt shown in Figure 5to develop the two thinking systems. The results are shown\nin Table 2. The accuracy drops significantly from 75.6% to\n72.6% on MathVista, and from 30.2% to 28.5% on Math-\nVision. During training, we find that the model quickly col-\nlapses to the thinking mode with higher initial likelihood\n(i.e., the fast-thinking mode, see Figure 6), which explains\nwhy the token length is shorter (120 vs. 184 on MathVista\nand 332 vs. 446 on MathVision). The collapse significantly\nlimits the development of reasoning and leads to shorter re-\nsponses and degraded overall performance. The results also\nsuggest that GRPO alone is insufficient to develop effective\nSystem 1+2 thinking.\nEffect of dual-mode RL.By removing dual-mode RL, we\ntrain the model on the same data as DualMindVLM but\nwithout using the thinking mode labels. Specifically, the\nmodel is trained with GRPO to just develop System 2 think-\ning, guided by the prompt\u201cPlease reason step by step. \u201d.\nTable 2 shows that the accuracy declines on both bench-\nmarks: from 75.6% to 75% on MathVista, and from 30.2%\nto 28.9% on MathVision. It is worth noting that this re-\nduced version improves upon the base model with notice-\nable gains in accuracy: 6.8% on MathVista and 4.8% on\nMathVision; and interestingly, the performance is even bet-\nter than some state-of-the-art models shown in Table 1, such\nas MM-Eureka and OpenVLThinker. These results strongly\ndemonstrate the importance of data curation for RL\u2014our\nauto-labeling stage can be viewed as data curation as it pro-\nduces datasets with well-balanced easy and hard samples.\nData-centric RL for reasoning is beyond the scope of our\nwork. We will investigate this topic in future work.\nEffect of free-form rollouts.As discussed, we use a mix-\nture of free-form and prefix-conditioned rollouts to facili-\ntate the learning of automatic System 1+2 thinking. Table 3\nshows the results obtained by varying the number of free-\nform generationsmduring GRPO sampling. We consider\nthree settings: no free-form generation (m= 0), half free-\n6\nTable 3. Effect of free-form rollouts during GRPO sampling.\nFree-Form RolloutsMathVista MathVision\nacc\u2191len\u2193acc\u2191len\u2193\nZero (m= 0) 73.4 172 29.6 519\nHalf (m= 4) 75.6 184 30.2 449\nFull (m= 8) 72.6 120 28.5 332\n/uni00000030/uni00000044/uni00000057/uni0000004b/uni00000039/uni0000004c/uni00000056/uni00000057/uni00000044 /uni00000030/uni00000044/uni00000057/uni0000004b/uni00000039/uni0000004c/uni00000056/uni0000004c/uni00000052/uni00000051/uni00000030/uni00000030/uni00000036/uni00000057/uni00000044/uni00000055/uni00000030/uni00000030/uni00000025/uni00000048/uni00000051/uni00000046/uni0000004b /uni00000036/uni00000046/uni0000004c/uni00000048/uni00000051/uni00000046/uni00000048/uni00000034/uni00000024/uni00000024/uni0000002c/uni00000015/uni00000027/uni00000013/uni00000015/uni00000017/uni00000019/uni0000001b/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000002c/uni00000050/uni00000053/uni00000055/uni00000052/uni00000059/uni00000048/uni00000050/uni00000048/uni00000051/uni00000057/uni00000003/uni0000000b/uni00000008/uni0000000c/uni000000ee/uni00000014/uni00000011/uni00000018/uni0000001a\n/uni000000ee/uni00000014/uni00000011/uni0000001b/uni00000015\n/uni000000ee/uni00000016/uni00000011/uni00000018/uni00000013/uni000000ee/uni00000013/uni00000011/uni0000001c/uni0000001b\n/uni000000ee/uni00000015/uni00000011/uni0000001c/uni00000014/uni000000ee/uni00000014/uni00000011/uni00000018/uni00000013\n/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013\n/uni00000037/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000003/uni00000036/uni00000044/uni00000059/uni0000004c/uni00000051/uni0000004a/uni00000056/uni00000003/uni00000035/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni00000010/uni00000003/uni00000032/uni00000058/uni00000055/uni00000056\n/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni00000010/uni00000003/uni0000002a/uni00000035/uni00000033/uni00000032/uni00000037/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000003/uni00000036/uni00000044/uni00000059/uni0000004c/uni00000051/uni0000004a/uni00000056/uni00000003/uni00000035/uni00000044/uni00000057/uni0000004c/uni00000052\nFigure 7.DualMindVLM vs. GRPO.We report the performance\nimprovements of DualMindVLM and the GRPO model compared\nto the base model, along with the token savings ratio relative to\nGRPO.\nform generation (m= 4), and full free-form generation\n(m= 8). When no free-form generation is adopted, the\nmodel is only guided by a pre-defined thinking mode prefix\nand therefore struggles to learn how to automate the pre-\nfix selection. The model using full free-form generation is\nequivalent to the model trained without the thinking mode\nlabels. In this case, the training collapses quickly and the\nmodel always selects the fast thinking mode.\nDualMindVLM vs. GRPO.Figure 7 compares Dual-\nMindVLM with the GRPO model (i.e., without auto-\nlabeling and dual-mode RL). The bar charts show the accu-\nracy improvement over the base model Qwen2.5-VL. Dual-\nMindVLM significantly beats the GRPO model on most\nbenchmarks, demonstrating the effectiveness of the dual-\nmode thinking mechanism. In terms of token usage, Dual-\nMindVLM saves tokens up to 60% compared with the\nGRPO model.\n4.4. Further Analysis\nThinking mode selection.We calculate the ratios between\nfast and slow thinking modes automatically selected by\nDualMindVLM during inference, as well as the average\noutput lengths for both modes. The thinking mode selec-\ntion ratios are presented in Figure 8. As expected, the\nmodel favors the slow thinking mode for challenging prob-\nlems like math (MathVista and MathVision) and exhibits a\n/uni00000016/uni00000013/uni00000011/uni00000017/uni00000008\n/uni00000019/uni0000001c/uni00000011/uni00000019/uni00000008/uni00000030/uni00000044/uni00000057/uni0000004b/uni00000039/uni0000004c/uni00000056/uni00000057/uni00000044\n/uni00000014/uni00000011/uni00000013/uni00000008\n/uni0000001c/uni0000001c/uni00000011/uni00000013/uni00000008/uni00000030/uni00000044/uni00000057/uni0000004b/uni00000039/uni0000004c/uni00000056/uni0000004c/uni00000052/uni00000051\n/uni00000018/uni0000001a/uni00000011/uni00000019/uni00000008/uni00000017/uni00000015/uni00000011/uni00000017/uni00000008/uni00000030/uni00000030/uni00000036/uni00000057/uni00000044/uni00000055\n/uni0000001a/uni00000015/uni00000011/uni0000001a/uni00000008/uni00000015/uni0000001a/uni00000011/uni00000016/uni00000008/uni00000030/uni00000030/uni00000025/uni00000048/uni00000051/uni00000046/uni0000004b\n/uni00000019/uni00000013/uni00000011/uni00000016/uni00000008/uni00000016/uni0000001c/uni00000011/uni0000001a/uni00000008/uni00000036/uni00000046/uni0000004c/uni00000048/uni00000051/uni00000046/uni00000048/uni00000034/uni00000024\n/uni00000016/uni0000001c/uni00000011/uni00000017/uni00000008\n/uni00000019/uni00000013/uni00000011/uni00000019/uni00000008/uni00000024/uni0000002c/uni00000015/uni00000027\n/uni00000029/uni00000044/uni00000056/uni00000057/uni00000003/uni00000037/uni0000004b/uni0000004c/uni00000051/uni0000004e/uni0000004c/uni00000051/uni0000004a /uni00000036/uni0000004f/uni00000052/uni0000005a/uni00000003/uni00000037/uni0000004b/uni0000004c/uni00000051/uni0000004e/uni0000004c/uni00000051/uni0000004aFigure 8.Thinking mode selection ratios.DualMindVLM\nadapts its thinking mode to task difficulty, favoring slow think-\ning for complex reasoning tasks and fast thinking for perceptual\ntasks.\n/uni00000030/uni00000044/uni00000057/uni0000004b/uni00000039/uni0000004c/uni00000056/uni00000057/uni00000044 /uni00000030/uni00000044/uni00000057/uni0000004b/uni00000039/uni0000004c/uni00000056/uni0000004c/uni00000052/uni00000051/uni00000030/uni00000030/uni00000036/uni00000057/uni00000044/uni00000055/uni00000030/uni00000030/uni00000025/uni00000048/uni00000051/uni00000046/uni0000004b /uni00000036/uni00000046/uni0000004c/uni00000048/uni00000051/uni00000046/uni00000048/uni00000034/uni00000024/uni00000024/uni0000002c/uni00000015/uni00000027/uni00000013/uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000035/uni00000048/uni00000056/uni00000053/uni00000052/uni00000051/uni00000056/uni00000048/uni00000003/uni0000002f/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni00000017/uni00000017 /uni00000017/uni00000015 /uni00000016/uni0000001a /uni00000016/uni00000018/uni00000017/uni00000019 /uni00000017/uni00000017/uni00000015/uni00000017/uni00000018/uni00000017/uni00000018/uni00000013\n/uni00000015/uni00000016/uni00000016\n/uni00000014/uni00000018/uni0000001a/uni00000014/uni0000001b/uni00000013\n/uni00000014/uni00000017/uni00000016/uni00000029/uni00000044/uni00000056/uni00000057/uni00000003/uni00000037/uni0000004b/uni0000004c/uni00000051/uni0000004e/uni0000004c/uni00000051/uni0000004a\n/uni00000036/uni0000004f/uni00000052/uni0000005a/uni00000003/uni00000037/uni0000004b/uni0000004c/uni00000051/uni0000004e/uni0000004c/uni00000051/uni0000004a\nFigure 9.Average response lengths on fast and slow thinking\nmodes.Fast-thinking responses are generally concise, while slow-\nthinking responses vary in length according to task complexity.\nrelatively balanced mode selection behavior on other bench-\nmarks. Figure 9 reports the average output lengths on the six\nbenchmarks. In general, the output generated in fast think-\ning mode remains below 50 tokens, demonstrating stable\nand concise thinking behavior. In contrast, the slow think-\ning mode leads to responses of varying lengths that reflect\ndifferent thinking efforts for different types of problems.\nDataset scale.We explore how the training dataset scale\nimpacts on the performance during our dual-mode training.\nTo this end, we vary the number of samples used for train-\ning DualMindVLM. Specifically, we start from 15k and\nthen gradually increase the number to 37k. Note that these\nnumbers are obtained after applying the thinking mode\nauto-labeling process. The results are drawn in Figure 11\nwhere the accuracy improvement is calculated relative to\nthe base model. We have made some intriguing observa-\ntions. Increasing the scale does not always yield better re-\nsults. Specifically, for challenging problems like those in\nMathVista and MathVision, expanding the dataset proves\n7\nWhat is the sum of the accuracies of the algorithms squad and warm?   Choices: (A) 14  (B) 15  (C) 6  (D) 7\nDualMindVLM\nShort Thinking: Add the values of the bars for \\\"squad\\\" and \\\"warm\\\" from the chart. \\\\boxed{C}\nDualMindVLM w/ For ced Slow Thinking\nLong Thinking: To solve this problem, we need to identify the accuracy values for the algorithms\n\\\"squad\\\" and \\\"warm\\\" from the bar chart and then sum them up.  1. Locate the bar for \\\"squad\\\" on\nthe chart . The height of the bar corresponds to the accuracy value, which appears to be 3 .  2. Locate\nthe bar for \\\"warm\\\" on the chart . The height of the bar corresponds to the accuracy value, which\nappears to be 4 .  3. Add the two values together: (3 + 4 = 7 ). Thus, the sum of the accuracies of the\nalgorithms \\\"squad\\\" and \\\"warm\\\" is 7.  \\\\boxed{D}Figure 10.Failure case.The model selects the wrong thinking mode, potentially caused by mode-selection biases present in training data.\n/uni00000013/uni0000004e /uni00000014/uni00000018/uni0000004e /uni00000015/uni00000013/uni0000004e /uni00000015/uni00000018/uni0000004e /uni00000016/uni00000013/uni0000004e /uni00000016/uni0000001a/uni0000004e\n/uni00000027/uni00000044/uni00000057/uni00000044/uni00000056/uni00000048/uni00000057/uni00000003/uni00000036/uni0000004c/uni0000005d/uni00000048/uni00000013/uni00000018/uni00000014/uni00000013/uni00000014/uni00000018/uni00000015/uni00000013/uni00000035/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000002c/uni00000050/uni00000053/uni00000055/uni00000052/uni00000059/uni00000048/uni00000050/uni00000048/uni00000051/uni00000057/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000030/uni00000044/uni00000057/uni0000004b/uni00000039/uni0000004c/uni00000056/uni00000057/uni00000044\n/uni00000030/uni00000044/uni00000057/uni0000004b/uni00000039/uni0000004c/uni00000056/uni0000004c/uni00000052/uni00000051\n/uni00000030/uni00000030/uni00000036/uni00000057/uni00000044/uni00000055/uni00000030/uni00000030/uni00000025/uni00000048/uni00000051/uni00000046/uni0000004b\n/uni00000036/uni00000046/uni0000004c/uni00000048/uni00000051/uni00000046/uni00000048/uni00000034/uni00000024\n/uni00000024/uni0000002c/uni00000015/uni00000027\nFigure 11.Effect of training dataset scale.Larger scale benefits\ncomplex problems like math. The impact is limited for simpler\nproblems.\nbeneficial, as evidenced by the clear upward trends in both\ncurves. In contrast, for scientific or perceptual tasks such as\nScienceQA, AI2D, MMBench, and MMStar, performance\ngains with increasing data are limited or fluctuate.\nHallucination.Longer reasoning chains are known to have\na higher risk of producing hallucinated answers. We eval-\nuate DualMindVLM as well as five other reasoning VLMs\non HumbleBench [32], a hallucination benchmark consist-\ning of 22,831 multiple-choice questions and covering hal-\nlucinations in relation, attribute, and object. Notably, each\nquestion includes a \u201cNone of the above\u201d option, requiring\nthe model to not only recognize correct visual information\nbut also refuse to choose when all answers are incorrect.\nTable 4 shows that DualMindVLM beats all the competi-\ntors by a clear margin across all hallucination types. These\nresults strongly demonstrate the effectiveness of dual-mode\nthinking in tackling hallucinations.\nLimitations.The thinking mode auto-labeling strategy,\nwhich gives \u201chard\u201d labels to slow and fast thinking, may\nintroduce mode-selection biases tied to specific problem\ntypes. Figure 10 shows a failure case where DualMindVLM\nfails to produce the correct answer\u2014even though the modelTable 4. Comparison of visual reasoning models on Hum-\nbleBench. DualMindVLM performs the best, meaning that dual-\nmode thinking has potential to mitigate hallucinations.\nModelHumbleBench\nRelation\u2191Attribute\u2191Object\u2191Overall\u2191\nR1-OneVision 65.2 73.4 61.4 66.9\nR1-VL 68.0 74.0 63.6 68.7\nMM-Eureka 63.2 74.7 64.1 67.5\nThinkLite 69.5 77.2 66.8 71.3\nVL-Rethinker 68.3 76.6 65.2 70.3\nDualMindVLM 70.0 77.5 67.0 71.7\nidentifies the correct steps that would lead to the right an-\nswer. However, when the model is forced to adopt the slow\nthinking mode, i.e., inserting the long thinking prefix to the\nprompt, it generates a coherent, step-by-step reasoning pro-\ncess and arrives at the correct answer. This problem may\nbe caused by that most chart-related tasks emphasize per-\nceptual ability and therefore are linked to fast thinking. As\na result, the model develops the bias to choose fast think-\ning when it comes to chart-related questions. This behavior\nresembles the mental shortcut in human\u2019s System-1 heuris-\ntics [11]\u2014efficient yet occasionally biased.\n5. Conclusion\nIn this paper, we propose a System 1+2 thinking VLM\nnamed DualMindVLM. The model is learned by first pre-\ndicting thinking mode labels on the training data and\nthen leveraging these labels to develop dual-mode think-\ning through RL. The results on six challenging multimodal\nreasoning benchmarks show that DualMindVLM achieves\nperformance on par with state-of-the-art visual reasoning\nmodels while using much less tokens on average. We hope\nthe findings shared in this work can inspire future research\non developing reasoning models that better mirror human\ncognitive thinking.\n1\nAppendix\nTable 5.Distribution of training samples across different\ndatasets.\u201cFast\u201d and \u201cSlow\u201d indicate the numbers of samples la-\nbeled as fast-thinking and slow-thinking, respectively.\nDataset Category Fast Slow Total\nA-OKVQA General Scene 2500 74 2574\nChartQA Chart 1000 1000 2000\nCLEVR-Math Counting 2500 1000 3500\nDocVQA Document 3000 249 3249\nIconQA Diagram 3000 440 3440\nTabMWP Table 3000 1000 4000\nTextVQA General Scene 3000 15 3015\nVirl Math & Science 728 15000 15728\nA. Training Dataset\nTable 5 presents the composition of our training\ndataset, which was constructed by aggregating eight\nwidely used question-answer datasets: A-OKVQA [25],\nChartQA [21], CLEVR-Math [13], DocVQA [22],\nIconQA [17], TabMWP [18], TextVQA [29], and Virl [34].\nFor each dataset, we report its category, which indicates the\ntype of images it contains. We also provide the numbers\nof samples labeled as fast-thinking, slow-thinking, and total\nsamples after applying the proposed thinking-mode labeling\nprocedure.\nB. More Experiments\nEffect of the labeling threshold.To investigate how the\nlabeling threshold influences model behavior, we evaluate\nfour configurations based on two length thresholds:\u03c4 fast\nand\u03c4 slow. Samples with an average response length below\n\u03c4fastare labeled as fast thinking, whereas those exceeding\n\u03c4sloware labeled as slow thinking. For each configuration,\nwe sample 5k fast-thinking and 5k slow-thinking examples\nfrom the training set according to these thresholds. The\n\u201cNone\u201d configuration serves as a baseline, where fast- and\nslow-thinking labels are assigned uniformly at random.\nFirst, all length-based labeling configurations consis-\ntently outperform the random baseline, suggesting that re-\nsponse length of the base model provides a reliable signal\nfor developing two distinct thinking modes. It encourages\nconcise fast-thinking responses and more elaborate slow-\nthinking reasoning. Second, under the same data scale,\nvarying the threshold values has only a minor effect on the\nfinal performance. However, a stricter fast-thinking thresh-\nold (\u03c4 fast= 50) overly constrains fast-thinking behavior,\ncausing the model to overuse slow thinking and ultimately\nproduce longer responses.Table 6.Effect of the labeling threshold.We report average\naccuracy (Accuracy, %), average response length (Length, # to-\nkens), and the fast thinking mode selection ratios (Ratio-F, %) over\nsix benchmarks. \u201cFast\u201d and \u201cSlow\u201d denote the average response\nlength in fast- and slow-thinking modes, and \u201cTotal\u201d denotes the\noverall average response length.\n\u03c4fast \u03c4slow AccuracyLengthRatio-F\nFast Slow Total\nNone None 68.9 72 131 98 56.3\n100 200 70.1 23 275 164 56.9\n50 200 69.8 23 302 197 47.9\n100 250 69.8 29 290 173 59.7\n/uni00000030/uni00000044/uni00000057/uni0000004b/uni00000039/uni0000004c/uni00000056/uni00000057/uni00000044 /uni00000030/uni00000044/uni00000057/uni0000004b/uni00000039/uni0000004c/uni00000056/uni0000004c/uni00000052/uni00000051/uni00000030/uni00000030/uni00000036/uni00000057/uni00000044/uni00000055/uni00000030/uni00000030/uni00000025/uni00000048/uni00000051/uni00000046/uni0000004b /uni00000036/uni00000046/uni0000004c/uni00000048/uni00000051/uni00000046/uni00000048/uni00000034/uni00000024/uni00000024/uni0000002c/uni00000015/uni00000027/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000002c/uni00000050/uni00000053/uni00000055/uni00000052/uni00000059/uni00000048/uni00000050/uni00000048/uni00000051/uni00000057/uni00000003/uni0000000b/uni00000008/uni0000000c/uni000000ee/uni00000014/uni00000011/uni0000001a/uni00000016/uni000000ee/uni00000014/uni00000011/uni00000015/uni00000017\n/uni000000ee/uni00000014/uni00000011/uni0000001a/uni00000017\n/uni000000ee/uni00000015/uni00000011/uni00000015/uni00000018/uni000000ee/uni00000014/uni00000011/uni00000015/uni00000017\n/uni000000ee/uni00000014/uni00000011/uni00000018/uni00000013\n/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013\n/uni00000037/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000003/uni00000036/uni00000044/uni00000059/uni0000004c/uni00000051/uni0000004a/uni00000056/uni00000003/uni00000035/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni00000010/uni00000003/uni00000032/uni00000058/uni00000055/uni00000056/uni00000010/uni00000016/uni00000025\n/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni00000010/uni00000003/uni0000002a/uni00000035/uni00000033/uni00000032/uni00000010/uni00000016/uni00000025/uni00000037/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000003/uni00000036/uni00000044/uni00000059/uni0000004c/uni00000051/uni0000004a/uni00000056/uni00000003/uni00000035/uni00000044/uni00000057/uni0000004c/uni00000052\nFigure 12.DualMindVLM-3B vs. GRPO-3B.We report the per-\nformance improvements of DualMindVLM-3B and the GRPO-3B\nmodel compared to the base model, along with the token savings\nratio relative to GRPO-3B.\nAdditional results with the 3B model.To assess the scal-\nability of our method, we further evaluate our method on\nQwen2.5-VL-3B [2]. As shown in Figure 12, it still delivers\nconsistent performance gains and substantial token savings\nover GRPO at this smaller scale.\nC. Case Study\nWe present case studies illustrating how DualMindVLM\nadapts to different question types. For relatively sim-\nple perception-centric queries (Figures 13\u201316), the model\nadopts the fast-thinking mode, reducing token usage while\nmaintaining accuracy compared with GRPO. For more\nchallenging reasoning-oriented queries (Figures 17\u201319), it\nswitches to the slow-thinking mode, allocating more tokens\nfor detailed step-by-step reasoning.\n2\nTo solve the problem, let's follow these steps:\\n\\n1. Identify the total number\nof dots: There are 4 dots in total.\\n2. Determine how many groups the dots\nare divided into: The dots are divided into 2 equal groups.\\n3. Calculate the\nnumber of dots in each group: Since there are 4 dots divided equally into 2\ngroups, we need to divide 4 by 2.\\n\\nLet's perform the division:\\n \\[ 4 \\div 2 =\n2 \\]\\n\\nSo, there are 2 dots in each group.\\n\\nThe final answer is: \\boxed{2}GRPO\nDualMindVLM\nShort Thinking: Divide the total number of dots by the number of groups. \\n\n\\boxed{2}Q:  Fill in the blank to describe the\nmodel. The model has 4 dots divided\ninto 2 equal gr oups. Ther e are (_)\ndots in each gr oup.\n123 Tokens\n20 Tokens\nFigure 13. Example responses of the GRPO model and DualMindVLM to a diagram-based VQA question.\nTo determine the highest amount this class measures, let's analyze the image\nstep by step:\\n\\n1. The beaker is labeled with measurement markings.\\n2. The\nmarkings indicate volume in milliliters (ml).\\n3. The highest marking on the\nbeaker is at the top, labeled as \\\"400\\\".\\n\\nTherefore, the highest amount this\nclass measures is 400 ml.\\n\\n \\boxed{400}GRPO\nDualMindVLM\nShort Thinking: The image shows a beaker with measurement markings. The\nhighest marking on the beaker indicates the maximum volume it can\nmeasure.\\n \\boxed{400}Q:  What is the highest amount this\nclass measur es?\n85 Tokens\n35 Tokens\nFigure 14. Example responses of the GRPO model and DualMindVLM to a general scene-based VQA question.\n3\nGRPO\nTo determine which or ganism consumes small fish as food, let's analyze\nthe food chain diagram step by step:\\n\\n1. Phytoplankton is at the base\nof the food chain, converting sunlight into ener gy.\\n2. Zooplankton\nfeeds on phytoplankton.\\n3. Small fish feed on zooplankton.\\n4.\nMackerel feeds on small fish.\\n5. Mahi mahi feeds on mackerel.\\n6.\nLarge shark feeds on mahi mahi.\\n\\nFrom the diagram, we can see that\nthe mackerel is shown to consume small fish.\\n\\nTherefore, the correct\nanswer is \\boxed{D}Q:  In the diagram shown, which\norganism consumes small fish as food?\nOptions: A: mahi mahi, B: large shark,\nC: zooplankton, D: macker el\nShort Thinking: The diagram shows arrows pointing from small fish to\nmackerel, indicating that mackerel consumes small fish. \\n \\boxed{D}DualMindVLM128 Tokens\n31 TokensFigure 15. Example responses of the GRPO model and DualMindVLM to a scientific VQA question.\nTo determine the value of the smallest bar in the bar chart, let's follow these\nsteps:\\n\\n1. Identify the height of each bar:\\n   - The bar labeled \\\"saw\\\" is\napproximately at 3.\\n   - The bar labeled \\\"sodium\\\" is approximately at 6.\\n  - The\nbar labeled \\\"silk\\\" is approximately at 7.\\n   - The bar labeled \\\"outset\\\" is\napproximately at 5.\\n   - The bar labeled \\\"tunnel\\\" is approximately at 4.\\n\\n2.\nCompare the heights to find the smallest one:\\n   - The bar labeled \\\"saw\\\" is the\nshortest among all.\\n\\n3. Confirm the value of the smallest bar:\\n   - The value of\nthe bar labeled \\\"saw\\\" is 3.\\n\\nTherefore, the value of the smallest bar is\n3.\\n\\nThe answer is: \\boxed{A}GRPO\nDualMindVLM\nShort Thinking: Compare the heights of the bars to identify the smallest one.\\n\nThe smallest bar corresponds to the value of 3.\\n \\boxed{A}Q:  Is Periwinkle the maximum? \nChoices: (A) yes;  (B) no\n176 Tokens\n31 Tokens\nFigure 16. Example responses of the GRPO model and DualMindVLM to a chart-based VQA question.\n4\nLong Thinking: To solve this problem, we need to use the properties of angles\nformed by intersecting lines and angle bisectors.\\n\\n1. **Identify the given\ninformation:**\\n   - \u2220 BOC = 50\u00b0.\\n   - Since \u2220 BOC and \u2220 AOD are vertical\nangles, \u2220 AOD = 50\u00b0.\\n   - OD bisects \u2220 AOE, meaning \u2220 DOE = \u2220 AOD =\n50\u00b0.\\n\\n2. **Determine \u2220 AOE:**\\n   - Since \u2220 DOE = 50\u00b0 and OD bisects\n\u2220AOE, \u2220 AOE = 2 * \u2220 DOE = 2 * 50\u00b0 = 100\u00b0.\\n\\n3. **Determine \u2220 EOB:**\\n \n - \u2220AOE and \u2220 BOE are supplementary angles because they form a straight\nline at point O. Therefore, \u2220 AOE + \u2220 BOE = 180\u00b0.\\n   - Substituting the value\nof \u2220AOE, we get 100\u00b0 + \u2220 BOE = 180\u00b0.\\n   - Solving for \u2220 BOE, we get\n\u2220BOE = 180\u00b0 - 100\u00b0 = 80\u00b0.\\n\\nThus, the correct answer is \\boxed{D}.DualMindVLMQ:  As shown in the figur e, the straight lines AB and CD intersect at point O, OD bisects \u2220 AOE, \u2220 BOC =\n50.0, then \u2220 EOB = ().\nChoices:(A) 50\u00b0, (B) 60\u00b0, (C) 70\u00b0, (D) 80\u00b0\n291 TokensFigure 17. An example response of DualMindVLM to a geometric reasoning VQA question.\nLong Thinking: To solve this problem, we need to consider how the tables can\nbe arranged and how many people can be seated at each table.\\n\\n1. **Single\nTable**: A single square table can seat 4 people, one at each side.\\n2. **T wo\nTables Together**: When two tables are pushed together , the two adjacent\nsides of the tables mer ge, and the remaining sides can be used for seating.\nThis means that two tables pushed together can seat \\(4 + 4 - 2 = 6 \\) people\n(since the two adjacent sides are shared, reducing the seating capacity by\n2).\\n3. **Three Tables Together**: Following the same logic, three tables\npushed together can seat \\(6 + 4 - 2 = 8 \\) people.\\n4. **Four Tables\nTogether**: Similarly , four tables pushed together can seat \\(8 + 4 - 2 = 10 \\)\npeople.\\n\\nThus, the maximum number of people that can be seated if four\ntables are pushed together in one row is \\(\\boxed{10} \\).DualMindVLMQ:  Four  people can be seated at a squar e table. How many people at most could be seated if we pushed\nfour tables of this kind together  in one r ow?\n213 Tokens\nFigure 18. An example response of DualMindVLM to a logic reasoning VQA question.\n5\nLong Thinking: To solve this problem, we need to determine the length of the\nroad from Anna's to John's house. We can use the given distances and the fact\nthat the roads form a closed loop.\\n\\n1. **Identify the given distances:**\\n   -\nThe road from Anna's to Mary's house is \\(16 \\text{ km} \\).\\n   - The road from\nMary's to John's house is \\(20 \\text{ km} \\).\\n   - The road from the crossing to\nMary's house is \\(9 \\text{ km} \\).\\n\\n2. **Determine the length of the road\nfrom the crossing to Anna's house:**\\n   - Let the length of the road from the\ncrossing to Anna's house be \\(x \\text{ km} \\).\\n   - The total length of the road\nfrom Anna's to Mary's house is the sum of the road from the crossing to\nMary's house and the road from the crossing to Anna's house. Therefore, \\(x +\n9 = 16 \\).\\n   - Solving for \\(x\\), we get \\(x = 16 - 9 = 7 \\text{ km} \\).\\n\\n3.\n**Determine the length of the road from the crossing to John's house:**\\n   -\nLet the length of the road from the crossing to John's house be \\(y \\text{\nkm}\\).\\n   - The total length of the road from Mary's to John's house is the\nsum of the road from the crossing to John's house and the road from the\ncrossing to Mary's house. Therefore, \\(y + 9 = 20 \\).\\n   - Solving for \\(y\\), we\nget \\(y = 20 - 9 = 1 1 \\text{ km} \\).\\n\\n4. **Calculate the total length of the\nroad from Anna's to John's house:**\\n   - The total length of the road from\nAnna's to John's house is the sum of the road from Anna's to the crossing and\nthe road from the crossing to John's house.\\n   - Therefore, the total length is \\\n(7 + 1 1 = 18 \\text{ km} \\).\\n\\nThus, the length of the road from Anna's to\nJohn's house is \\(\\boxed{E} \\).Q:  The r oad fr om Anna's to Mary's house is 16 km long. The r oad fr om Mary's to John's house is $20 km\nlong. The r oad fr om the cr ossing to Mary's house is 9 km long. How long is the r oad fr om Anna's to John's\nhouse?\nOptions: A. 7km,   B. 9km,  C. 1 1km,  D.16km,   E. 18km\nDualMindVLM\n466 TokensFigure 19. An example response of DualMindVLM to a distance reasoning VQA question.\n6\nReferences\n[1] Sandhini Agarwal, Lama Ahmad, Jason Ai, Sam Altman,\nAndy Applebaum, Edwin Arbus, Rahul K Arora, Yu Bai,\nBowen Baker, Haiming Bao, et al. gpt-oss-120b & gpt-oss-\n20b model card.arXiv preprint arXiv:2508.10925, 2025.\n[2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin\nGe, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun\nTang, et al. Qwen2. 5-vl technical report.arXiv preprint\narXiv:2502.13923, 2025.\n[3] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang\nZang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao,\nDahua Lin, et al. Are we on the right way for evaluating\nlarge vision-language models?Advances in Neural Informa-\ntion Processing Systems, 37:27056\u201327087, 2024.\n[4] Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei\nWang, and Kai-Wei Chang. Openvlthinker: An early ex-\nploration to complex vision-language reasoning via iterative\nself-improvement.arXiv preprint arXiv:2503.17352, 2025.\n[5] Yuhao Dong, Zuyan Liu, Hai-Long Sun, Jingkang Yang,\nWinston Hu, Yongming Rao, and Ziwei Liu. Insight-v: Ex-\nploring long-chain visual reasoning with multimodal large\nlanguage models. InProceedings of the Computer Vision and\nPattern Recognition Conference, pages 9062\u20139072, 2025.\n[6] Jonathan St BT Evans. Dual process theory: Perspectives\nand problems.Dual process theory 2.0, pages 137\u2013155,\n2017.\n[7] Jonathan St BT Evans and Keith E Stanovich. Dual-process\ntheories of higher cognition: Advancing the debate.Perspec-\ntives on psychological science, 8(3):223\u2013241, 2013.\n[8] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song,\nRuoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi\nWang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning\ncapability in llms via reinforcement learning.arXiv preprint\narXiv:2501.12948, 2025.\n[9] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao,\nZheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin.\nVision-r1: Incentivizing reasoning capability in multimodal\nlarge language models.arXiv preprint arXiv:2503.06749,\n2025.\n[10] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richard-\nson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander\nMadry, Alex Beutel, Alex Carney, et al. Openai o1 system\ncard.arXiv preprint arXiv:2412.16720, 2024.\n[11] Daniel Kahneman.Thinking, fast and slow. macmillan,\n2011.\n[12] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon\nSeo, Hannaneh Hajishirzi, and Ali Farhadi. A diagram is\nworth a dozen images. InEuropean conference on computer\nvision, pages 235\u2013251. Springer, 2016.\n[13] Adam Dahlgren Lindstr \u00a8om and Savitha Sam Abra-\nham. Clevr-math: A dataset for compositional lan-\nguage, visual and mathematical reasoning.arXiv preprint\narXiv:2208.05358, 2022.\n[14] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang\nZhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He,\nZiwei Liu, et al. Mmbench: Is your multi-modal model anall-around player? InEuropean conference on computer vi-\nsion, pages 216\u2013233. Springer, 2024.\n[15] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu\nPang, Chao Du, Wee Sun Lee, and Min Lin. Understanding\nr1-zero-like training: A critical perspective.arXiv preprint\narXiv:2503.20783, 2025.\n[16] Chenwei Lou, Zewei Sun, Xinnian Liang, Meng Qu,\nWei Shen, Wenqi Wang, Yuntao Li, Qingping Yang, and\nShuangzhi Wu. Adacot: Pareto-optimal adaptive chain-of-\nthought triggering via reinforcement learning.arXiv preprint\narXiv:2505.11896, 2025.\n[17] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao,\nWei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu.\nIconqa: A new benchmark for abstract diagram under-\nstanding and visual language reasoning.arXiv preprint\narXiv:2110.13214, 2021.\n[18] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei\nChang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and\nAshwin Kalyan. Learn to explain: Multimodal reasoning via\nthought chains for science question answering.Advances\nin Neural Information Processing Systems, 35:2507\u20132521,\n2022.\n[19] Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-\nChun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin\nKalyan. Dynamic prompt learning via policy gradient for\nsemi-structured mathematical reasoning.arXiv preprint\narXiv:2209.14610, 2022.\n[20] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li,\nHannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel\nGalley, and Jianfeng Gao. Mathvista: Evaluating mathemat-\nical reasoning of foundation models in visual contexts.arXiv\npreprint arXiv:2310.02255, 2023.\n[21] Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and\nEnamul Hoque. Chartqa: A benchmark for question answer-\ning about charts with visual and logical reasoning. InFind-\nings of the association for computational linguistics: ACL\n2022, pages 2263\u20132279, 2022.\n[22] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar.\nDocvqa: A dataset for vqa on document images. InProceed-\nings of the IEEE/CVF winter conference on applications of\ncomputer vision, pages 2200\u20132209, 2021.\n[23] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou,\nQuanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Jun-\njun He, Kaipeng Zhang, et al. Mm-eureka: Exploring visual\naha moment with rule-based large-scale reinforcement learn-\ning.CoRR, 2025.\n[24] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, Ilya Sutskever, et al. Language models are unsu-\npervised multitask learners.OpenAI blog, 1(8):9, 2019.\n[25] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark,\nKenneth Marino, and Roozbeh Mottaghi. A-okvqa: A\nbenchmark for visual question answering using world knowl-\nedge. InEuropean conference on computer vision, pages\n146\u2013162. Springer, 2022.\n[26] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao\nSong, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li,\nYang Wu, et al. Deepseekmath: Pushing the limits of math-\n7\nematical reasoning in open language models.arXiv preprint\narXiv:2402.03300, 2024.\n[27] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo\nMa, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao,\nQianqian Zhang, et al. Vlm-r1: A stable and generaliz-\nable r1-style large vision-language model.arXiv preprint\narXiv:2504.07615, 2025.\n[28] Yi Shen, Jian Zhang, Jieyun Huang, Shuming Shi, Wenjing\nZhang, Jiangze Yan, Ning Wang, Kai Wang, Zhaoxiang Liu,\nand Shiguo Lian. Dast: Difficulty-adaptive slow-thinking for\nlarge reasoning models.arXiv preprint arXiv:2503.04472,\n2025.\n[29] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang,\nXinlei Chen, Dhruv Batra, Devi Parikh, and Marcus\nRohrbach. Towards vqa models that can read. InProceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 8317\u20138326, 2019.\n[30] Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Minglan Lin,\nPengwei Wang, Zhongyuan Wang, and Shanghang Zhang.\nReason-rft: Reinforcement fine-tuning for visual reasoning.\narXiv preprint arXiv:2503.20752, 2025.\n[31] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu\nJiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang\nDu, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement\nlearning with llms.arXiv preprint arXiv:2501.12599, 2025.\n[32] Bingkui Tong, Jiaer Xia, Sifeng Shang, and Kaiyang Zhou.\nMeasuring epistemic humility in multimodal large language\nmodels.arXiv preprint arXiv:2509.09658, 2025.\n[33] Leandro von Werra, Younes Belkada, Lewis Tunstall, Ed-\nward Beeching, Tristan Thrush, Nathan Lambert, Shengyi\nHuang, Kashif Rasul, and Quentin Gallou \u00b4edec. Trl: Trans-\nformer reinforcement learning.https://github.com/\nhuggingface/trl, 2020.\n[34] Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu,\nFangzhen Lin, and Wenhu Chen. Vl-rethinker: Incentivizing\nself-reflection of vision-language models with reinforcement\nlearning.arXiv preprint arXiv:2504.08837, 2025.\n[35] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing\nRen, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Mea-\nsuring multimodal mathematical reasoning with math-vision\ndataset.Advances in Neural Information Processing Sys-\ntems, 37:95095\u201395169, 2024.\n[36] Xiyao Wang, Zhengyuan Yang, Chao Feng, Hongjin Lu, Lin-\njie Li, Chung-Ching Lin, Kevin Lin, Furong Huang, and Li-\njuan Wang. Sota with less: Mcts-guided sample selection\nfor data-efficient visual reasoning self-improvement.arXiv\npreprint arXiv:2504.07934, 2025.\n[37] Jiaer Xia, Bingkui Tong, Yuhang Zang, Rui Shao, and\nKaiyang Zhou. Bootstrapping grounded chain-of-thought\nin multimodal llms for data-efficient model adaptation. In\nICCV, 2025.\n[38] Jiaer Xia, Yuhang Zang, Peng Gao, Yixuan Li, and\nKaiyang Zhou. Visionary-r1: Mitigating shortcuts in vi-\nsual reasoning with reinforcement learning.arXiv preprint\narXiv:2505.14677, 2025.\n[39] Guowei Xu, Peng Jin, Ziang Wu, Hao Li, Yibing Song,\nLichao Sun, and Li Yuan. Llava-cot: Let vision languagemodels reason step-by-step. InProceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 2087\u2013\n2098, 2025.\n[40] Silei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He.\nChain of draft: Thinking faster by writing less.arXiv\npreprint arXiv:2502.18600, 2025.\n[41] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang,\nBinyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen\nHuang, Chenxu Lv, et al. Qwen3 technical report.arXiv\npreprint arXiv:2505.09388, 2025.\n[42] Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan\nDeng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun\nRao, Minfeng Zhu, et al. R1-onevision: Advancing gen-\neralized multimodal reasoning through cross-modal formal-\nization.arXiv preprint arXiv:2503.10615, 2025.\n[43] Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang,\nYibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song,\nHaocheng Feng, Li Shen, et al. Mulberry: Empowering mllm\nwith o1-like reasoning and reflection via collective monte\ncarlo tree search.arXiv preprint arXiv:2412.18319, 2024.\n[44] Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu,\nXikun Zhang, Shijian Lu, and Dacheng Tao. R1-vl: Learn-\ning to reason with multimodal large language models via\nstep-wise group relative policy optimization.arXiv preprint\narXiv:2503.12937, 2025.\n[45] Enshen Zhou, Jingkun An, Cheng Chi, Yi Han, Shanyu\nRong, Chi Zhang, Pengwei Wang, Zhongyuan Wang, Tiejun\nHuang, Lu Sheng, et al. Roborefer: Towards spatial referring\nwith reasoning in vision-language models for robotics.arXiv\npreprint arXiv:2506.04308, 2025.\n8",
      "summary": "This paper introduces DualMindVLM, a visual language model that dynamically switches between fast and slow reasoning modes based on task complexity. The proposed RL-based approach improves both reasoning efficiency and performance compared to traditional models that rely solely on lengthy reasoning chains.",
      "key_contributions": [
        "Proposes a dual-mode thinking mechanism for visual language models, inspired by human cognitive processes.",
        "Introduces a simple RL approach to automatically label and train models for fast or slow thinking based on output length.",
        "Demonstrates significant improvements in token efficiency and reasoning performance over baseline models.",
        "Achieves results comparable to state-of-the-art visual reasoning models with reduced computational costs."
      ],
      "methodology": "The method involves labeling data as requiring fast or slow thinking based on answer length, then training the model with GRPO and these labels to enable adaptive reasoning modes.",
      "results": "DualMindVLM outperforms the base model and matches state-of-the-art visual reasoning models, while maintaining high token efficiency.",
      "keywords": [
        "visual language models",
        "dual-mode reasoning",
        "reinforcement learning",
        "token efficiency",
        "cognitive modeling",
        "chain-of-thought",
        "adaptive inference"
      ],
      "status": "new",
      "notes": null,
      "created_at": "2025-11-23T20:14:12.914345",
      "needs_llm_processing": false
    },
    "7": {
      "id": "2511.16669v1",
      "title": "Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO",
      "authors": [
        {
          "name": "Junhao Cheng",
          "affiliation": null
        },
        {
          "name": "Liang Hou",
          "affiliation": null
        },
        {
          "name": "Xin Tao",
          "affiliation": null
        },
        {
          "name": "Jing Liao",
          "affiliation": null
        }
      ],
      "abstract": "While language models have become impactful in many real-world applications, video generation remains largely confined to entertainment. Motivated by video's inherent capacity to demonstrate physical-world information that is difficult to convey through language alone (e.g., imagine teaching someone to tie a tie using only text), we identify an underutilized opportunity to extend video as a new answer modality for Next-Event Prediction (NEP), formalized as Video-Next-Event Prediction (VNEP). While the established NEP task takes a video with a procedural or predictive question as input to predict the next event in text, VNEP requires dynamic video responses. This shift from telling to showing unlocks more intuitive and customized answers for procedural learning and creative exploration. However, this task remains challenging for existing models, as it demands an understanding of multimodal input, instruction-conditioned reasoning, and the generation of video with visual and semantic consistency. To address this, we introduce VANS, a model that leverages reinforcement learning to align a Vision-Language Model (VLM) with a Video Diffusion Model (VDM) for VNEP. The core of VANS is our proposed Joint-GRPO that orchestrates the VLM and VDM to function as a unit. Driven by a shared reward on their respective output, it optimizes the VLM to produce captions that are both accurate and friendly to visualize, while guiding the VDM to generate videos that are faithful to these captions and the input visual context. To enable this learning, we craft VANS-Data-100K, a dedicated dataset for the VNEP task. Experiments on procedural and predictive benchmarks demonstrate that VANS achieves state-of-the-art performance in both video event prediction and visualization. Codes are released in https://github.com/KlingTeam/VANS.",
      "arxiv_id": "2511.16669v1",
      "arxiv_url": "http://arxiv.org/abs/2511.16669v1",
      "pdf_url": "https://arxiv.org/pdf/2511.16669v1",
      "published": "2025-11-20T18:59:44Z",
      "updated": "2025-11-20T18:59:44Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "full_text": "Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO\nJunhao Cheng1\u2020Liang Hou2Xin Tao2Jing Liao1\n1City University of Hong Kong2Kling Team, Kuaishou Technology\nhttps://video-as-answer.github.io/\nHow to \u2026 ? What will \u2026 ?\nVA[Think]  The user  has already  completed  the main  part  \nof the paper  pinwheel . Pinwheels  usually  need  a handle  to \nhold\u2014often  made  from  a stick\u2026so the next  step  should  \nbe attaching  the stick  to the back  of the pinwheel . \n[/Think][Ans ]One  hand  secure  a stick\u2026[/Ans]How to proceed with making my windmill?\nHi, how to move on to the next step.\n[Think]  The input  video  shows  a girl getting  scalded . A \ncommon  trope  in exaggerated  movie  to show  a character  \nbeing  overheated  is having  them  emit  steam  from  their  \nmouth [/Think][Ans ] [Ans]The  girl lets out a sharp  gasp,  a \nwisp  of steam  escaping  her lips\u2026[/Ans]Assuming  she's  in an exaggerated  movie,  what  will  she \nlook  like the moment  after?\nWhat  will  she most  likely  do next?\nVA\nVA[Think]  From  the input  video,  I can tell that  the user  are \nwrapping  Samosa . According  to standard  procedural,  \nthe next  step  is to pinch  the edges  of the wrapper  \ntogether  to seal  it completely , and forming  a triangular  \nshape .[/Think][Ans]Pinch  the\u2026[/Ans][Think ]The video shows a woman hammering a nail into \nher bedroom wall , she most  likely  to presumably hang a \npainting  and complete the room's d\u00e9 cor. [/Think ][Ans]A \nwoman takes carefully\u2026[/Ans]VA\nFigure 1. Given an input video and a procedural (e.g. \u201chow to do it?\u201d) or predictive question (e.g. \u201cwhat will happen next?\u201d), our proposed\nVANSis capable of predicting and generating the next video event as an intuitive and customized answer.\nAbstract\nWhile language models have become impactful in many real-\n\u2020This work was conducted during the author\u2019s internship at Kling\nTeam, Kuaishou Technology.world applications, video generation remains largely con-\nfined to entertainment. Motivated by video\u2019s inherent capac-\nity to demonstrate physical-world information that is difficult\nto convey through language alone (e.g., imagine teaching\nsomeone to tie a tie using only text), we identify an underuti-\n1arXiv:2511.16669v1  [cs.CV]  20 Nov 2025\nlized opportunity to extend video as a new answer modality\nfor Next-Event Prediction (NEP), formalized asVideo-Next-\nEvent Prediction (VNEP). While the established NEP task\ntakes a video with a procedural or predictive question as\ninput to predict the next event in text, VNEP requires dy-\nnamic video responses. This shift from telling to showing\nunlocks more intuitive and customized answers for proce-\ndural learning and creative exploration. However, this task\nremains challenging for existing models, as it demands an\nunderstanding of multimodal input, instruction-conditioned\nreasoning, and the generation of video with visual and se-\nmantic consistency. To address this, we introduceVANS,\na model that leverages reinforcement learning to align a\nVision-Language Model (VLM) with a Video Diffusion Model\n(VDM) for VNEP . The core of VANS is our proposedJoint-\nGRPOthat orchestrates the VLM and VDM to function as a\nunit. Driven by a shared reward on their respective output,\nit optimizes the VLM to produce captions that are both accu-\nrate and friendly to visualize, while guiding the VDM to gen-\nerate videos that are faithful to these captions and the input\nvisual context. To enable this learning, we craftVANS-Data-\n100K, a dedicated dataset for the VNEP task. Experiments\non procedural and predictive benchmarks demonstrate that\nVANS achieves state-of-the-art performance in both video\nevent prediction and visualization. Codes are released in\nhttps://github.com/KlingTeam/VANS.\n1. Introduction\nWhile generative AI [ 21,36,38,48,50] has revolutionized\ntext-based tasks in real-life domains like healthcare [ 13] and\neducation [ 10], video generation models remain largely con-\nfined to entertainment [ 7,52]. This is a missed opportunity,\nas video encapsulates rich, dynamic information about the\nphysical world that text alone struggles to convey [28, 49].\nMotivated by this observation, we pioneerVideo-Next-\nEvent Prediction (VNEP), a novel paradigm that uses video\nas the answer modality for Next-Event Prediction (NEP).\nWhile the established NEP task takes a video with a procedu-\nral or predictive question as input to predict the next event in\ntext [ 9,16,31,43], VNEP instead generates dynamic video\nresponses. The shift from telling to showing enables VNEP\nto offer more intuitive and customized answers by leveraging\nvideo\u2019s ability to convey spatial layout, motion, and tempo-\nral ordering, while adapting the demonstration to the user\u2019s\ncurrent state. As shown in Figure 2, the video answer guides\nthe user through the remaining steps of the Windsor knot\nbased on his current tie configuration (e.g., color, orientation\nand tightness), providing clarity and personalization that a\ngeneric text-only description cannot achieve.\nHowever, VNEP introduces significant challenges that go\nbeyond mere visual continuation. Unlike existing video ex-\ntension task [ 55] which predicts frames based on spatiotem-\nVAHelp!  I'm stuck  on the next  step  of my Windsor  knot .\nHold  the wide  end,  then  pass  it from  the inside  to the outside  \nthrough  the intersection  point  below  to form  a loop\u2026\n[Think]\u2026 Pass  the wide  end from  the loop  inside  out like this:\nFigure 2. Video answer (our V ANS) versus text-only answer (Gem-\nini) on a procedural question. Video answer provides an intu-\nitive and customized response by demonstrating the action directly,\nwhile text-only answer falls short in clarity.\nporal patterns (e.g., forecasting a ball\u2019s trajectory), VNEP\nfocus on event-conditioned reasoning. It requires a model\nto first comprehend the video and question, reason about\nthe subsequent event from causal or procedural logic (e.g.,\ninferring that adding soap is needed after observing dirty\ndishes being scrubbed with water), and then generate a video\nthat is both visually coherent and semantically faithful to\nthis inferred event. A straightforward solution is to employ\na VLM for prediction followed by a VDM for generation.\nHowever, this cascaded pipeline suffers from a semantic-\nto-visual misalignment; the VLM\u2019s textual output may be\nlinguistically correct but visually unrealistic or unexecutable\nby the VDM, leading to semantically and visually divergent\nvideos [ 44]. In contrast, unified models [ 23,34,42] attempt\nto align understanding and generation within a single model\nbut face a capability trade-off, often excelling in one at the\nexpense of the other and struggling to achieve optimal per-\nformance in both simultaneously [ 45]. Consequently, neither\nparadigm alone offers a satisfactory solution.\nHowever, the limitations of these two paradigms point\ntoward a more promising direction: a tight integration of\nspecialized models that preserves their strengths while re-\nsolving their interoperability issues. To this end, we propose\nV ANS, a model that employs reinforcement learning (RL)\npost-training as an effective alignment process to fully re-\nalize the complementary strengths of VLMs (in semantic\nreasoning) and VDMs (in visual synthesis), enabling them to\noperate in concert for VNEP. Central to our approach is the\nJoint-GRPORL strategy, which orchestrates both models\nusing a joint reward derived from their respective outputs.\nThrough a two-stage optimization, Joint-GRPO trains the\nVLM to produce captions that are both accurate and friendly\n2\nProcedural Data\nadd scallions to \nthe meatadd cheese to the \nmeatmix by hand\nSplit Shot \n& Crop\n00:00 00:01 00:01 00:15 00:15 00:30 00:25\nToo short\nClip \nSelection\nQA Pair\nGeneration00:15 00:20\nVideo Caption Time Step\n00:09 00:14\nVid Cap TS Task\nVideo 1 Video 2\nVideo 1 Caption 1 Video 2 Caption 2\nTask: Simulate user instruction, model thinking and ground truth dense \ndescriptions for the output video\nInstruction: I'm making meat patties. How to finish the next step?\nThinking: From the input video, I can see that the user has added cheese to the \nminced meat. Logically, the next step should be to mix them together thoroughly. \nI'll show a person using their hands to combine the cheese and meat evenly.\nDescription: A cook in a black apron and white sleeves stands at a kitchen \ncounter. He uses his bare hand to thoroughly mix and knead the contents, \nsqueezing the meat and cheese together until they form a uniform mixture.\nSplit Shot \nby Model\n00:00 00:02 00:02 00:09\nToo short\nCaption \nGeneration\n& Clip \nSelection\nQA Pair\nGenerationVideo 2 Caption 2 Video 3 Caption 3\nTask: First evaluate logical and visual consistency, then \nsimulate the user instruction, model reasoning, and \nground truth, finally performing a self -check\nInstruction: what will appear on the phone?\nThinking: The video shows a girl raising her \nphone with a smile \u2026This suggests she is taking \na selfie,  her own face would appear on the screen.\nCheck: information leaking \u00d7 logically sound \u221a Predictive Data\n00:09 00:14 00:14 00:20\nShot-Boundary Detection Model\n00:16 00:20\nVideo\nTime Step\nA woman lifts a cell phone\u202600:02 00:07\nEmpty room\u2026\n00:09 00:14\nNext to a table\u2026\nVideo 2 Caption 2Video 1 Caption 1\nThe two videos lack \nlogical and visual \nconsistency.\nTaskTask: Select clip that best \nrepresents the video with \ndetailed caption\n2\n3\n42\n3\n4Video 1 Video 2Video 31\n 1\nData Collection \n(From Internet \n& NEP Dataset)Data \nCollection \n(From NEP \nDataset)\nTask: Select clip that best match the captionFigure 3. Data curation pipeline of V ANS-Data-100K, which processes raw videos through shot splitting, clip selection, and QA generation\nto produce high-quality data for both procedural and predictive Video-Next-Event Prediction.\nto visualize, while guiding the VDM to generate videos that\nare faithful to these captions and the input visual context.\nTo enable this learning, we construct V ANS-Data-100K,\na dedicated dataset with 100K video-question-answer triplets\nfor supervised fine-tuning (SFT) on VNEP task. From this\ncollection, we manually select 1K high-quality samples to\nsupport the subsequent RL post-training.\nExperimental results on procedural and predictive VNEP\nbenchmarks demonstrate that V ANS performs favorably\nagainst state-of-the-art (SOTA) approaches in both event\nprediction accuracy and the quality of the generated videos.\nWe make the following contributions in this work:\n\u2022We pioneerVNEP, advancing next-event reasoning from\ntextual description to dynamic video demonstration.\n\u2022We proposeV ANSand its coreJoint-GRPOstrategy,\nwhich aligns a VLM and a VDM through RL with a joint\nreward, yielding video answers that are both semantically\nfaithful and visually coherent.\n\u2022We constructV ANS-Data-100K, a dataset of 100K (input\nvideo, question, output video) triplets for training and\nevaluating models on VNEP.\n2. Related Work\nNext-Event Prediction.The established NEP task requires\npredicting a future event given a video and a procedural\nor predictive question [ 9,16,32,39]. Existing efforts pre-\ndominantly address this as a textual NEP problem, focusing\non generating descriptive answers. This line of work is\nsupported by benchmarks like VLEP [ 17], MVP [ 33] and\nV1-33K [ 39], and leverages techniques ranging from eventunderstanding [ 3] to multiscale temporal modeling and com-\nmonsense reasoning [ 4]. The recent rise of VLMs has further\npropelled this field, with methods that fine-tune on large-\nscale data [ 9] or utilize RL to elicit NEP capabilities from\npre-trained models [ 39]. However, a fundamental limitation\npersists: the answer modality remains exclusively textual.\nOur work breaks from this paradigm by introducing VNEP,\nadvancing next-event reasoning from textual description to\ndynamic video demonstration.\nGroup Relative Policy Optimization.Group Relative\nPolicy Optimization (GRPO) was initially introduced by\nDeepSeek-Math [ 29] to enhance the reasoning capabilities\nof language models and align their outputs with human pref-\nerences. Its effectiveness has led to its adoption in video un-\nderstanding and reasoning [ 5,6,11,12,18,41], where it im-\nproves model performance on complex, open-ended queries.\nBeyond understanding, GRPO has also been applied to im-\nage and video generation [ 15,25,44,46,47]. In video gen-\neration, its primary role has been to enhance the alignment\nbetween the generated video and the text prompt [ 22,46], as\nwell as to improve consistency in reference-based generation\ntasks [ 30]. While these works apply GRPO to optimize a sin-\ngle model, our Joint-GRPO coordinates two models (a VLM\nfor reasoning and a VDM for visualization) simultaneously,\nensuring they are jointly aligned for the VNEP task.\n3. V ANS-Data-100K\nExisting NEP datasets are unsuitable for direct use in VNEP\ndue to suboptimal video quality and a lack of diverse in-\nstructional questions. To bridge this gap, we construct the\n3\nV ANS-Data-100K dataset, comprising 30K procedural and\n70K predictive samples. Each sample contains an input\nvideo, a question, and a multi-modal answer (text and video),\ntailored for the VNEP task. As illustrated in Figure 3, our\ncuration pipeline involves four stages.\nRaw Data Collection.We collect data from two distinct\nsources to cover both procedural and predictive scenarios.\nFor procedural data, we source high-resolution videos from\nCOIN [ 51] and YouCook2 [ 54] to ensure clear visual demon-\nstrations of step-by-step tasks. For predictive data, we\ngather videos from general-scene datasets [ 2,39] and short-\nfilms [8], which are rich in narrative and causal dynamics.\nShot Split.Raw videos are segmented into coherent clips.\nProcedural videos are segmented using ground-truth times-\ntamps, while predictive videos employ a shot-boundary de-\ntection model. We filter out segments shorter than 3 seconds\nto ensure action completeness.\nClip Selection.We employ Gemini-2.5-Flash [ 35] as an\nautomated quality filter to identify the optimal 3-5 second\nclip. For procedural data, it selects the clip that best aligns\nwith the given caption. For predictive data, it first generates\na detailed caption for each segment, ensuring the selected\nclip is both high-quality and semantically representative.\nQA Pair Generation.Using Gemini-2.5-Flash, we generate\nQA pairs from video-caption sequences. The VLM simu-\nlates diverse questions\u2014focusing on the logical next step for\nprocedural tasks and hypothetical \u201cwhat-if\u201d scenarios for pre-\ndictive ones. It also produces a chain-of-thought reasoning\nand ground-truth answer for each question, with a self-check\nto ensure logical soundness and prevent information leakage.\nPlease refer to Appendix A for more dataset details.\n4. V ANS\nFigure 4 introduces the overall architecture of V ANS. The\ninput question is tokenized and fed into the VLM alongside\nhigh-level ViT visual features from the input video. We task\nthe VLM with performing instruction-grounded reasoning\nto generate a textual caption describing the predicted next\nevent, which serves as the semantic guide for the VDM.\nTo ensure visual consistency, the VDM is conditioned on\nboth the generated caption and low-level visual cues, which\nare extracted by tokenizing nsampled input frames using\na V AE [ 38]; these tokens are then concatenated into the\nVDM\u2019s conditioning latent space. This enables fine-grained\nvisual correspondence while generating novel scenes.\nThis design faces a fundamental limitation:the VLM\nand VDM are optimized in isolation. The VLM is trained\nfor textual accuracy but receives no feedback on whether its\ndescriptions lead to visually plausible videos. Conversely,\nthe VDM faces the challenge of coordinating two condition-\ning signals: the VLM\u2019s specific caption and the input\u2019s visual\nVision -Language ModelVideo Diffusion Model\nJoint -\nGRPO\n\u201cHi, I'd like to know \nhow to move on to the \nnext stage\u201d\nTokenizer\nViT\nEncoder\nVAE\nEncoder\nVAE Token\nCaption\nThinking\nFigure 4. Overall architecture of V ANS.\ncontext. While SFT equips the VDM with basic capabilities,\nachieving consistent performance on both semantic accuracy\nand visual fidelity requires further refinement. This discon-\nnect creates asemantic-to-visual gapwhere both models\noperate without awareness of each other\u2019s constraints and\ncapabilities. To resolve this, we introduce Joint-GRPO to\norchestrate the two models into a cohesive unit for VNEP.\n4.1. Preliminary of GRPO\nGRPO is an RL algorithm designed to align model outputs\nwith human preferences or complex objectives. The core\nconcept involves using areward functionto evaluate the\nquality of generated samples, and then adjusting the model\u2019s\npolicy to increase the likelihood of high-reward generations.\nFor each input context c, the policy model \u03c0\u03b8generates a\ngroup of Gtrajectories {oi}G\ni=1. Each trajectory receives a\nreward rireflecting its quality. GRPO computes a normal-\nized advantage \u02dcAithat measures how much better or worse\neach trajectory is compared to the group average:\n\u02dcAi=ri\u2212\u00afr\n\u03c3r,\u00afr=1\nGGX\nj=1rj, \u03c3 r=vuut1\nGGX\nj=1(rj\u2212\u00afr)2.\n(1)\nThe policy is then optimized using the GRPO objective:\nJ(\u03b8) =E\"\n1\nGGX\ni=1 \n1\nTiTi\u22121X\nt=0min\u0010\nri\nt(\u03b8)\u02dcAi,\nclip(ri\nt(\u03b8),1\u2212\u03f5,1 +\u03f5) \u02dcAi\u0011\u0011\n\u2212\u03b2D KL(\u03c0\u03b8\u2225\u03c0ref)i\n,(2)\n4\nPolicy Model (VLM)\ns1 s2 s3\nText Reward FunctionVideo + Question\nr1 r2 r3 rn\u2026\n\u2026\nVideo Reward FunctionVideo + Caption\nPolicy Model (VDM)\n\u2026\nPolicy Model (VLM)\nJoint Reward Function 1Video + Question\nReward Model (VDM)\n\u2026\nAnchor Model (VLM)\nJoint Reward Function 2Video + Question\n\u2026\n(a) GRPO (b) Joint -GRPOPolicy Model (VDM)\nsStage 2Stage 1 Stage 2\nv1 v2 v3 vn v1 v2 v3 vn v1 v2 v3 vn\nr1 r2 r3 rn\u2026 r1 r2 r3 rn\u2026 r1 r2 r3 rn\u2026s1 s2 s3\u2026Stage 1\nCompute Advantage\n Compute Advantage\n Compute Advantage\n Compute Advantagesn\nsnFigure 5. Comparison of standard GRPO with Joint-GRPO. While standard GRPO optimizes a single model at a time, our Joint-GRPO\ncoordinates their optimization under a joint reward function.\nwhere ri\nt(\u03b8) =\u03c0\u03b8(oi\nt|c)\n\u03c0\u03b8old(oi\nt|c)is the probability ratio for the i-th\ntrajectory. The clipping mechanism and KL divergence term\nensure training stability by preventing drastic policy updates.\n4.2. Joint-GRPO\nStandard GRPO, while effective for single-model alignment,\nfaces a fundamental limitation in multi-model scenarios like\nVNEP: it optimizes models in isolation. Applying it sepa-\nrately to the VLM and VDM fails to bridge the semantic-\nto-visual gap, as it does not encourage their outputs to be\nmutually reinforcing. Conversely, a one-stage joint training\nof both models is also problematic. This approach is prone\nto reward hacking and training instability, since when a gen-\nerated video is of poor quality, it is ambiguous whether the\nVLM\u2019s caption or the VDM\u2019s generation is at fault, leading\nto conflicting gradient signals.\nTo address this attribution problem and enable effective\nco-steering, we propose Joint-GRPO. This approach coor-\ndinates the VLM and VDM using a joint reward function\nvia a structured two-stage optimization process. Our key\ninsight is that the two models must be co-steered such that\nthe VLM\u2019s reasoning becomes visually grounded to guide\nthe VDM effectively, while the VDM\u2019s generation remains\nfaithful to the VLM\u2019s prediction and visual context.\nStage 1: Visualization-Friendly VLM Tuning.We first\nalign the VLM\u2019s reasoning with the VDM\u2019s generation re-\nsults. We optimize the VLM policy \u03c0VLMwhile keeping the\nVDM frozen. For an input video vinand question Q, we sam-\npleGtextual captions {si}G\ni=1from \u03c0VLM. Each caption si\nis then used by the frozen VDM to generate a correspondingvideo vi\nout. The joint reward r1for the VLM is computed as:\nr1(si, vi\nout) =\u03bb frf(si)|{z}\nformat+\u03bbt1rt1(si, sgt)|{z}\ntext fidelity+\u03bbv1rv1(vi\nout, vgt)|{z }\nvideo fidelity,\nwhere \u03bbf, \u03bbt1, \u03bbv1are weighting coefficients for each re-\nward, which are defined as follows:\n\u2022rf(si)ensures the output follows the specified instruction\nformat. A reward of 1 is given if the response adheres to\nthe \u201creason-then-answer\u201d template, and 0 otherwise.\n\u2022rt1(si, sgt)measures semantic similarity between gener-\nated and ground-truth captions using ROUGE-L [19].\n\u2022rv1(vi\nout, vgt)evaluates visual coherence of generated\nvideos with ground-truth using CLIP Similarity [27].\nThis composite reward is designed to steer the VLM\nbeyond mere linguistic correctness. Relying solely on rt1\ncan lead to captions that are linguistically correct but visually\nunrealistic or unexecutable by the VDM. Conversely, using\nonlyrv1provides a reward that is too distal and ambiguous\nto effectively guide the VLM\u2019s reasoning process. The joint\nreward guides the VLM to generate captions that are not\nmerely semantically accurate, but also visually plausible and\nactionable for the VDM. This process effectively forces the\nVLM to internalize the VDM\u2019s capabilities and constraints.\nStage 2: Context-Faithful VDM Adaptation.Building\nupon the visually-grounded captions from Stage 1, this stage\ntackles the challenge of cross-modal alignment by adapting\nthe VDM to render these captions faithfully while preserv-\ning visual consistency with the input visual context. We\noptimize the VDM policy \u03c0VDM using the VLM as a frozen\nanchor model. As shown in Figure 5, the \u2018now-improved\u2019\nVLM from Stage 1 generates a candidate anchor caption\n(samples with low semantic similarity to the ground truth\n5\nTable 1. Quantitative comparison with baseline models on Video-Next-Event Prediction.\nModel BELU@1\u2191BELU@2\u2191BELU@3\u2191BELU@4\u2191ROUGE-L\u2191 FVD\u2193CLIP-V\u2191CLIP-T\u2191\nProcedural Benchmarks\nVideo-GPT - - - - - 105.32 0.7334 0.1997\nOmni-Video 0.0948 0.0253 0.0040 0.0008 0.1075 236.38 0.6293 0.2323\nQwen-Wan 0.0981 0.0260 0.0046 0.0013 0.1530 148.75 0.6619 0.2448\nTEMPURA-Wan 0.1984 0.1063 0.0336 0.0167 0.1915 143.80 0.6738 0.2498\nGemini-Wan 0.2432 0.1077 0.0448 0.0215 0.2802 120.34 0.6898 0.2547\nQwen-FilmWeaver 0.0981 0.0260 0.0046 0.0013 0.1530 129.44 0.6831 0.2532\nTEMPURA-FilmWeaver 0.1984 0.1063 0.0336 0.0167 0.1915 120.34 0.6923 0.2562\nGemini-FilmWeaver 0.2432 0.1077 0.0448 0.0215 0.2802 110.54 0.7102 0.2773\nV ANS (SFT) 0.2524 0.1162 0.0501 0.0233 0.2812 85.34 0.7655 0.3202\nV ANS (Joint-GRPO) 0.3257 0.1834 0.1242 0.0987 0.3631 78.32 0.8021 0.3824\nPredictive Benchmarks\nVideo-GPT - - - - - 170.32 0.7031 0.2124\nOmni-Video 0.0885 0.0232 0.0035 0.0006 0.1012 252.47 0.6083 0.2218\nQwen-Wan 0.0927 0.0241 0.0040 0.0010 0.1453 158.92 0.6427 0.2349\nTEMPURA-Wan 0.1639 0.0647 0.0132 0.0105 0.2142 152.86 0.6524 0.2398\nGemini-Wan 0.1981 0.0760 0.0182 0.0112 0.2298 128.65 0.6673 0.2446\nQwen-FilmWeaver 0.0927 0.0241 0.0040 0.0010 0.1453 137.84 0.6608 0.2431\nTEMPURA-FilmWeaver 0.1839 0.0647 0.0132 0.0105 0.2142 128.32 0.6709 0.2463\nGemini-FilmWeaver 0.1981 0.0760 0.0182 0.0112 0.2298 118.27 0.6874 0.2663\nV ANS (SFT) 0.2247 0.0873 0.0206 0.0136 0.2435 94.12 0.7512 0.3038\nV ANS (Joint-GRPO) 0.2789 0.1351 0.0853 0.0694 0.3058 86.85 0.7872 0.3759\nare discarded and regenerated to ensure quality). The re-\nsulting semantically-grounded caption sanchor is then used\nto condition the VDM. We then sample Goutput videos\n{vi\nout}G\ni=1from \u03c0VDM. The VDM\u2019s core task is to generate\na novel scene by dynamically attending to and preserving\nrelevant visual elements (e.g., IDs, backgrounds) from the\ninput video\u2019s V AE tokens, as guided by the semantic content\nofs anchor . The reward functionr 2is defined as:\nr2(vi\nout, sanchor) =\u03bb v2rv2(vi\nout, vgt)|{z }\nvideo fidelity+\u03bbc2rc2(vi\nout, sanchor)| {z }\nsemantic alignment,\nwhere\u03bb v2, \u03bbc2are balancing coefficients, and:\n\u2022rv2(vi\nout, vgt)maintains visual quality and coherence with\nthe input video, using the same metric as in Stage 1.\n\u2022rc2(vi\nout, sanchor)measures semantic consistency between\nthe output video and the anchor caption using CLIPScore.\nThis joint-reward design tackles the core challenge of\ncross-modal alignment. rv2ensures the output remains vi-\nsually plausible and continuous. rc2compels the VDM to\nstrictly adhere to the event described in sanchor , preventing\nit from ignoring the caption and merely reconstructing or\nslightly altering the input video.\nThrough this two-stage optimization, the VLM and VDM\nco-evolve into a synergistic unit. The distinct, complemen-\ntary roles of each reward component, along with the training\nreward curves, are presented in B.\n5. Experiments\nWe conduct experiments to evaluate the effectiveness of our\nV ANS and to compare it with cutting-edge solutions.5.1. Settings\nBenchmarks.We construct our evaluation benchmark by\nsampling 400 procedural and 400 predictive samples from\nour dataset with source videos from established bench-\nmarks [ 2,8,51,54] to ensure reliable ground-truth text &\nvideo answers. The evaluation set is strictly separated from\nthe training data, with no video or question overlap.\nMetrics.Following [ 16], we employ BELU@1/2/3/4 [ 26]\nand ROUGE-L [ 20] for textual prediction quality. For videos,\nwe use Fr \u00b4echet Video Distance (FVD) [ 37], CLIP-Video\nScore (CLIP-V) \u2191[27], and CLIP-Text Score (CLIP-T) [ 27]\nto assess visual quality and semantic alignment.\nBaselines.As no existing methods are designed for VNEP,\nwe adapt top-performing models from related fields to es-\ntablish baselines. These include: (1) Video extension model\nVideo-GPT [ 55]; (2) Cascaded pipelines formed by com-\nbining top-tire VLMs (Gemini-2.5-Flash [ 35], Qwen-2.5-\nVL-3B [ 1], and its NEP-finetuned version TEMPURA [ 9])\nwith VDMs (Wan-2.1-1.3B [ 38], FilmWeaver [ 24]); and (3)\nUnified model Omni-Video [34].\nImplementation Details.We initialize V ANS with\nQwen2.5-VL-3B as the VLM and Wan-2.1-1.3B as the VDM.\nFor Video-GPT, we provide the input video and utilize its\nnative capability for video continuation. For V ANS and\nother baseline methods, we supply the input video and the\ncorresponding question to perform NVEP. Implementation\ndetails of our V ANS and baselines are presented in C.\n6\nShow me the next step for baked chicken Parmesan.\nThe input  video  shows  two fried  cutlets  in a pan being  \ndrizzled  with  red sauce . The most  logical  next  step  would  be \nto move  the cutlets  from  the pan to a plate  and add  the sauce .Omni -Video\nFrom  the video,  you have  already  pan-fried  the breaded  \nchicken  and you are now  drizzling  ketchup  over  it. The next  \nstep  is to bake  the chicken  in the oven  with  cheese .Gemini -FilmWeaver\n[Think]You\u2018ve  pan-fried  the breaded  chicken  until  golden  \nand are now  drizzling  it with  ketchup ...[/Think][Ans]A  \nspoon  adding  a layer  of cheese  directly  over  the ketchup\u2026Ours (SFT)\nShower  grated  cheese  over  the chicken  cutlet .Ground Truth[Think]  The video  shows  the breaded  chicken  is now  crispy,  \ngolden,  and being  glazed  with  ketchup ...[/Think][Ans]A  \nhand  sprinkle  cheese  over  the ketchup -coated  chicken\u2026Ours (Joint -GRPO) What will the sitting  man do if he is irritable?\nThe input  video  shows  a man  in white  slapping  a man  in \nblack  (the  sitting  one) . Based  on the assumption,  the man  in \nblack  would  likely  retaliate  by hitting  back .\nThe input  video  shows  a man  in white  scolding  a man  in \nblack . Given  their  potential  superior -subordinate  relationship,  \nit might  be assumed  that  the man  in black  would  talk  back .\n[Think]The  video  shows  the man  in black  being  interviewed . \nIt is assumed  that  he would  likely  grow  impatient  with  the \nprocess .[/Think][Ans]A  man  shouting  to a camera\u2026\nA man  in black  suit shouting  back  angrily .[Think]A  man  in white  (superior)  verbally  abusing  a seated  \nman  (subordinate) . Given  the hierarchy \u2026[/Think][Ans]the  \nman  in black  would  shout  back  to express  his anger\u2026\nCase 2 (Procedural Question) Case 1 (Predictive Question)Figure 6. Visual comparison on VNEP. Captions are color-coded: green (correct), red (incorrect), blue (semantically correct but visually\nunfriendly). Yellow boxes highlight key regions. Baselines often fail in event prediction or visual consistency. Our SFT model improves\nreasoning but retains errors like semantic hallucination (predicting non-existent inreview in Case 1) and action misalignment (\u201cadding cheese\u201d\nyields pouring in Case 2). Joint-GRPO addresses both issues, enhancing model capability (correctly identifying document relationships and\nmaintaining character appearance in Case 1) and fine-grained alignment (\u201csprinkle cheese\u201d matching the GT \u201cshower\u201d in Case 2).\n5.2. Main Results\nQuantitative Comparisons.Table 1 shows that V ANS per-\nforms favorably against all baselines. On procedural bench-\nmarks, V ANS (Joint-GRPO) achieves a ROUGE-L of 0.3631\nand CLIP-V of 0.8021, outperforming the strongest cascaded\nbaseline (Gemini-FilmWeaver at 0.2802 and 0.7102) and\nunified model (Omni-Video at 0.1075 and 0.6293). More\nimportantly, Joint-GRPO brings a significant gain over theSFT version (e.g., ROUGE-L from 0.2812 to 0.3631 and\nCLIP-V from 0.7655 to 0.8021), demonstrating the effec-\ntiveness of our Joint-GRPO strategy. The Video Extension\nmodel Video-GPT yields the lowest CLIP-T (0.1997), as it\ngenerates frames without event reasoning. Please refer to D\nfor additional results.\nQualitative Comparisons.As shown in Figure 6, baseline\nmodels frequently produce errors in either event prediction\n7\nTable 2. Quantitative results of ablation study.\nModel BELU@1\u2191BELU@2\u2191BELU@3\u2191BELU@4\u2191ROUGE-L\u2191 FVD\u2193CLIP-V\u2191CLIP-T\u2191\nProcedural Benchmarks\nSFT 0.2524 0.1162 0.0501 0.0233 0.2812 85.34 0.7655 0.3202\nGRPO (VLM) 0.2831 0.1498 0.0987 0.0698 0.3190 83.88 0.7798 0.3224\nGRPO (VDM) 0.2524 0.1162 0.0501 0.0233 0.2812 84.76 0.7671 0.3013\nGRPO (VLM+VDM) 0.2831 0.1498 0.0987 0.0698 0.2894 83.14 0.7703 0.3398\nJoint-GRPO Stage 10.3257 0.1834 0.1242 0.0987 0.3631 80.23 0.7803 0.3521\nJoint-GRPO Stage 1 (w/or t1) 0.3176 0.1623 0.1123 0.0889 0.3498 83.31 0.7762 0.3454\nJoint-GRPO Stage 1 (w/or v1) 0.3252 0.1828 0.1240 0.0978 0.3625 82.34 0.7668 0.3403\nJoint-GRPO Stage 1 + 2 (w/or c2)0.3257 0.1834 0.1242 0.0987 0.3631 78.55 0.7921 0.3673\nJoint-GRPO Stage 1 + 2 (w/or v2)0.3257 0.1834 0.1242 0.0987 0.3631 79.76 0.7887 0.3806\nJoint-GRPO Stage 1 + 2 (Ours) 0.3257 0.1834 0.1242 0.0987 0.3631 78.32 0.8021 0.3824\nJoint-GRPO (all-in-one) 0.3012 0.1773 0.1003 0.0632 0.3577 81.01 0.7800 0.3423\nPredictive Benchmarks\nSFT 0.2247 0.0873 0.0206 0.0136 0.2435 94.12 0.7512 0.3038\nGRPO (VLM) 0.2521 0.1124 0.0412 0.0289 0.2758 92.54 0.7643 0.3218\nGRPO (VDM) 0.2247 0.0873 0.0206 0.0136 0.2435 93.45 0.7525 0.3051\nGRPO (VLM+VDM) 0.2521 0.1124 0.0412 0.0289 0.2552 91.83 0.7558 0.3342\nJoint-GRPO Stage 10.2789 0.1351 0.0853 0.0694 0.3058 89.92 0.7654 0.3462\nJoint-GRPO Stage 1 (w/or t1) 0.2718 0.1203 0.0765 0.0627 0.2934 92.11 0.7613 0.3396\nJoint-GRPO Stage 1 (w/or v1) 0.2785 0.1346 0.0851 0.0685 0.3052 91.15 0.7529 0.3345\nJoint-GRPO Stage 1 + 2 (w/or c2)0.2789 0.1351 0.0853 0.0694 0.3058 88.36 0.7772 0.3612\nJoint-GRPO Stage 1 + 2 (w/or v2)0.2789 0.1351 0.0853 0.0694 0.3058 89.57 0.7738 0.3642\nJoint-GRPO Stage 1 + 2 (Ours) 0.2789 0.1351 0.0853 0.0694 0.3058 86.85 0.7872 0.3759\nJoint-GRPO (all-in-one) 0.2574 0.1308 0.0684 0.0452 0.3012 90.82 0.7651 0.3365\nQuestion: What will she do after 15 minutes Question: Show me the next step for baked chicken Parmesan\nFigure 7. Visualization Results of ablation studies. Key regions are highlighted with yellow boxes: the left example shows degradation in\nthe \u201cmask removal\u201d action completion without rt1; the right example illustrates the loss of visual consistency without rv2and semantic\nalignment (leading to static frames) withoutr c2.\nor visual consistency. For instance, Omni-Video misinter-\nprets a quarrel as a fight and generates characters that deviate\nfrom the input. Our V ANS after SFT demonstrates improved\nreasoning but reveals two key limitations: individual com-\nponent errors, such as the VLM hallucinating non-existent\ntext like \u201cinreview\u201d in Case 1, and a semantic-visual mis-\nalignment where the instruction \u201cadding cheese\u201d results in\na pouring action instead of the ground-truth \u201cshowering\u201d in\nCase 2. V ANS with Joint-GRPO enhances the capability ofeach component and achieves semantic-visual alignment, as\nevidenced by the precise caption \u201csprinkle cheese\u201d and its\nfaithful visualization that matches the \u201cshower\u201d action.\n5.3. Ablation Study\nWe conduct ablation studies to validate the design of Joint-\nGRPO, with results in Table 2 and Figure 7.\nJoint vs. Isolated Optimization.Joint-GRPO outperforms\nvariants where GRPO is applied solely to the VLM or VDM,\n8\nor where their individually optimized versions are simply\ncascaded. This confirms the necessity of joint optimization\nfor coherent caption-video generation, where the VLM and\nVDM are co-adapted to bridge the semantic-to-visual gap.\nEffect of Staged Training.The two-stage design proves\ncritical: using only Stage 1 often produces captions and\nvideos that deviate semantically, while an all-in-one variant\nsuffers from optimization instability due to reward ambigu-\nity\u2014it becomes unclear whether a poor reward stems from\nthe VLM\u2019s caption or the VDM\u2019s video generation.\nReward Component Analysis.Further ablation tests val-\nidate the contribution of each reward component. In Stage\n1, removing the text fidelity reward rt1reduces caption ac-\ncuracy (e.g., failing to predict \u201cremoving the mask\u201d), while\nremoving the video fidelity reward rv1harms visual consis-\ntency. In Stage 2, removing the semantic alignment reward\nrc2causes reward hacking with static frames, and remov-\ning the video fidelity reward rv2reduces output coherence.\nThese findings validate our complete design with staged\noptimization and balanced reward components.\n6. Conclusion\nThis work pioneers Video-Next-Event Prediction (VNEP), a\nnovel task that advances next-event reasoning from textual\ndescription to dynamic video demonstration. To address its\nunique challenges, we introduce V ANS, which synergizes\na VLM and a VDM through Joint-GRPO\u2014a two-stage RL\nstrategy coordinating both models under a joint reward. We\nconstruct V ANS-Data-100K dataset to provide the essential\ntraining and evaluation foundation for this task. Experiments\non established benchmarks demonstrate that V ANS achieves\nSOTA performance in both event prediction accuracy and\nvideo generation quality.\nAcknowledgement\nThis work was supported by Kuaishou Technology.\n9\nVideo-as-Answer: Predict and Generate Next Video Event with Joint-GRPO\nSupplementary Material\nThis Appendix is organized as follows:\n\u2022 Section A provides the dataset details.\n\u2022Section B offers an intuitive illustration of the Joint-GRPO\nreward design, along with training details.\n\u2022Section C describes the implementation details of models.\n\u2022 Section D reports additional experimental results.\nA. Details of V ANS-Data-100K\nTable 3 demonstrates the composition and key statistics of\nour V ANS-Data-100K dataset. It contains a total of 100K\nsamples, with 30K dedicated to procedural tasks and 70K to\npredictive scenarios. These are sourced from a diverse set of\npublicly available video datasets and Internet to ensure broad\ncoverage of real-world dynamics and instructional content.\nIn terms of video characteristics, the input videos have\nan average duration of 9.43 seconds, providing sufficient\ncontext for event reasoning. The corresponding target videos,\nwhich depict the predicted next event, average 3.76 seconds\nin length, ensuring concise and focused demonstrations.\nB. Details of Joint-GRPO\nB.1. Reward Design\nFigure 8 provides an intuitive illustration of how the individ-\nual reward components work in concert within our two-stage\ntraining process of Joint-GRPO.\nIn Stage 1 (VLM Tuning), we examine the role of the text\nfidelity reward ( rt1) and the video fidelity reward ( rv1). For\nthe provided example, if only rt1is used, Sample 2 receives\na high score comparable to the Ground Truth (GT), as both\ncaptions correctly describe the action. However, Sample\n2 exhibits poor visual consistency. Conversely, if only rv1\nis used, both Sample 1 and Sample 2 receive similarly low\nscores, failing to reflect that Sample 1 is semantically worse\ndue to an incorrect action prediction. Only when both rt1\nandrv1are combined does the composite reward correctly\nrank the samples, successfully identifying the GT as the best.\nIn Stage 2 (VDM Adaptation), we analyze the video\nfidelity reward ( rv2) and the semantic consistency reward\n(rc2). Relying solely on rv2results in Sample 1 receiving\na score similar to the GT, even though Sample 1 depicts\nan incorrect semantic action (it should show two people\npointing guns at each other). Using only rc2causes Sample\n2 to be scored similarly to the GT, despite its poor visual\nconsistency. The joint reward effectively combines these\nsignals to prioritize samples that are correct in both semantics\nand visual quality.\nThe final combined reward in each stage is a sum of theTable 3. Statistics of V ANS-Data-100K dataset.\nComponent Size/Duration\nData Composition\nProcedural (Total: 30K)\nYouCook2 [54] 9K\nCOIN [51] 21K\nPredictive (Total: 70K)\nVideo-Holmes [8] 10K\nActivityNet [2] 20K\nV1-33K [39] 10K\nYouTube Videos 30K\nVideo Duration (Avg.)\nInput Video 9.43s\nTarget Video 3.76s\nnormalized individual rewards. All weighting coefficients\n(\u03bb) are set to 1, assigning equal importance to each objec-\ntive. This design ensures a balanced optimization towards\ncaptions that are both semantically accurate and visually\nplausible (Stage 1), and videos that are both high-quality and\nsemantically faithful (Stage 2).\nB.2. Training Process\nFigure 9 illustrates the training dynamics of Joint-GRPO. In\nStage 1 (VLM Tuning), the format reward ( rf) in Figure 9(a)\nquickly saturates, indicating rapid adoption of the instruction\ntemplate. Both text fidelity ( rt1, Figure 9b) and video fidelity\n(rv1, Figure 9c) rewards show progressive improvement, re-\nflecting the VLM\u2019s learning to generate captions that are\nboth semantically accurate and visually plausible. The com-\nbined reward (Figure 9d) stabilizes after approximately 600\nsteps, demonstrating effective optimization. Concurrently,\nthe increasing thinking length (Figure 9e) indicates more\ndetailed reasoning chains.\nIn Stage 2 (VDM Adaptation), both video fidelity ( rv2,\nFigure 9f) and semantic alignment ( rc2, Figure 9g) rewards\nimprove consistently, with convergence occurring after about\n1000 steps. This demonstrates the VDM\u2019s successful adap-\ntation to generate videos that preserve visual consistency\nwhile faithfully rendering the semantically-grounded cap-\ntions from Stage 1. The total reward (Figure 9h) also reaches\na stable level, confirming effective cross-modal alignment.\nCollectively, these training curves validate the effective-\nness of our Joint-GRPO design, demonstrating coordinated\nimprovement across both stages.\n1\nPolicy Model (VLM)\nReward Model (VDM)\nQ: How to continue \nwith my slime?[Think]  From  the input  video,  I can \nsee that  the user  is preparing  to \npour  the gel\u2026[Ans]  A hand  pours  \nwhite  gel from  a glass  into  the oval  \nbasin .[/Ans]\n[Think] The white  gel has been  \nadded,  along  with  red and blue  \npigment\u2026[/Thinnk ][Ans] A hand  \nstir the blue  and red pigment  in the \nbasin  with  a spoon [/Ans]\n[Think] The white  gel has been  \nadded,  along  with  red and blue  \npigment\u2026[/Thinnk ][Ans] A hand  \nstir the red and blue  pigment  in the \noval  basin  with  a spoon [/Ans]r2r1\nrgt\nFormat reward\nText reward\nVideo reward\nQ: What will the man \ndo to protect himself?r1\nConsistency \nreward\n[Think]  Based  on the input  video,  \nthe man  on the left is being  \nthreatened  with  a gun. In such  a \nsituation,  he might  also  draw  a \ngun to defend  \nhimself[/Think][Ans]  Two  men  \nare in a standoff,  aiming  their  \nguns  at one  another . [/Ans]\nPolicy  Model (VDM)\nrgtStage 1 Stage 2\nAnchor  Model (VLM)\nr2\nSample 1 Sample 2 Ground Truth Sample 1 Sample 2 Ground TruthFigure 8. Illustration of our Joint-GRPO reward design. Top: For a Stage-1 case, we simulate three reasoning samples during GRPO training.\nThe text-only reward fails to penalize Sample 2\u2019s visual inconsistency, while the video-only reward fails to penalize Sample 1\u2019s semantic\nerror. Bottom: For a Stage-2 case, the video-only reward fails to penalize Sample 1\u2019s semantic inaccuracy, while the consistency-only reward\nfails to penalize Sample 2\u2019s visual inconsistency.\n(a) Stage 1: format reward rf (b) Stage 1: text fidelity reward rt1 (c) Stage 1: video fidelity reward rv1 (d) Stage 1: total reward\n(e) Stage 1: thinking length (f) Stage 2: video fidelity reward rv2 (g) Stage 2: alignment reward rc2 (h) Stage 2: total reward\nFigure 9. Training curves of Joint-GRPO: (a) format reward ( rf) in Stage 1; (b) text fidelity reward ( rt1) in Stage 1; (c) video fidelity reward\n(rv1) in Stage 1; (d) total reward in Stage 1; (e) thinking length evolution in Stage 1; (f) video fidelity reward ( rv2) in Stage 2; (g) semantic\nalignment reward (r c2) in Stage 2; (h) total reward in Stage 2.\n2\nC. Implementation Details\nC.1. Training of V ANS\nWe initialize V ANS with Qwen2.5-VL-3B as the VLM and\nWan-2.1-1.3B as the VDM. The VDM is configured to use\nn= 6reference frames.\nIn the SFT stage, the VLM is trained for 10K steps using\nLoRA [ 14] (rank=8, alpha=32) with a learning rate of 5\u00d7\n10\u22125, while the VDM is fully fine-tuned for 20K steps across\nall DiT blocks with the same learning rate of5\u00d710\u22125.\nFor Joint-GRPO post-training, Stage 1 is optimized for\n800 steps with a learning rate of 5\u00d710\u22125. In Stage 2, to\nensure the quality of anchor captions, we filter out those with\nROUGE-L scores below 0.6 before proceeding with VDM\nadaptation. Stage 2 is then trained for 1K steps with the same\nlearning rate of 5\u00d710\u22125. We equip the VLM with LoRA\n(rank=8, alpha=32). For the VDM, we adopt the method\nfrom [ 22] to convert a deterministic Ordinary Differential\nEquation (ODE) into an equivalent Stochastic Differential\nEquation (SDE) to enable GRPO training. We set the KL\ncoefficient \u03b2= 0.004 , clip range to 1\u00d710\u22123, and sample\ngroup size to 8 per prompt.\nC.2. Evaluation\nEvaluation Protocol.All methods are evaluated under a\nunified protocol. For Video-GPT, we provide only the in-\nput video and utilize its native video continuation capability.\nFor V ANS and other baseline methods, we provide the in-\nput video along with the corresponding question and the\nfollowing system prompt:\nYou will be given a video. Your task is to predict the\nnext event based on the input video and the user\u2019s\ninstructions. Please begin by providing your de-\ntailed reasoning between the [Think][/Think] tags,\nfollowed by your detailed description of the next\nevent within the [Ans][/Ans] tags.\nInput Adaptation.To accommodate different model archi-\ntectures, we adapt the video input accordingly: for models\nthat can directly process video input (e.g., Gemini), we pro-\nvide the original video; for other models (e.g., Qwen), we\nset the input video fps = 1 for their video processors.\nOutput Specification.All methods are required to generate\na video answer with a resolution of 352\u00d7640 and a length\nof 33 frames to ensure consistent and fair comparison.\nMetric Computation.The CLIP-Score for video consis-\ntency (CLIP-V) and semantic consistency (CLIP-T) is com-\nputed using a ViT-B/32 model. Specifically, each frame\nof the generated video is compared with the corresponding\nframe in the ground-truth video, and the scores are averaged\nacross all frames.Table 4. Results on procedural VNEP. The comparison with fine-\ntuned baselines (*) shows that our architectural design, rather than\ndata advantage, is the primary source of improvement.\nModel BELU@4\u2191ROUGE-L\u2191FVD\u2193CLIP-V\u2191CLIP-T\u2191\nQwen-Wan 0.0013 0.1530 148.75 0.6619 0.2448\nQwen*-Wan 0.0233 0.2812 140.32 0.6790 0.2466\nQwen*-Wan* 0.0233 0.2812 140.07 0.6795 0.2470\nGemini-Wan 0.0215 0.2802 120.34 0.6898 0.2547\nV ANS (SFT) 0.0233 0.2812 85.34 0.7655 0.3202\nV ANS (Joint-GRPO) 0.0987 0.3631 78.32 0.8021 0.3824\nD. Additional Results\nD.1. Inference Time\nThe inference time of V ANS is comparable to other cascaded\npipelines, requiring approximately 4 seconds for caption\ngeneration and 35 seconds for video generation using the\nofficial V AN library. In contrast, unified models exhibit\nlonger inference times: Omni-Video requires approximately\n50 seconds, while VideoGPT needs about 60 seconds for\ncomplete generation.\nD.2. Comparison with Fine-tuned Baseline\nTo analyze the source of performance improvements in\nV ANS, we compare it with fine-tuned baselines. As shown\nin Table 4, the results indicate three main observations: data\nquality provides a foundation, architectural modification\ncontributes to noticeable gains, and Joint-GRPO provides\nthe decisive enhancement that pushes performance to the\nstate-of-the-art level.\nData Quality as the Foundation.When fine-tuned on\nour V ANS-Data-100K for 10K steps (denoted as Qwen*),\nthe model achieves reasoning capability competitive with\nGemini-2.5-Flash in a zero-shot setting (ROUGE-L: 0.2812\nvs. 0.2802). This confirms that our high-quality dataset\nenables smaller models to learn sophisticated reasoning.\nArchitectural Modification Contributes to Gains.Fine-\ntuning both components of the Qwen-Wan pipeline (denoted\nas Qwen*-Wan*) yields limited video metric improvements\nover the base fine-tuned VLM (denoted as Qwen*-Wan).\nIn contrast, V ANS (SFT) with the same text input achieves\nbetter video results: FVD decreases from 140.07 to 85.34\nand CLIP-V increases from 0.6795 to 0.7655, suggesting the\nproposed V AE reference feature aids visual consistency.\nJoint-GRPO Delivers the Decisive Enhancement.The\nmost striking improvement comes from Joint-GRPO, which\nelevates V ANS to unprecedented performance levels across\nall metrics. Compared to V ANS (SFT), Joint-GRPO boosts\nROUGE-L from 0.2812 to 0.3631 (29.1% relative improve-\nment) and CLIP-T from 0.3202 to 0.3824 (19.4% relative\nimprovement), while further reducing FVD to 78.32. These\nresults unequivocally demonstrate that Joint-GRPO is the\n3\nPrompt: Leave the bananas for a weekGemini -WanI2V Omni -Video OursPrompt: Man picking up unused paper\nFigure 10. Visual comparison results on UI2V-Bench.\nInput Video\nQ1: What is her reaction if she gets burned?\nQ2: What is her reaction if she gets burned in an exaggerated movie?\nQ3: Show her reaction if she eats something spicy in an exaggerated movie.\nFigure 11. Multi-Future Prediction Results.\nmost critical component for achieving state-of-the-art perfor-\nmance, effectively aligning both textual and visual outputs\nwith human preferences.\nD.3. Generalization\nMulti-Future Prediction.The established NEP task typ-\nically assumes a single, causal progression from the input\ncontext. In contrast, our V ANS demonstrates a key gener-\nalization capability: multi-future prediction. This allows\nthe model to generate semantically distinct and contextuallyappropriate video answers based on different hypothetical\nquestions applied to the same input video, moving beyond\ndeterministic continuation.\nAs shown in Figure 11, when presented with a scene of a\nwoman reacting to a hot object, V ANS can generate funda-\nmentally different yet plausible outcomes conditioned on the\nscenario: in a realistic everyday context, it predicts a natural\nreaction of \u201ccoughing\u201d; whereas in a stylized cinematic con-\ntext, it visualizes a dramatic effect of \u201csmoke exhaling from\nthe mouth\u201d. This flexibility stems from our model\u2019s ability\nto ground its predictions in both the visual evidence and the\ndiverse textual hypotheses provided, effectively exploring\nmultiple potential futures from a single starting point.\nReasoning Image-to-Video Generation.V ANS general-\nizes effectively to the reasoning image-to-video (I2V) task\nby treating a single image as a static video clip. This gen-\neralization capability is attributed to the model\u2019s training\non mixed datasets including Koala-36M [ 40] for I2V tasks.\nFigure 10 demonstrates examples from UI2V-Bench [ 53],\nwhen given an image of a banana and the instruction \u201dleave\nthe banana for a week,\u201d our model accurately predicts the\ntemporal evolution, generating a video where the banana\nskin darkens. In contrast, other strong baselines struggle to\ncapture this causal-physical transformation correctly. This\ndemonstrates the robustness of our approach in understand-\ning static visual contexts and reasoning about their potential\ndynamic futures.\nD.4. Human Evaluation\nTo complement automatic metrics, we conduct a human eval-\nuation to assess the subjective quality of generated video\nanswers. We recruit 30 evaluators (mean age = 25 years;\nall hold at least a bachelor\u2019s degree, including 20 postgrad-\nuate/PhD students and 10 full-time professionals). Each\n4\nTable 5. Human evaluation results (scale: 1-5). Our V ANS with\nJoint-GRPO achieves the highest scores across all criteria.\nModel Semantic Correctness Visual Consistency Overall\nVideo-GPT 1.5 3.6 1.5\nOmni-Video 2.1 3.2 2.2\nGemini-FilmWeaver 3.9 3.1 3.5\nV ANS (SFT) 3.8 3.9 3.7\nV ANS (Joint-GRPO)4.7 4.6 4.8\nevaluator is presented with 20 randomly selected examples\n(10 procedural and 10 predictive) and rates the results on\nthree dimensions: semantic correctness, visual consistency,\nand overall satisfaction.\nThe results in Table 5 reveal that V ANS (SFT) achieves se-\nmantic correctness comparable to the strong baseline Gemini-\nFilmWeaver, while demonstrating superior visual consis-\ntency. Furthermore, V ANS with Joint-GRPO receives the\nhighest ratings across all three criteria, indicating that our full\napproach yields video answers that are not only semantically\nand visually accurate but also subjectively more satisfactory\nto human observers.\nD.5. Video Results\nAll video results corresponding to the figures in this paper,\nalong with additional examples, are provided in our project\npage:https://video-as-answer.github.io.\nReferences\n[1]Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xi-\naodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al.\nQwen technical report. arXiv preprint arXiv:2309.16609 ,\n2023. 6\n[2]Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and\nJuan Carlos Niebles. Activitynet: A large-scale video bench-\nmark for human activity understanding. In Proceedings ofthe\nieee conference oncomputer vision andpattern recognition ,\npages 961\u2013970, 2015. 4, 6, 1\n[3]Yu-Wei Chao, Zhan Wang, Yugeng He, Jiaxuan Wang, and\nJia Deng. Hico: A benchmark for recognizing human-\nobject interactions in images. In Proceedings oftheIEEE\ninternational conference oncomputer vision , pages 1017\u2013\n1025, 2015. 3\n[4]Brian Chen, Xudong Lin, Christopher Thomas, Manling Li,\nShoya Yoshida, Lovish Chum, Heng Ji, and Shih-Fu Chang.\nJoint multimedia event extraction from video and article.\narXiv preprint arXiv:2109.12776, 2021. 3\n[5]Yi Chen, Yuying Ge, Rui Wang, Yixiao Ge, Junhao Cheng,\nYing Shan, and Xihui Liu. Grpo-care: Consistency-aware rein-\nforcement learning for multimodal reasoning. arXiv preprint\narXiv:2506.16141, 2025. 3\n[6]Yi Chen, Yuying Ge, Rui Wang, Yixiao Ge, Lu Qiu, Ying\nShan, and Xihui Liu. Exploring the effect of reinforcement\nlearning on video understanding: Insights from seed-bench-r1.\narXiv preprint arXiv:2503.24376, 2025. 3[7]Junhao Cheng, Yuying Ge, Yixiao Ge, Jing Liao, and Ying\nShan. Animegamer: Infinite anime life simulation with next\ngame state prediction. In Proceedings oftheIEEE/CVF\nInternational Conference onComputer Vision , pages 10875\u2013\n10885, 2025. 2\n[8]Junhao Cheng, Yuying Ge, Teng Wang, Yixiao Ge, Jing\nLiao, and Ying Shan. Video-holmes: Can mllm think\nlike holmes for complex video reasoning? arXiv preprint\narXiv:2505.21374, 2025. 4, 6, 1\n[9]Jen-Hao Cheng, Vivian Wang, Huayu Wang, Huapeng Zhou,\nYi-Hao Peng, Hou-I Liu, Hsiang-Wei Huang, Kuang-Ming\nChen, Cheng-Yen Yang, Wenhao Chai, et al. Tempura: Tem-\nporal event masked prediction and understanding for reason-\ning in action. arXiv preprint arXiv:2505.01583 , 2025. 2, 3,\n6\n[10] Zhendong Chu, Shen Wang, Jian Xie, Tinghui Zhu, Yibo\nYan, Jinheng Ye, Aoxiao Zhong, Xuming Hu, Jing Liang,\nPhilip S Yu, et al. Llm agents for education: Advances and\napplications. arXiv preprint arXiv:2503.11733, 2025. 2\n[11] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo,\nYibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang,\nBenyou Wang, and Xiangyu Yue. Video-r1: Reinforcing\nvideo reasoning in mllms. arXiv preprint arXiv:2503.21776 ,\n2025. 3\n[12] Yuying Ge, Yixiao Ge, Chen Li, Teng Wang, Junfu Pu, Yizhuo\nLi, Lu Qiu, Jin Ma, Lisheng Duan, Xinyu Zuo, et al. Arc-\nhunyuan-video-7b: Structured video comprehension of real-\nworld shorts. arXiv preprint arXiv:2507.20939, 2025. 3\n[13] Sagar Goyal, Eti Rastogi, Sree Prasanna Rajagopal, Dong\nYuan, Fen Zhao, Jai Chintagunta, Gautam Naik, and Jeff Ward.\nHealai: A healthcare llm for effective medical documentation.\nInProceedings ofthe17th ACM International Conference on\nWeb Search andData Mining, pages 1167\u20131168, 2024. 2\n[14] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,\nYuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora:\nLow-rank adaptation of large language models. ICLR , 1(2):3,\n2022. 3\n[15] Dongzhi Jiang, Ziyu Guo, Renrui Zhang, Zhuofan Zong,\nHao Li, Le Zhuo, Shilin Yan, Pheng-Ann Heng, and Hong-\nsheng Li. T2i-r1: Reinforcing image generation with col-\nlaborative semantic-level and token-level cot. arXiv preprint\narXiv:2505.00703, 2025. 3\n[16] Chenghang Lai, Weifeng Ge, and Xiangyang Xue. Cross-\nmodal complementary learning and template-based reason-\ning chains for future event prediction in videos. IEEE\nTransactions onMultimedia, 2025. 2, 3, 6\n[17] Jie Lei, Licheng Yu, Tamara L Berg, and Mohit Bansal. What\nis more likely to happen next? video-and-language future\nevent prediction. arXiv preprint arXiv:2010.07999, 2020. 3\n[18] Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng,\nYinan He, Yali Wang, Yu Qiao, Yi Wang, and Limin Wang.\nVideochat-r1: Enhancing spatio-temporal perception via re-\ninforcement fine-tuning. arXiv preprint arXiv:2504.06958 ,\n2025. 3\n[19] Chin-Yew Lin. Rouge: A package for automatic evaluation\nof summaries. In Text summarization branches out, pages\n74\u201381, 2004. 5\n5\n[20] Chin-Yew Lin and Franz Josef Och. Automatic evaluation\nof machine translation quality using longest common sub-\nsequence and skip-bigram statistics. In Proceedings ofthe\n42nd annual meeting oftheassociation forcomputational\nlinguistics (ACL-04), pages 605\u2013612, 2004. 6\n[21] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao\nWu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu\nZhang, Chong Ruan, et al. Deepseek-v3 technical report.\narXiv preprint arXiv:2412.19437, 2024. 2\n[22] Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu,\nXintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang.\nFlow-grpo: Training flow matching models via online rl.\narXiv preprint arXiv:2505.05470, 2025. 3\n[23] Jiabin Luo, Junhui Lin, Zeyu Zhang, Biao Wu, Meng Fang,\nLing Chen, and Hao Tang. Univid: The open-source unified\nvideo model. arXiv preprint arXiv:2509.24200, 2025. 2\n[24] Xiangyang Luo, Qingyu Li, Xiaokun Liu, Wenyu Qin, Miao\nYang, Pengfei Wan, Di Zhang, Kun Gai, and Shao-Lun\nHuang. Filmweaver: Weaving consistent multi-shot videos\nwith cache-guided autoregressive diffusion. AAAI, 2026. 6\n[25] Yapeng Mi, Hengli Li, Yanpeng Zhao, Chenxi Li, Huimin Wu,\nXiaojian Ma, Song-Chun Zhu, Ying Nian Wu, and Qing Li.\nMilr: Improving multimodal image generation via test-time\nlatent reasoning. arXiv preprint arXiv:2509.22761, 2025. 3\n[26] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing\nZhu. Bleu: a method for automatic evaluation of machine\ntranslation. In Proceedings ofthe40th annual meeting ofthe\nAssociation forComputational Linguistics , pages 311\u2013318,\n2002. 6\n[27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference onmachine learning , pages\n8748\u20138763. PmLR, 2021. 5, 6\n[28] John R Searle. Minds, brains, and programs. Behavioral and\nbrain sciences, 3(3):417\u2013424, 1980. 2\n[29] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao\nSong, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li,\nYang Wu, et al. Deepseekmath: Pushing the limits of mathe-\nmatical reasoning in open language models. arXiv preprint\narXiv:2402.03300, 2024. 3\n[30] Liao Shen, Wentao Jiang, Yiran Zhu, Tiezheng Ge, Zhiguo\nCao, and Bo Zheng. Identity-preserving image-to-video\ngeneration via reward-guided optimization. arXiv preprint\narXiv:2510.14255, 2025. 3\n[31] Tom \u00b4a\u02c7s Sou \u02c7cek, Prajwal Gatti, Michael Wray, Ivan Laptev,\nDima Damen, and Josef Sivic. Showhowto: Generating scene-\nconditioned step-by-step visual instructions. In Proceedings\noftheComputer Vision andPattern Recognition Conference ,\npages 27435\u201327445, 2025. 2\n[32] Qile Su, Shoutai Zhu, Shuai Zhang, Baoyu Liang, and Chao\nTong. Eventformer: A node-graph hierarchical attention\ntransformer for action-centric video event prediction. In\nProceedings ofthe33rd ACM International Conference on\nMultimedia, pages 4698\u20134707, 2025. 3[33] Reuben Tan, Matthias De Lange, Michael Iuzzolino, Bryan A\nPlummer, Kate Saenko, Karl Ridgeway, and Lorenzo Tor-\nresani. Multiscale video pretraining for long-term activity\nforecasting. arXiv preprint arXiv:2307.12854, 2023. 3\n[34] Zhiyu Tan, Hao Yang, Luozheng Qin, Jia Gong, Meng-\nping Yang, and Hao Li. Omni-video: Democratizing uni-\nfied video understanding and generation. arXiv preprint\narXiv:2507.06119, 2025. 2, 6\n[35] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-\nBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk,\nAndrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a\nfamily of highly capable multimodal models. arXiv preprint\narXiv:2312.11805, 2023. 4, 6\n[36] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell,\nLibin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent,\nZhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking mul-\ntimodal understanding across millions of tokens of context.\narXiv preprint arXiv:2403.05530, 2024. 2\n[37] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach,\nRapha \u00a8el Marinier, Marcin Michalski, and Sylvain Gelly. Fvd:\nA new metric for video generation. 2019. 6\n[38] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao,\nChen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao\nYang, et al. Wan: Open and advanced large-scale video\ngenerative models. arXiv preprint arXiv:2503.20314 , 2025.\n2, 4, 6\n[39] Haonan Wang, Hongfu Liu, Xiangyan Liu, Chao Du,\nKenji Kawaguchi, Ye Wang, and Tianyu Pang. Fostering\nvideo reasoning via next-event prediction. arXiv preprint\narXiv:2505.22457, 2025. 3, 4, 1\n[40] Qiuheng Wang, Yukai Shi, Jiarong Ou, Rui Chen, Ke Lin,\nJiahao Wang, Boyuan Jiang, Haotian Yang, Mingwu Zheng,\nXin Tao, Fei Yang, Pengfei Wan, and Di Zhang. Koala-36m:\nA large-scale video dataset improving consistency between\nfine-grained conditions and video content, 2024. 4\n[41] Ziyang Wang, Jaehong Yoon, Shoubin Yu, Md Mohaiminul\nIslam, Gedas Bertasius, and Mohit Bansal. Video-rts: Re-\nthinking reinforcement learning and test-time scaling for\nefficient and enhanced video reasoning. arXiv preprint\narXiv:2507.06485, 2025. 3\n[42] Cong Wei, Quande Liu, Zixuan Ye, Qiulin Wang, Xintao\nWang, Pengfei Wan, Kun Gai, and Wenhu Chen. Univideo:\nUnified understanding, generation, and editing for videos.\narXiv preprint arXiv:2510.08377, 2025. 2\n[43] Chi Hsuan Wu, Kumar Ashutosh, and Kristen Grauman.\nStitch-a-recipe: Video demonstration from multistep descrip-\ntions. arXiv preprint arXiv:2503.13821, 2025. 2\n[44] Yicheng Xiao, Lin Song, Yukang Chen, Yingmin Luo,\nYuxin Chen, Yukang Gan, Wei Huang, Xiu Li, Xiaojuan\nQi, and Ying Shan. Mindomni: Unleashing reasoning gen-\neration in vision language models with rgpo. arXiv preprint\narXiv:2505.13031, 2025. 2, 3\n[45] Yicheng Xiao, Lin Song, Rui Yang, Cheng Cheng, Zun-\nnan Xu, Zhaoyang Zhang, Yixiao Ge, Xiu Li, and Ying\nShan. Haploomni: Unified single transformer for multi-\nmodal video understanding and generation. arXiv preprint\narXiv:2506.02975, 2025. 2\n6\n[46] Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu,\nMengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin\nHuang, et al. Dancegrpo: Unleashing grpo on visual genera-\ntion. arXiv preprint arXiv:2505.07818, 2025. 3\n[47] Zhiyuan Yan, Kaiqing Lin, Zongjian Li, Junyan Ye, Hui Han,\nZhendong Wang, Hao Liu, Bin Lin, Hao Li, Xue Xu, et al.\nCan understanding and generation truly benefit together\u2013or\njust coexist? arXiv e-prints, pages arXiv\u20132509, 2025. 3\n[48] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan\nHui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang,\nChenxu Lv, et al. Qwen3 technical report. arXiv preprint\narXiv:2505.09388, 2025. 2\n[49] Sherry Yang, Jacob Walker, Jack Parker-Holder, Yilun Du,\nJake Bruce, Andre Barreto, Pieter Abbeel, and Dale Schu-\nurmans. Video as the new language for real-world decision\nmaking. arXiv preprint arXiv:2402.17139, 2024. 2\n[50] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu\nHuang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiao-\nhan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video\ndiffusion models with an expert transformer. arXiv preprint\narXiv:2408.06072, 2024. 2\n[51] Tang Yansong, Ding Dajun, Rao Yongming, Zheng Yu, Zhang\nDanyang, Zhao Lili, Lu Jiwen, and Zhou Jie. Coin: A large-\nscale dataset for comprehensive instructional video analy-\nsis. In IEEE Conference onComputer Vision andPattern\nRecognition (CVPR), 2019. 4, 6, 1\n[52] Jiwen Yu, Yiran Qin, Xintao Wang, Pengfei Wan, Di Zhang,\nand Xihui Liu. Gamefactory: Creating new games with gen-\nerative interactive videos. arXiv preprint arXiv:2501.08325 ,\n2025. 2\n[53] Ailing Zhang, Lina Lei, Dehong Kong, Zhixin Wang, Jiaqi\nXu, Fenglong Song, Chun-Le Guo, Chang Liu, Fan Li, and Jie\nChen. Ui2v-bench: An understanding-based image-to-video\ngeneration benchmark. arXiv preprint arXiv:2509.24427 ,\n2025. 4\n[54] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards\nautomatic learning of procedures from web instructional\nvideos. In Proceedings oftheAAAI conference onartificial\nintelligence, 2018. 4, 6, 1\n[55] Shaobin Zhuang, Zhipeng Huang, Ying Zhang, Fangyikang\nWang, Canmiao Fu, Binxin Yang, Chong Sun, Chen Li, and\nYali Wang. Video-gpt via next clip diffusion. arXiv preprint\narXiv:2505.12489, 2025. 2, 6\n7",
      "summary": "This paper introduces Video-Next-Event Prediction (VNEP), a novel task where models generate dynamic video responses to procedural or predictive questions, moving beyond traditional text-based answers. The authors propose VANS, a model that jointly aligns a Vision-Language Model and a Video Diffusion Model using a new Joint-GRPO reinforcement learning approach, and present a dedicated dataset for this task.",
      "key_contributions": [
        "Formalization of Video-Next-Event Prediction (VNEP) as a new task requiring video-based answers.",
        "Development of VANS, a model that integrates vision-language and video generation components via reinforcement learning.",
        "Introduction of Joint-GRPO, a novel method for joint optimization of captioning and video generation with shared rewards.",
        "Creation of VANS-Data-100K, a large-scale dataset specifically designed for VNEP."
      ],
      "methodology": "The authors use a reinforcement learning framework (Joint-GRPO) to jointly train a Vision-Language Model and a Video Diffusion Model, optimizing both for semantic accuracy and visual consistency based on shared rewards, leveraging the new VANS-Data-100K dataset.",
      "results": "VANS achieves state-of-the-art performance on procedural and predictive benchmarks, demonstrating superior capability in both next-event video prediction and visualization tasks.",
      "keywords": [
        "Video Generation",
        "Next-Event Prediction",
        "Vision-Language Model",
        "Video Diffusion Model",
        "Reinforcement Learning",
        "Multimodal Learning",
        "Procedural Reasoning"
      ],
      "status": "new",
      "notes": null,
      "created_at": "2025-11-23T20:14:17.030060",
      "needs_llm_processing": false
    },
    "8": {
      "id": "2511.16668v1",
      "title": "V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models",
      "authors": [
        {
          "name": "Yang Luo",
          "affiliation": null
        },
        {
          "name": "Xuanlei Zhao",
          "affiliation": null
        },
        {
          "name": "Baijiong Lin",
          "affiliation": null
        },
        {
          "name": "Lingting Zhu",
          "affiliation": null
        },
        {
          "name": "Liyao Tang",
          "affiliation": null
        },
        {
          "name": "Yuqi Liu",
          "affiliation": null
        },
        {
          "name": "Ying-Cong Chen",
          "affiliation": null
        },
        {
          "name": "Shengju Qian",
          "affiliation": null
        },
        {
          "name": "Xin Wang",
          "affiliation": null
        },
        {
          "name": "Yang You",
          "affiliation": null
        }
      ],
      "abstract": "Recent progress in generative video models, such as Veo-3, has shown surprising zero-shot reasoning abilities, creating a growing need for systematic and reliable evaluation. We introduce V-ReasonBench, a benchmark designed to assess video reasoning across four key dimensions: structured problem-solving, spatial cognition, pattern-based inference, and physical dynamics. The benchmark is built from both synthetic and real-world image sequences and provides a diverse set of answer-verifiable tasks that are reproducible, scalable, and unambiguous. Evaluations of six state-of-the-art video models reveal clear dimension-wise differences, with strong variation in structured, spatial, pattern-based, and physical reasoning. We further compare video models with strong image models, analyze common hallucination behaviors, and study how video duration affects Chain-of-Frames reasoning. Overall, V-ReasonBench offers a unified and reproducible framework for measuring video reasoning and aims to support the development of models with more reliable, human-aligned reasoning skills.",
      "arxiv_id": "2511.16668v1",
      "arxiv_url": "http://arxiv.org/abs/2511.16668v1",
      "pdf_url": "https://arxiv.org/pdf/2511.16668v1",
      "published": "2025-11-20T18:59:42Z",
      "updated": "2025-11-20T18:59:42Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "full_text": "V-ReasonBench: Toward Unified Reasoning Benchmark Suite for\nVideo Generation Models\nYang Luo1,\u2217, Xuanlei Zhao1,\u2217, Baijiong Lin2,\u2217, Lingting Zhu3, Liyao Tang4\nYuqi Liu5, Ying-Cong Chen2, Shengju Qian6,\u2020, Xin Wang6, Yang You1\n1NUS2HKUST(GZ)3HKU4USYD5CUHK6LIGHTSPEED\nProject Page:https://oahzxl.github.io/VReasonBench/\nAbstract\nRecent progress in generative video models, such as Veo-\n3, has shown surprising zero-shot reasoning abilities, cre-\nating a growing need for systematic and reliable evalua-\ntion. We introduce V-ReasonBench, a benchmark designed\nto assess video reasoning across four key dimensions: struc-\ntured problem-solving, spatial cognition, pattern-based in-\nference, and physical dynamics. The benchmark is built\nfrom both synthetic and real-world image sequences and\nprovides a diverse set of answer-verifiable tasks that are\nreproducible, scalable, and unambiguous. Evaluations of\nsix state-of-the-art video models reveal clear dimension-\nwise differences, with strong variation in structured, spatial,\npattern-based, and physical reasoning. We further compare\nvideo models with strong image models, analyze common\nhallucination behaviors, and study how video duration af-\nfects Chain-of-Frames reasoning. Overall, V-ReasonBench\noffers a unified and reproducible framework for measuring\nvideo reasoning and aims to support the development of\nmodels with more reliable, human-aligned reasoning skills.\n1. Introduction\nRecent advancements in generative video models [2, 16, 24,\n28, 44], such as Veo-3.1 [14], Sora-2 [27], and Kling-2.5\n[19], have demonstrated remarkable zero-shot reasoning ca-\npabilities across diverse visual tasks. These models exhibit\nemergent abilities to perceive, model, and reason over dy-\nnamic visual data, reflecting a growing trajectory toward\ngeneral-purpose vision foundation models. Such progress\nsuggests that video models are beginning to move beyond\nmere frame synthesis, displaying cognitive-like behaviors\n*Equal Contribution.\n\u2020Corresponding Author.\n0.570.575.1446.86\n26.2972.00\n8.825.8813.24\n33.8226.4736.7633.33\n36.6723.33\n36.6733.3326.67023.64\n3.6432.73\n19.9140.00StructuredProblem-Solving\nSpatial CognitionPhysical DynamicsPattern-basedInference\n020406080\nSeedance-1.0-Lite\nVidu-Q2\nKling-2.5-Turbo-Pro\nHailuo-02\nVeo-3.1\nSora-2Figure 1. Evaluation of Video Generation Models on V-\nReasonBench. The performance of six video generation models\nacross the four core reasoning dimensions is illustrated. Detailed\nnumerical results are provided in Tab. 2.\nsuch as pattern inference, causal prediction, and strategic\nplanning [41]. However, despite these promising signs, the\ncapacity to quantitatively evaluate reasoning ability in these\nmodels remains underexplored and poorly standardized.\nThe \u201cChain-of-Frame\u201d (CoF) [12, 41] paradigm treats\nvideo generation as a sequence of reasoning steps, in di-\nrect analogy to \u201cChain-of-Thought\u201d in language models\n[23, 37, 38, 40, 47]. In this framework, a model receives an\ninitial image and a prompt, then produces a series of frames\nwhere the intermediate frames embody its reasoning trajec-\ntory and the final frame represents its answer or outcome.\nBecause the final frame encapsulates the model\u2019s inferred\nsolution, CoF enables a last-frame evaluation pipeline: wearXiv:2511.16668v1  [cs.CV]  20 Nov 2025\njudge the model on its concluding frame rather than re-\nquiring annotation of all intermediate steps. This provides\nan efficient and scalable approach for unambiguous evalua-\ntion of video reasoning, because evaluating every interme-\ndiate step requires substantial annotation effort, incurs pro-\nhibitive computational cost, and often yields noisy or am-\nbiguous supervisory signals [6, 18, 33].\nWhile last-frame evaluation offers an efficient way to as-\nsess reasoning within the CoF paradigm, vision-language\nmodels (VLMs) are not always reliable as sole automatic\njudges. In particular, VLMs may face difficulty interpret-\ning visually dense or grid-structured layouts that require\nprecise recognition of small cells, thin boundaries, or sub-\ntle geometric relationships [18, 21, 29]. To complement\nthese limitations and ensure stable evaluation across diverse\ntask types, V-ReasonBench adopts a hybrid strategy: mask-\nbased evaluation for tasks with well-defined object regions,\ngrid-based evaluation for tasks requiring fine-grained struc-\ntural accuracy, and lightweight VLM-based evaluation only\nfor visually simple outputs where consistency can be main-\ntained. Each method produces a numerical score that is con-\nverted into a pass or fail decision using task-specific thresh-\nolds, enabling scalable and reproducible pass@k evaluation\nwhile preserving strong alignment with human judgment.\nTo support consistent evaluation under the Chain-of-\nFrame framework and our last-frame scoring strategy, V-\nReasonBench is partitioned into four complementary rea-\nsoning classes, each targeting a specific dimension of rea-\nsoning: 1) Structured problem-solving encompasses tasks\ninvolving numerical manipulation, strategic planning in dy-\nnamic game states, and procedural logic derived from vi-\nsualized program traces. 2) Spatial cognition evaluates\nunderstanding of spatial relations, geometric transforma-\ntions, and symmetry-based patterns. 3) Pattern-based in-\nference probes sequence completion, analogical mapping,\nand abstract rule induction beyond surface-level visual cues.\n4) Physical dynamics examines comprehension of motion,\nforce interactions, temperature effects, and pressure-driven\nbehavior. Together, these four classes provide a compre-\nhensive assessment of cognitive capabilities in video under-\nstanding and reasoning.\nBuilt upon these methodological principles, V-\nReasonBench provides a unified suite for evaluating\nreasoning in generative video models under the CoF\nframework. Each instance consists of an initial image,\na task instruction, and a target final image, with both\nprocedurally generated and curated scenarios enabling\nfine control over difficulty and perceptual variation. By\napplying deterministic, last-frame scoring across four\ncomplementary reasoning classes, the benchmark offers\nconsistent measurements of a model\u2019s ability to interpret\nstructured inputs, follow task-specific rules, and produce\ncoherent final outcomes.Evaluating six cutting-edge video generation models\n(Sora-2 [27], Veo-3.1 [14], Kling-2.5-Turbo-Pro [19],\nSeedance-1.0-Lite [11], Vidu-Q2 [32], and Hailuo-02 [26])\nreveals clear strengths and limitations across multiple rea-\nsoning dimensions in Fig. 1. Our analysis further examines\neach model\u2019s characteristic reasoning patterns, the influence\nof video duration on CoF performance, and the contrast be-\ntween video and image models, highlighting several recur-\nring hallucination behaviors unique to video generation. As\nan early exploratory effort, V-ReasonBench provides an ef-\nficient and reproducible reasoning-centric framework that\noffers a unified foundation for benchmarking video reason-\ning and guiding future models toward more reliable, human-\naligned visual reasoning.\n2. Related Works\nVideo Generation.Recent advances in video generation\nhave been strongly driven by diffusion\u2013transformer models,\nwhich provide scalable architectures for producing high-\nquality visual content [3, 10, 25, 46]. Commercial sys-\ntems such as OpenAI\u2019s Sora-2 [27], Minimax\u2019s Hailuo-\n02 [26], Runway\u2019s Gen-3 [31], and Google DeepMind\u2019s\nVeo-3.1 [14] achieve impressive visual quality and strong\nalignment with text prompts, but their training data, model\ndesigns, and evaluation procedures remain undisclosed.\nAlongside these developments, recent research efforts, in-\ncluding CogVideoX [42], HunyuanVideo [35], and the Wan\nseries [36], have focused on enhancing temporal consis-\ntency, motion fidelity, and semantic coherence, reflecting\na broader trend toward more controllable and robust video\ngeneration systems.\nVideo Reasoning.Recent video generation models have\nshown emergent visual reasoning abilities that go beyond\ntheir original training goals [41]. TheChain-of-Frame\n(CoF) paradigm [41], analogous to chain-of-thought rea-\nsoning in language models [39], frames video generation as\na stepwise spatial and temporal reasoning process. This idea\nhas been explored through approaches such as VChain [17],\nwhich uses multimodal models to produce keyframes as in-\ntermediate reasoning signals, and visual chain-of-thought\nmethods [4, 45], which combine visual perception with it-\nerative reasoning for knowledge-intensive tasks and con-\ntrol. However, despite growing interest in evaluating these\nemergent capabilities, there remains a lack of a scalable and\ncomprehensive benchmark that systematically measures the\nbreadth and depth of video reasoning across diverse task\nsettings.\n3. V-ReasonBench Pipeline\n3.1. Design Principles\nV-ReasonBench is designed to evaluate the reasoning capa-\nbilities of generative video models through a comprehensive\nData Preparation\nVidu-Q2Generated Videos\nHailuo-02 Seedance-1.0-Lite\nSora-2 Veo-3.1 Kling-2.5-T urbo-Pro\nVideo Generation Models\nSampled V ideo\nEvaluation\nLast Frame GT (if any)\nPass or Not\nFigure 2. Overview of V-ReasonBench pipeline. The benchmark covers four reasoning dimensions, integrates both synthetic and real-\nworld scenarios, and supports reproducible, large-scale evaluation of video reasoning capabilities.\nsuite of tasks that require genuine understanding of visual\nreasoning, as illustrated in Fig. 2. Our benchmark is built\non three core principles:\n\u2022Last-Frame Dependency:All tasks are designed such\nthat the final answer can be determined exclusively from\nthe last frame of generated videos, enabling unambiguous\nand scalable evaluation of reasoning outcomes.\n\u2022Unified Evaluation Metric:We employ pass@k as our\nprimary evaluation metric across all reasoning classes,\nproviding a consistent measure of model performance\nand facilitating direct comparison between different ap-\nproaches.\n\u2022Multi-Faceted Reasoning:The benchmark spans 4 dis-\ntinct reasoning dimensions to comprehensively assess\nmodels\u2019 capabilities across mathematical, strategic, spa-\ntial, logical, physical, and programmatic reasoning.\n3.2. Reasoning Tasks and Formulations\nV-ReasonBench formalizes a comprehensive suite of rea-\nsoning classes, each targeting specific cognitive capabili-\nties essential for holistic video understanding. The follow-\ning is an overview of the 4 core reasoning dimensions in\nour benchmark. More explanations and visualizations of all\ntasks are provided in Appendix 7.\nStructured Problem-Solving.This dimension evaluates\nthe model\u2019s ability to perform systematic, rule-based rea-\nsoning across quantitative and procedural contexts. It con-\nsists of four sub-tasks:\n\u2022Arithmetic Operationexamines numerical reasoning\nfrom visual representations, requiring the model to per-\nform basic arithmetic operations, including addition, sub-\ntraction, multiplication, and division.\n\u2022Code Executiontests procedural and logical thinking\nthrough visualized programming tasks, where the model\nobserves LeetCode-style code with inputs and must pre-dict the correct program output.\n\u2022Sudokuevaluates constraint-based reasoning by asking\nthe model to complete a partially filled grid while adher-\ning to fixed numeric rules.\n\u2022Tic-Tac-Toeassesses strategic planning in a dynamic, ad-\nversarial environment, requiring the model to infer or se-\nlect the optimal next move given a game state.\nSpatial Cognition.This dimension examines spatial intel-\nligence across three sub-tasks:\n\u2022Shape Fittingassesses mental rotation and spatial ar-\nrangement skills.\n\u2022Visual Symmetryevaluates recognition of reflective and\nrotational symmetries.\n\u2022Color Connectiontests pathfinding by requiring the\nmodel to establish valid connections between elements of\nmatching color.\nPattern-based Inference.This dimension probes abstract\nrelational reasoning and inductive generalization across\nthree sub-domains:\n\u2022Sequence Completionevaluates the ability to infer the\nnext element in a visual or symbolic progression by rec-\nognizing hidden temporal or spatial rules.\n\u2022Analogy Solvingtests the understanding of relational\nstructure through problems of the form \u201cA:B as C:?\u201d\nrequiring cross-domain correspondence beyond surface\nsimilarity.\n\u2022Rule Followingexamines the capacity to infer governing\nprinciples from a few examples and apply them consis-\ntently to novel instances.\nPhysical Dynamics.This dimension evaluates the under-\nstanding of fundamental physical principles through four\nsub-tasks:\n\u2022Block Slidingexamines the ability to predict motion un-\nder gravity and friction, determining whether an object\nplaced on a slope will remain stationary or slide down.\nInputModel PredictionGround Truth\nVLM: Since the provided results perfectly and logically complete all three independent patterns, the reasoning is flawless.\nV-ReasonBench:                 Score = 0.71                 Pass = FalseEvaluationFigure 3. Example failure case fromSequence Completiontask illustrating the limitations of VLM-based automatic evaluation. Although\nthe underlying rule is simple, the VLM incorrectly assesses the model\u2019s output due to difficulties in recognizing small grid cells and fine\nstructural differences. More examples are given in Appendix 9.\n\u2022Communicating Vessels (CV)evaluates understanding\nof fluid pressure and equilibrium, asking the model to in-\nfer how liquid levels adjust across connected containers\nwhen pressure or volume changes.\n\u2022Temperature-Induced Deformationassesses reasoning\nabout material properties under thermal variation, such\nas predicting melting, shrinking, or deformation of ice\nblocks as temperature changes.\n3.3. Dataset Construction\nImage Pair Generation.We instantiate the benchmark\nwith an image\u2013pair framework rather than full videos. Ap-\nproximately 90% of instances are programmatically synthe-\nsized in custom simulation environments, yielding anini-\ntial imagethat represents the starting state and afinal image\nthat serves as the ground truth outcome, which enables scal-\nable data generation. Procedural generation provides broad\ncoverage across reasoning types while preserving consistent\nstate transitions, and every pair passes automated validation\nfollowed by targeted manual inspection to ensure that each\ntransformation is unambiguous, solvable by reasoning, and\nfree of annotation shortcuts. This design follows best prac-\ntices in controlled datasets for spatiotemporal and causal\nreasoning [13, 15, 43].\nDataset Statistics.V-ReasonBench includes 326 reason-\ning instances represented by 652 images paired with corre-\nsponding question annotations across four reasoning classes\nas summarized in Tab. 1. By generating 5 videos for each\ninstance-model pair, this yields a total of 9,780 generated\nvideos assessed in the benchmark. Each instance is speci-\nfied by an initial\u2013final image pair that encodes the state tran-\nsition necessary for reasoning.Table 1. Overview of reasoning dimensions, tasks, and number of\nvideos in V-ReasonBench. Each instance is an initial\u2013final image\npair, with each model generating five videos for pass@5 evalua-\ntion.\nDimension Task # Videos\nStructured\nProblem-SolvingArithmetic Operation\n5,250Code Execution\nSudoku\nTic-tac-toe\nSpatial\nCognitionShape Fitting\n2,040 Visual Symmetry\nColor Connection\nPattern-based\nInferenceSequence Complete\n1,590 Analogy Solving\nRule Follow\nPhysical\nDynamicsBlock Sliding\n900 CV\nTemperature\n3.4. Limitations of VLM-Based Evaluation\nAlthough recent benchmarks increasingly rely on vision-\nlanguage models (VLMs) for automatic judgment, this ap-\nproach faces notable limitations. VLMs often struggle to\ninterpret complex visual layouts, particularly grid-based or\ndensely structured scenes that require precise detection of\nsmall cells, thin boundaries, or fine spatial relationships, as\nshown in Fig. 3. These difficulties lead to incorrect judg-\nments even when the task logic is straightforward, as the\nmodel may miscount, overlook subtle structural cues, or\nmisidentify local patterns. These challenges highlight the\nneed for more reliable evaluation methods for tasks that de-\npend on fine-grained visual understanding.\n3.5. Evaluation Methodology\nTo enable precise and scalable last-frame-based evaluation,\nwe design three complementary assessment methods tai-\nlored to the characteristics of different task types in V-\nReasonBench.\n\u2022Mask-based Evaluation.Tasks with clear object bound-\naries and localized reasoning regions, includingSequence\nCompletion,Analogy Solving,Block Sliding,Communi-\ncating Vessel, andTic-tac-toe, are evaluated using a mask-\nbased comparison strategy. Masks are generated either\nfrom annotated templates or through automated segmen-\ntation tools such as SAM-2 [30] to isolate target regions.\nPixel-level mean squared error is then computed with\nhigher weights inside the masked areas and lower weights\nin non-essential regions, reducing the influence of back-\nground changes or stylistic variations.\n\u2022Grid-based Evaluation.For tasks that require structured\nlayouts or fine-grained spatial precision, includingVisual\nSymmetry, andRule Following, we employ a grid-based\nevaluation. Each frame is divided into uniform cells, and\ncell-wise accuracy is computed by comparing the pre-\ndicted and ground truth states in corresponding grid lo-\ncations. This method captures discrete positional errors\nand ensures sensitivity to geometric and structural con-\nstraints.\n\u2022VLM-based Evaluation.Tasks composed of simple\nitems that VLMs can easily handle, includingArith-\nmetic Operation,Sudoku,Code Execution,Temperature-\nInduced Deformation,Color Connection, andShape Fit-\nting, are scored using a lightweight VLM-based proce-\ndure. For mathematical and code tasks, the VLM ex-\ntracts textual or symbolic outputs from designated re-\ngions. For perception-oriented tasks, the VLM assesses\nstructural correctness and relational consistency based on\ncarefully designed prompts. This approach provides flex-\nible and standardized scoring where purely pixel-based\nmetrics are insufficient.\nEach of these evaluation strategies produces a numeri-\ncal score for the final frame, which is then converted into\na binarypassedorunpasseddecision using task-specific\nthresholds. This unified procedure ensures consistent, scal-\nable, and interpretable pass@k evaluation across all reason-\ning categories, with strong alignment to human assessment.\n4. Experiments\n4.1. Experimental Setup\nModels Evaluated.We evaluate six prevalent generative\nvideo models that represent the current frontier of com-\nmercial video generation capabilities. The models include:\nSora-2 [27], Veo-3.1 [14], Hailuo-02 [26], Vidu-Q2 [32],KlingAI-2.5-Turbo-Pro [19], and Seedance-1.0-Lite [11].\nThis diverse selection supports a comprehensive evaluation\nof reasoning behaviors across models with different archi-\ntectures, training pipelines, and capability tiers.\nEvaluation Protocol.We evaluate each model using a stan-\ndardized protocol that generates five videos per prompt.\nAll models are run with their default parameter settings\nand produce videos of approximately five seconds in du-\nration. Resolutions of 720p or 768p are used when sup-\nported, and every video is generated in a 16:9 aspect ratio\nto ensure consistency. Video generation prompts are care-\nfully designed and kept identical across models to minimize\nprompt-induced bias. Full prompts, score calculation meth-\nods and corresponding thresholds for each task are provided\nin Appendix 7. For all tasks, we use pass@5 as the primary\nevaluation metric. In VLM-based evaluation tasks, such as\ncolor connect and shape fit tasks, where VLM-based assess-\nment is reliable, we employ Gemini-2.5-Pro [7] as the auto-\nmatic evaluator.\n4.2. Per-dimension Evaluation\nAcross the four reasoning dimensions, the results in Tab. 2\nsuggest that current video models exhibit distinct areas of\nstrength rather than uniformly high performance. Sora-\n2 [27] continues to lead inStructured Problem-Solving\n(72.00),Spatial Cognition(36.76), andPattern-based In-\nference(40.00), with Hailuo-02 [26] following as a strong\nperformer in these categories. These trends indicate that\nsome models have developed comparatively stronger capa-\nbilities in discrete decision-making, spatial understanding,\nand abstraction from visual patterns. In contrast, Seedance-\n1.0-Lite [11], Vidu-Q2 [32], and Kling-2.5-Turbo-Pro [19]\nscore lower in these dimensions, which may reflect differing\ndesign priorities, such as a stronger focus on visual genera-\ntion fidelity over rule-driven or symbolic reasoning.\nA different pattern emerges inPhysical Dynamics.\nHere, Hailuo-02 and Vidu-Q2 achieve the highest scores\n(both 36.67), while Sora-2 reaches a moderate 26.67 de-\nspite its strengths in the other dimensions. This divergence\nsuggests that the inductive biases supporting structured and\npattern-based reasoning may not directly translate into ro-\nbust physical understanding, and that some models may in-\nstead focus on producing visually coherent physical mo-\ntions without fully capturing underlying physical principles.\nOverall, the dimension-wise results highlight that video rea-\nsoning is multifaceted: different systems capture different\naspects of the task, and there remains substantial opportu-\nnity to develop models that more uniformly integrate ab-\nstract, spatial, and physically grounded reasoning.\n4.3. Human Preference Alignment\nTo verify the reliability of our scoring pipeline, we per-\nform a human\u2013alignment study that directly compares the\nTable 2. Model-level overall and per-dimension performance on V-ReasonBench. A pass@5 score for each model is calculated within each\ndimension and presented accordingly.\nModelStructured\nProblem-SolvingSpatial\nCognitionPattern-based\nInferencePhysical\nDynamicsAverage\nSeedance-1.0-Lite [11] 0.57 8.82 0.00 33.33 10.68\nVidu-Q2 [32] 0.57 5.88 23.64 36.6716.69\nKling-2.5-Turbo-Pro [19] 5.14 13.24 3.64 23.33 11.34\nVeo-3.1 [14] 26.29 26.47 10.91 33.33 24.25\nHailuo-02 [26] 46.86 33.82 32.73 36.6737.52\nSora-2 [27] 72.00 36.76 40.0026.67 43.86\nTic Tac ToeMath Code\nSudoku\nRule Following Visual Analogy\nSequence CompletionShape Fitting\nVisual Symmetry Color Connection\nTemperature-Induced DeformationCommunicating VesselsBlock Sliding02040608097.09Accuracy (%)Human Preference Alignment\nFigure 4. Human\u2013alignment validation of our benchmark\u2019s scor-\ning pipeline. Each point compares binaryPass/Unpassdecisions\nfrom the automatic evaluation with human judgments across four\nreasoning categories.\nbinary passed/unpassed decisions produced by our bench-\nmark with human judgments. For each reasoning task, we\nrandomly sample 120 videos spanning a range of difficulty\nlevels. Six graduate-level examiners familiar with reason-\ning benchmarks independently assess whether each model\u2019s\nfinal-frame prediction correctly solves the task, following\nthe same execution-based criteria defined in our benchmark.\nWe then compute the accuracy of agreement between\nthe benchmark\u2019s passed/unpassed outputs and the human\npassed/unpassed labels. Across models and categories, we\nobserve consistently high alignment (accuracy = 97.09% on\naverage) as presented in Fig. 4, demonstrating that our au-\ntomatic decisions closely track human evaluations. The re-\nmaining discrepancies typically arise in visually ambiguous\ncases, such as nearly symmetric configurations or physics\nscenarios with partial occlusion, where human evaluators\nshow slightly higher tolerance to minor perceptual devia-\ntions.\nOverall, these results indicate that our evaluation pro-\ntocol produces human-consistent pass/fail judgments and\nthat V-ReasonBench provides a scalable, reproducible as-\nthinkingFigure 5. Example from the Seedance-1.0-Lite model on the hori-\nzontal visual symmetry task. The model introduces additional dec-\norative patterns across the mirrored axis, illustrating its tendency\nto enrich visual appearance rather than preserve original geometric\nform.\nsessment framework that remains well aligned with human\nreasoning preferences.\n5. Discussions\n5.1. Reasoning Patterns in Video Generation\nAcross several reasoning dimensions, we observe a consis-\ntent phenomenon in which some video models emphasize\nvisual enhancement over structural accuracy, which corre-\nlates with lower reasoning performance. Taking Seedance-\n1.0-Lite as examples in Fig. 5, the tendency appears on\ntasks featuring minimal, clean backgrounds (often pure\nwhite or black) with only the required elements. In these\nminimalist settings, the models frequently add extra tex-\ntures or objects or modify the scene layout, which alters\nthe intended structure and reduces pass@5 scores. The\nsame pattern appears beyond structured problem solving,\nincluding spatial and geometric tasks, pattern-based infer-\nence, and parts of physical reasoning that require a stable\ncanvas. More visualizations of reason patterns can be found\nin Appendix 10.\nA likely cause is a creative bias inherited from pretrain-\ning on large, open-domain video corpora where realism and\nvisual richness are valued. When the input scene is visually\nsparse, the model may treat it as incomplete and attempt\nto improve it with additional detail rather than preserve\nthe given structure. Training and decoding choices can\nstrengthen this behavior, such as reconstruction objectives\n4s thinking8s thinkingInputsOutputsDifferent CoF Time\n5s thinking10s thinkingInputsOutputs\nDifferent CoF TimeFigure 6. Effect of video duration on reasoning outcomes of Sora-\n2 in the Chain-of-Frame setting. Each row compares model gen-\nerations with different \u201cthinking\u201d durations for tasks such asSu-\ndokuandRule Following. Although longer durations correspond\nto longer reasoning processes (4s vs. 8s, 5s vs. 10s), the resulting\noutputs do not consistently improve.\nthat reward fine texture, temporal smoothness terms that\nencourage motion even when the correct solution is static,\nand limited exposure to diagram-like data with small sym-\nbols and thin boundaries. Together, these factors push the\ngenerator toward aesthetic completion instead of structure-\npreserving rendering, which can conflict with tasks that rely\non precise spatial or symbolic constraints.\n5.2. Influence of Duration on Video Reasoning\nWe regard the Chain-of-Frame (CoF) as the thinking pro-\ncess of video reasoning and analyze how video duration in-\nfluences the correctness of final answers. In CoF, a longer\nduration corresponds to a longer or more detailed reason-\ning process, which intuitively might be expected to enhance\nreasoning accuracy. However, as shown in Fig. 6, our ob-\nservations reveal a counterintuitive pattern: extending the\nduration does not consistently lead to better reasoning or\nhigher-quality outputs. Instead, longer sequences often in-\ntroduce redundant or irrelevant content, and in some cases,\ncause the model to hallucinate unrelated objects in the final\nframe.\nThis phenomenon aligns with findings from prior stud-\nies on temporal reasoning, which indicate that increas-\ning sequence length expands the available causal evidence\nbut also magnifies attention drift and temporal mis-binding[8, 22, 34, 48]. While longer clips can improve perfor-\nmance when additional frames contain relevant information\nand the model effectively integrates distant cues, excessive\ntemporal expansion tends to dilute attention and accumu-\nlate noise. A deeper investigation into how frame budget,\nsampling stride, and context window interact with reason-\ning quality will be an important direction for future work.\n5.3. Video Models vs. Image Models\nTo isolate the contribution of temporal reasoning, we com-\npare Veo-3.1 with NanoBanana [9] on V-ReasonBench,\ntreating them as representative video-based and image-\nbased reasoning paradigms. Veo-3.1 is evaluated in full\nvideo-generation mode, while NanoBanana functions as a\npowerful image-only baseline that performs reasoning with-\nout temporal progression. Fig. 7 shows representative out-\ncomes on physics-law and code-reasoning tasks, highlight-\ning the performance gap attributable to temporal modeling.\nImage-based models operate on a single static frame and\ntherefore rely heavily on structural priors, textual cues, and\npattern recognition. This enables high reliability on code-\nreasoning and symbolic tasks, where syntax, layout, and\ncharacter-level precision drive performance. However, the\nlack of temporal information limits their ability to infer dy-\nnamics. When facing tasks involving momentum trans-\nfer, balance, collisions, spatial transformations, or chain-\nstructured geometric manipulation, they often choose visu-\nally plausible outcomes that do not reflect the correct causal\nprocess.\nVideo models exhibit the opposite strength profile. By\ngenerating a CoF sequence, Veo-3.1 can explicitly model\ntransitions, represent latent motion paths, and maintain spa-\ntial and causal continuity across time. This frame-wise evo-\nlution provides the model with an internal mechanism for\nsimulating physical dynamics and multi-step spatial trans-\nformations, which directly improves accuracy on physics-\noriented tasks. Importantly, the same CoF mechanism also\nbenefits code-reasoning tasks: intermediate frames act as\nvisual checkpoints that stabilize the symbolic generation\nprocess, reducing local inconsistencies and improving step-\nwise logical execution.\nTemporal modeling through CoF gives video models\nclear advantages in both physical and procedural reason-\ning. Image models are strong at static structural tasks, while\nvideo models leverage process-aware temporal dynamics to\nhandle multi-step, causal, and simulation-heavy problems.\nCombining precise static parsing with CoF-based tempo-\nral modeling offers a promising path toward stronger visual\nreasoning systems.\n5.4. Hallucination in Video Reasoning\nDuring the exploratory stage of benchmark design, we ob-\nserved several hallucination phenomena emerging within\nInputNanoBanana\nVeo-3.1\nPrompt: Given the initial frame of a rigid rectangular block atop an incline and the annotated parameters, predict the final frame showing where the block comes to rest after sliding. \nInputNanoBanana\nVeo-3.1\nPrompt: Complete this code problem. Enter the answer behind \\\"# Output:\\\". Do not modify the existing code.\nFigure 7. Comparison between Veo-3.1 (video model) and NanoBanana (image model) onBlock Sliding(left) andCode Execution(right).\nVideo models leverage the Chain-of-Frames process to simulate intermediate states, enabling stronger performance on tasks that require\ncausal or temporal reasoning, although intermediate transitions may still appear physically inconsistent. Image models provide clean and\nstable outputs and excel at text-based tasks such as code execution, but their single-frame reasoning limits their ability to capture the\nunderlying physical dynamics.\n\u2026\u2026\u2026\u2026\u2026\u2026\n\u2026\u2026\n\u2026\u2026\u2026\u2026\n\u2026\u2026\n\u2026\u2026\u2026\u2026InputsOutputsChain-of-Frames Thinking ProcessNavigationMaze SolvingNewtonCradle\nFigure 8. Examples of hallucinations in the Chain-of-Frame reasoning process. Each row shows a trajectory from input to output, where\nthe final frame is correct but intermediate frames display unrealistic or physically inconsistent transitions.\nthe Chain-of-Frame process. Models may sometimes pro-\nduce the correct final outcome (last frame) while follow-\ning an incorrect reasoning process. As illustrated in Fig. 8,\nin themaze-solvingtask, a mouse successfully reaches the\ncheese in the final frame even though its intermediate trajec-\ntory passes through solid walls. A similar issue also appears\ninnavigationtasks. In theNewton\u2019s cradletask, the final\nconfiguration of moving and stationary balls aligns with the\nground-truth label, yet the intermediate frames violate mo-\nmentum conservation. For instance, when the leftmost ball\nis released, the entire system remains still instead of trans-\nferring motion immediately. These cases exemplify tem-\nporal hallucination, where invented or misordered actions\nand fabricated transitions preserve the correct endpoint butbreak causal consistency. This phenomenon has been doc-\numented in recent evaluations of video language models\n[5, 20] and corroborated by multimodal hallucination sur-\nveys highlighting their vulnerability to dense or abstract vi-\nsual patterns [1].\nFrom a benchmarking viewpoint, such \u201cright answer,\nwrong process\u201d failures are hard to detect if we only check\nendpoints, and they are also difficult to adjudicate using\nVLMs as mid-frame judges because VLMs themselves can\nmis-bind temporal relations or hallucinate missing steps\n[5, 20]. Accordingly, we favorend-state\u2013verifiabletasks\nwhere any process error necessarily yields an incorrect ter-\nminal state.\n6. Conclusion\nWe present V-ReasonBench, a unified benchmark suite\nfor evaluating video reasoning under the Chain-of-Frame\nparadigm. Using scalable last-frame scoring across four\ncore reasoning dimensions, it provides a reliable and scal-\nable assessment beyond visual fidelity. Experiments on six\nstate-of-the-art models reveal distinct strengths and system-\natic failure modes, underscoring the gap between genera-\ntion quality and true reasoning ability. As an early explo-\nration, V-ReasonBench offers a reproducible foundation for\nadvancing human-aligned video reasoning models.\nReferences\n[1] Zhuosheng Bai and et al. Hallucination of multimodal large\nlanguage models.arXiv preprint, 2024. 8\n[2] Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin,\nJunchen Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen,\nZheng Chen, Chengcheng Ma, Weiming Xiong, Wei Wang,\nNuo Pang, Kang Kang, Zhiheng Xu, Yuzhe Jin, Yupeng\nLiang, Yubing Song, Peng Zhao, Boyuan Xu, Di Qiu, De-\nbang Li, Zhengcong Fei, Yang Li, and Yahui Zhou. Skyreels-\nv2: Infinite-length film generative model, 2025. 1\n[3] Shoufa Chen, Mengmeng Xu, Jiawei Ren, Yuren Cong, Sen\nHe, Yanping Xie, Animesh Sinha, Ping Luo, Tao Xiang, and\nJuan-Manuel Perez-Rua. Gentron: Diffusion transformers\nfor image and video generation, 2024. 2\n[4] Zhuowan Chen, Qihang Zhou, Yibo Shen, Yuzhe Hong,\nZhiyuan Sun, Dan Gutfreund, and Chuang Gan. Visual\nchain-of-thought prompting for knowledge-based visual rea-\nsoning. InProceedings of the AAAI Conference on Artificial\nIntelligence, pages 1244\u20131252, 2024. 2\n[5] W. Y . Choong and et al. Benchmarking temporal hallucina-\ntions in vision llms.arXiv preprint, 2024. 8\n[6] Z Chu and et al. Navigate through enigmatic labyrinth: A\nsurvey of chain-of-thought reasoning in large language mod-\nels.ACL Long 2024, 2024. 2\n[7] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice\nPasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blis-\ntein, Ori Ram, Dan Zhang, Evan Rosen, Luke Marris, Sam\nPetulla, and et al. Gemini 2.5: Pushing the frontier with\nadvanced reasoning, multimodality, long context, and next\ngeneration agentic capabilities, 2025. 5, 13, 16\n[8] Xinyu Fang and et al. Mmbench-video: A long-form\nmulti-shot benchmark for video understanding. InNeurIPS\nDatasets and Benchmarks, 2024. 7\n[9] Alisa Fortin, Guillaume Vernade, Kat Kampf, and Ammaar\nReshi. Introducing Gemini 2.5 Flash Image (aka \u201cnano\nbanana\u201d). Blog post, Google Developers, 2025. Avail-\nable at: https://developers.googleblog.com/en/introducing-\ngemini-2-5-flash-image/. 7\n[10] Peng Gao, Le Zhuo, Dongyang Liu, Ruoyi Du, Xu\nLuo, Longtian Qiu, Yuhang Zhang, Chen Lin, Rongjie\nHuang, Shijie Geng, Renrui Zhang, Junlin Xi, Wenqi Shao,\nZhengkai Jiang, Tianshuo Yang, Weicai Ye, He Tong, Jing-\nwen He, Yu Qiao, and Hongsheng Li. Lumina-t2x: Trans-forming text into any modality, resolution, and duration via\nflow-based large diffusion transformers, 2024. 2\n[11] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu\nJiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xi-\naojie Li, et al. Seedance 1.0: Exploring the boundaries of\nvideo generation models.arXiv preprint arXiv:2506.09113,\n2025. 2, 5, 6\n[12] S. Ghazanfari and et al. Chain-of-frames: Advancing video\nunderstanding in multimodal llms via frame-aware reason-\ning.arXiv preprint arXiv:2506.00318, 2025. 1\n[13] Rohit Girdhar and Deva Ramanan. Cater: A diagnostic\ndataset for compositional actions and temporal reasoning.\narXiv preprint arXiv:1910.04744, 2019. 4\n[14] Google DeepMind. Veo 3.1: Google deepmind video gener-\nation model.https://deepmind.google/models/\nveo/, 2025. 1, 2, 5, 6\n[15] Michaela Gr \u00a8unde-McLaughlin, Ranjay Krishna, Juan Carlos\nNiebles, and Li Fei-Fei. Agqa: A benchmark for composi-\ntional spatio-temporal reasoning. InCVPR, 2021. 4\n[16] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel\nShalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy\nShiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weiss-\nbuch, Victor Kulikov, Yaki Bitterman, Zeev Melumian, and\nOfir Bibi. Ltx-video: Realtime video latent diffusion.arXiv\npreprint arXiv:2501.00103, 2024. 1\n[17] Ziqi Huang et al. Vchain: Chain-of-visual-thought\nfor reasoning in video generation.arXiv preprint\narXiv:2510.05094, 2025. 2\n[18] Ryo Kamoi, Yusen Zhang, Sarkar Snigdha Sarathi Das, Ran-\nran Haoran Zhang, and Rui Zhang. Visonlyqa: Large vision\nlanguage models still struggle with visual perception of geo-\nmetric information, 2025. 2\n[19] Kuaishou Technology. Kling ai: Next-generation ai creative\nstudio.https://klingai.com/, 2024. 1, 2, 5, 6\n[20] Chong Li and et al. Vidhalluc: Evaluating temporal hallu-\ncinations in multimodal large language models. InCVPR,\n2025. 8\n[21] Chengxi Li, Wenxuan Zhang, Xuwu Sun, Zhaoxiang Zhang,\nYuntao Chen, and Kaipeng Zhang. Vision-language models\nas top-view spatial reasoners. InEMNLP, 2024. 2\n[22] D. Li, Z. Huang, H. Liu, K. Zou, Y . He, F. Zhang, Y . Zhang,\nJ. He, W.-S. Zheng, Y . Qiao, and Z. Liu. Mvbench: A com-\nprehensive multi-modal video understanding benchmark. In\nCVPR, 2024. preprint. 7\n[23] Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu\nZeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, and Limin\nWang. Videochat-r1: Enhancing spatio-temporal perception\nvia reinforcement fine-tuning, 2025. 1\n[24] Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan\nDuan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiao-\nniu Song, Xing Chen, Yu Zhou, Deshan Sun, Deyu Zhou,\nJian Zhou, Kaijun Tan, Kang An, Mei Chen, Wei Ji, Qil-\ning Wu, Wen Sun, Xin Han, Yanan Wei, Zheng Ge, Aojie\nLi, Bin Wang, Bizhu Huang, Bo Wang, Brian Li, Changxing\nMiao, Chen Xu, Chenfei Wu, Chenguang Yu, Dapeng Shi,\nDingyuan Hu, Enle Liu, Gang Yu, Ge Yang, Guanzhe Huang,\nGulin Yan, Haiyang Feng, Hao Nie, Haonan Jia, Hanpeng\nHu, Hanqi Chen, Haolong Yan, Heng Wang, Hongcheng\nGuo, Huilin Xiong, Huixin Xiong, Jiahao Gong, Jianchang\nWu, Jiaoren Wu, Jie Wu, Jie Yang, Jiashuai Liu, Jiashuo Li,\nJingyang Zhang, Junjing Guo, Junzhe Lin, Kaixiang Li, Lei\nLiu, Lei Xia, Liang Zhao, Liguo Tan, Liwen Huang, Liy-\ning Shi, Ming Li, Mingliang Li, Muhua Cheng, Na Wang,\nQiaohui Chen, Qinglin He, Qiuyan Liang, Quan Sun, Ran\nSun, Rui Wang, Shaoliang Pang, Shiliang Yang, Sitong Liu,\nSiqi Liu, Shuli Gao, Tiancheng Cao, Tianyu Wang, Weipeng\nMing, Wenqing He, Xu Zhao, Xuelin Zhang, Xianfang Zeng,\nXiaojia Liu, Xuan Yang, Yaqi Dai, Yanbo Yu, Yang Li, Yi-\nneng Deng, Yingming Wang, Yilei Wang, Yuanwei Lu, Yu\nChen, Yu Luo, Yuchu Luo, Yuhe Yin, Yuheng Feng, Yuxi-\nang Yang, Zecheng Tang, Zekai Zhang, Zidong Yang, Binx-\ning Jiao, Jiansheng Chen, Jing Li, Shuchang Zhou, Xiangyu\nZhang, Xinhao Zhang, Yibo Zhu, Heung-Yeung Shum, and\nDaxin Jiang. Step-video-t2v technical report: The practice,\nchallenges, and future of video foundation model, 2025. 1\n[25] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei\nLiu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: La-\ntent diffusion transformer for video generation, 2024. 2\n[26] MiniMax AI. Hailuo 02 model card. Technical report, Min-\niMax AI, 2025. 2, 5, 6\n[27] OpenAI. Sora 2 system card. Technical report, OpenAI,\n2025. 1, 2, 5, 6\n[28] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjan-\ndra, Animesh Sinha, Ann Lee, Apoorv Vyas, and et al.\nBowen Shi. Movie gen: A cast of media foundation mod-\nels, 2025. 1\n[29] Atin Pothiraj, Elias Stengel-Eskin, Jaemin Cho, and Mohit\nBansal. Capture: Evaluating spatial reasoning in vision lan-\nguage models via occluded object counting. InICCV, 2025.\n2\n[30] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang\nHu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman\nR\u00a8adle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junt-\ning Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-\nYuan Wu, Ross Girshick, Piotr Doll \u00b4ar, and Christoph Feicht-\nenhofer. Sam 2: Segment anything in images and videos,\n2024. 5\n[31] Runway Research. Introducing gen-3 alpha: A new\ngeneration of generative video models.https : / /\nrunwayml.com/research/introducing- gen-\n3-alpha, 2024. 2\n[32] ShengShu Technology. Vidu q2 model release. Technical\nreport, ShengShu Technology, 2025. 2, 5, 6\n[33] Gaurav Shinde, Anuradha Ravi, Emon Dey, Shadman Sakib,\nMilind Rampure, and Nirmalya Roy. A survey on efficient\nvision-language models, 2025. 2\n[34] Xichen Tan, Yuanjing Luo, Yunfan Ye, Fang Liu, and Zhip-\ning Cai. Allvb: All-in-one long video understanding bench-\nmark, 2025. 7\n[35] Tencent Hunyuan Video Team. Hunyuanvideo: A systematic\nframework for large video generation models.https://\ngithub.com/Tencent-Hunyuan/HunyuanVideo,\n2024. 2\n[36] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie,\nDi Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al.Wan: Open and advanced large-scale video generative mod-\nels.arXiv preprint arXiv:2503.20314, 2025. 2\n[37] Qi Wang, Yanrui Yu, Ye Yuan, Rui Mao, and Tianfei Zhou.\nVideorft: Incentivizing video reasoning capability in mllms\nvia reinforced fine-tuning, 2025. 1\n[38] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H.\nChi, Sharan Narang, Aakanksha Chowdhery, and Denny\nZhou. Self-consistency improves chain-of-thought reason-\ning in language models.arXiv preprint arXiv:2203.11171,\n2022. 1\n[39] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny\nZhou. Chain-of-thought prompting elicits reasoning in large\nlanguage models.Advances in Neural Information Process-\ning Systems, 35:24824\u201324837, 2022. 2\n[40] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V . Le, and\nDenny Zhou. Chain-of-thought prompting elicits reasoning\nin large language models.arXiv preprint arXiv:2201.11903,\n2022. 1\n[41] Thadd \u00a8aus Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane\nGu, Nick Matarese, Kevin Swersky, Been Kim, Priyank\nJaini, and Robert Geirhos. Video models are zero-shot learn-\ners and reasoners.arXiv preprint arXiv:2509.20328, 2025.\n1, 2\n[42] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu\nHuang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiao-\nhan Zhang, Guanyu Feng, Da Yin, Yuxuan Zhang, Weihan\nWang, Yean Cheng, Bin Xu, Xiaotao Gu, Yuxiao Dong, and\nJie Tang. Cogvideox: Text-to-video diffusion models with\nan expert transformer, 2025. 2\n[43] Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun\nWu, Joshua B. Tenenbaum, and Antonio Torralba. Clevrer:\nCollision events for video representation and reasoning. In\nICLR, 2020. 4\n[44] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui\nZhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng\nShou. Show-1: Marrying pixel and latent diffusion models\nfor text-to-video generation, 2025. 1\n[45] Xinyi Zhao et al. Cot-vla: Visual chain-of-thought rea-\nsoning for vision-language-action models.arXiv preprint\narXiv:2503.22020, 2025. 2\n[46] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen,\nShenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang\nYou. Open-sora: Democratizing efficient video production\nfor all, 2024. 2\n[47] Denny Zhou, Nathanael Sch \u00a8arli, Le Hou, Jason Wei, Nathan\nScales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet,\nQuoc Le, and Ed H. Chi. Least-to-most prompting enables\ncomplex reasoning in large language models. InProceedings\nof the Advances in Neural Information Processing Systems\n(NeurIPS), 2022. 1\n[48] Jun Zhou and et al. Mlvu: Benchmarking multi-task long\nvideo understanding. InCVPR, 2025. 7\n7. Additional Details for Each Task\n7.1. Structured Problem-Solving\nThis reasoning class targets systematic computational and logical reasoning abilities. The four tasks collectively assess\nmodels\u2019 capacity to handle rule-driven problems that require step-by-step execution, numerical manipulation, and strategic\nthinking.\nArithmetic Operation.Models compute mathematical expressions presented visually, including addition, subtraction,\nmultiplication, and division. Problems are categorized by difficulty: easy (addition/subtraction, range 1-15), medium (addi-\ntion/subtraction/multiplication, range 1-50), and hard (all four operations, range 1-100). Each instance may contain 2 or 4\nproblems. All instances are programmatically generated with randomized numerical values and operator combinations, with\nvisual representations using standard mathematical notation rendered in various fonts and sizes to ensure diversity. Fig. 9\nillustrates an example of the generated video, showing the starting frame with the math problems (left), an intermediate frame\n(middle), and the final frame with all answers filled in (right). The evaluation prompt is:\u201cComplete the math problems in\nthe picture. Enter the final answer directly. Do not change anything else in the picture. Static camera, no zoom, no pan,\nno dolly. \u201dWe employ VLM-based evaluation where Gemini-2.5-Pro extracts all numerical results from the final frame. The\nprediction is marked aspassedonly if all problems and their answers are completely correct.\nFigure 9. An illustration of the arithmetic operation task, showing the starting frame (left), an intermediate frame (middle), and the final\nframe with completed answers (right).\nCode Execution.Models simulate program execution by tracing through visualized Python code with given inputs to\npredict outputs. The task includes three difficulty levels: easy, medium, and hard, based on the complexity of the Python\ncode problems sourced from LeetCode. Each instance contains only 1 problem. Code snippets are curated from classical\nalgorithm problems, with inputs and expected outputs synthetically generated. Fig. 10 illustrates an example of the generated\nvideo, showing the starting frame with the code and input (left), an intermediate frame (middle), and the final frame with\nthe output correctly filled in (right). The evaluation prompt is:\u201cComplete this code problem. Enter the answer behind \u201c#\nOutput:\u201d. Do not modify the existing code. \u201dVLM-based evaluation extracts the code problem and output from the final\nframe. The predictionpassesonly if the extracted problem and result exactly match the ground truth.\nFigure 10. An illustration of the code execution task, showing the starting frame with code and input (left), an intermediate frame (middle),\nand the final frame with the output filled in (right).\nSudoku.Models complete partially filled4\u00d74or9\u00d79Sudoku grids, with difficulty levels determined by the number\nof pre-filled cells: easy (more givens), medium (moderate givens), hard (fewer givens requiring deeper logical deduction).\nPuzzles are generated using constraint satisfaction algorithms that ensure unique solutions, and each puzzle is validated for\nsolvability before inclusion with visual grids using clear cell boundaries and digit rendering. Fig. 11 illustrates an example\nof the generated video, showing the starting frame with the initial puzzle (left), an intermediate solving stage (middle), and\nthe final frame with the complete solution (right). The evaluation prompt is:\u201cCreate a static, smooth, animation that solves\nthe given [4\u00d74 or 9\u00d79] sudoku. Enter the missing numbers one by one. Do not change anything else in the picture. Only\nfill the numbers in the empty cells so the sudoku is solved properly. A cursor moves and fills the correct number in the empty\nboxes. \u201d(The grid size in the prompt matches the actual puzzle size.) VLM-based evaluation extracts all cell values from the\nlast frame, and a puzzlepassesonly if all cells are the same numbers as the ground truth.\nFigure 11. An illustration of the4\u00d74Sudoku task, showing the starting frame with the initial puzzle (left), an intermediate solving frame\n(middle), and the final frame with the completed solution (right).\nTic-Tac-Toe.Models identify optimal moves in3\u00d73Tic-Tac-Toe games across different scenarios: 2 \u2018X\u2019/\u2018O\u2019, 3 \u2018X\u2019/\u2018O\u2019,\nand 4 \u2018X\u2019/\u2018O\u2019. Game states are programmatically generated to cover diverse tactical situations, with each state validated to\nhave a clearly optimal move according to the minimax strategy. Fig. 12 illustrates an example of the generated video, showing\nthe starting game state (left) and the final frame with the optimal move placed (right). The evaluation prompt is:\u201cGiven the\ncurrent state of the Tic-Tac-Toe board in the image, predict the next move that results in a win for the player whose turn it\nis. Output the position of the winning move. \u201dGrid-based evaluation partitions the board into individual cells and measures\naccuracy by comparing each predicted cell with its ground-truth counterpart; a prediction is marked aspassedonly when all\ncells match exactly.\nFigure 12. An illustration of the Tic-Tac-Toe task, showing the starting game state (left) and the final frame with the optimal move placed\n(right).\n7.2. Spatial Cognition\nThis category focuses on visual-spatial reasoning, testing how well models comprehend geometric properties, spatial trans-\nformations, and positional relationships in 2D environments.\nShape Fitting.Models receive a collection of geometric pieces and a target outline, then must determine the correct ar-\nrangement through rotation and positioning, with difficulty varying by shape complexity (simple triangles/rectangles vs.\nirregular polygons) and the number of pieces (2-5 pieces). The images were collected from the Internet or generated by im-\nage generation models. Fig. 13 illustrates an example of the generated video, showing the starting frame with separate pieces\nand empty holes (left), an intermediate fitting process (middle), and the final frame with all pieces correctly placed in their\ncorresponding holes (right). The evaluation prompt is:\u201cThe scene shows three colored pieces, and a panel with three holes.\nEach colored piece fits into one and only one hole. A hand grabs each colored piece and puts it into an empty hole that has\nthe exact same shape - if it doesn\u2019t fit, the hand tries another hole. All the objects must be placed in their respective holes. \u201d\nVLM-based evaluation uses Gemini-2.5-Pro [7] to assess whether all pieces are correctly positioned. A trial is considered a\npassonly if the VLM confirms that all pieces are properly fitted without any shape alteration.\nFigure 13. An illustration of the shape fitting task, showing the starting frame with pieces and empty holes (left), an intermediate fitting\nprocess (middle), and the final frame with all pieces correctly fitted (right).\nVisual Symmetry.Models complete incomplete symmetric patterns, with tasks categorized by symmetry type: reflective\n(mirror across vertical/horizontal/diagonal axes) and180\u25e6rotational symmetry. For diagonal symmetry, patterns are gener-\nated on8\u00d78grids by randomly coloring cells; for other symmetry types,10\u00d716grids are used with the same random\ncoloring process. Partial patterns are created by showing only half of the symmetric structure, requiring models to infer and\ncomplete the missing portion. Fig. 14 illustrates an example of the generated video, showing the starting frame with the\nincomplete pattern (left), an intermediate completion stage (middle), and the final frame with the fully symmetric pattern\n(right). The evaluation prompt is:\u201cInstantly reflect this pattern [along the central, vertical axis or along the central, hori-\nzontal axis or along the main diagonal axis (top-left to bottom-right) or180\u25e6along the central, vertical axis] while keeping\nthe existing colored pattern without modification. Static camera perspective, no zoom or pan. \u201dGrid-based evaluation divides\nthe image into cells and compares cell-wise matching between predicted and ground truth patterns, and a predictionpasses\nonly if all cells match exactly.\nFigure 14. An illustration of the vertical symmetric in the visual symmetry task, showing the starting frame with an incomplete pattern\n(left), an intermediate completion stage (middle), and the final frame with the fully symmetric pattern (right).\nColor Connection.Models link colored endpoints through non-intersecting paths on grids, with difficulty varying by the\nnumber of color pairs (2-4 pairs). Puzzles are generated with random positions for all circles. Fig. 15 illustrates an example of\nthe generated video, showing the starting frame with colored endpoints (left), an intermediate path-drawing stage (middle),\nand the final frame with all color pairs correctly connected (right). The evaluation prompt is:\u201cDraw three curves, one\nconnecting each pair of circles of the same color. Do not change anything else in the picture. Static camera, no zoom, no\npan, no dolly. \u201dVLM-based evaluation assesses path connectivity and non-intersection, and the predictionpassesif the VLM\nconfirms all color pairs are correctly connected without crossings.\n7.3. Pattern-based Inference\nThis reasoning class examines inductive capabilities and abstract thinking, focusing on how models extract implicit rules\nfrom limited observations and generalize them to new situations.\nFigure 15. An illustration of the color connection task, showing the starting frame with colored endpoints (left), an intermediate drawing\nstage (middle), and the final frame with all paths correctly connected (right).\nSequence Completion.Models infer the next element in a visual sequence by recognizing how objects evolve across\nsuccessive frames. Each sequence introduces variation in object count, shape, and color, requiring the model to detect\nconsistent visual regularities and extend them to the next plausible state. Sequences are programmatically generated using\nrule-based transformations, with each sequence following a deterministic rule that governs element transitions, and 3-5\nexamples are provided before asking for a prediction. Fig. 16 shows one demonstration of this task. The evaluation prompt\nis:\u201cFinish the incomplete frames of this shape-sequence puzzle. Do not change given patterns. Each row contains equal-\nsized square cells outlined in black on white. Shapes are solid black arrows, dots, or stripes that obey a clear mathematical\nrule (rotation, translation, scaling, or toggling). Produce the missing panels so the sequence remains consistent and has a\nsingle logical solution. \u201dMask-based evaluation compares the predicted next element against ground truth using pixel-level\naccuracy within the target region, and a predictionpassesif Acc>0.90.\nFigure 16. An illustration of the sequence completion task, showing the starting frame with one blank cell (left) and the final frame with\nall completed cells (right).\nAnalogy Solving.Models solve visual analogies of the form \u201cA:B::C:?\u201d, testing different relationship types: spatial trans-\nformations (rotation, reflection, scaling), attribute changes (color, texture), and structural modifications (adding/removing\ncomponents). Analogy problems are created by first defining a transformation, applying it to create the A\u2192B pair, then\nproviding a different C and asking models to apply the same transformation. Fig. 17 presents an example of this relationship,\nshowing the A:B:C triplet and ground truth answer. The evaluation prompt is:\u201cCreate a smooth animation to generate the\nmissing object in the lower right region and solve the visual analogy. The original three objects must remain still. Static shot,\nno zoom no pan no dolly.\u201cMask-based evaluation measures pixel-wise similarity between predicted and ground truth answer\nimages at the expected location for the transformed object, and a predictionpassesif the mean MSE distance<0.1.\nRule Following.Models infer transformation rules from example pairs and apply them to novel inputs, with rules varying\nin grid size and number of colored components in each grid. Rule sets are synthetically defined, with 2-4 demonstration pairs\ngenerated per rule, and test cases use different inputs but follow the same underlying transformation logic. Fig. 18 illustrates\nan example of the generated video, showing the starting frame with demonstration pairs and an empty test grid (left) and the\nfinal frame with the rule correctly applied to the test grid (right). The evaluation prompt is:\u201cModify the lower-right grid to\nadhere to the rule established by the other grids. You can fill cells, clear cells, or change a cell\u2019s color in the lower-right\nFigure 17. An illustration of the scaling relation in the analogy-solving task, showing the starting frame with the exemplar transformation\nin the top row (left), and the final frame with the corresponding transformation applied to the target object in the bottom row (right).\ngrid only. Don\u2019t modify any of the other grids. Static scene, no zoom, no pan, no dolly. \u201dGrid-based evaluation partitions\nthe image into grids and verifies the cell-wise correctness of the bottom-right table to be predicted against the ground truth;\na predictionpassesonly when all cells match perfectly (100% accuracy).\nFigure 18. An illustration of the rule following task, showing the starting frame with demonstration pairs and empty test grid (left) and the\nfinal frame with the rule correctly applied to the test grid (right).\n7.4. Physical Dynamics\nThis category assesses intuitive physics understanding, probing whether models can predict real-world physical behaviors\nbased on visual observations of forces, materials, and environmental conditions.\nBlock Sliding.Models predict whether objects will slide down inclined planes, with scenarios varying by slope angle (15\u00b0-\n60\u00b0), object shape (cube, sphere, cylinder), and surface properties (smooth, rough). Scenes are programmatically generated\nthrough controlled code specifications, and each scenario is verified to ensure a single, unambiguous outcome regarding\nwhether sliding should occur under the defined conditions. Fig. 19 illustrates an example of the generated video, showing\nthe starting frame with objects on the inclined surface (left), an intermediate sliding motion (middle), and the final frame\nshowing the final positions of all objects (right). The evaluation prompt is:\u201cObserve the objects on the inclined surface.\nPredict which objects will slide down and which will remain stationary. Show the final state after gravity acts. \u201dMask-based\nevaluation compares object positions in the final frame against ground truth, checking if each object\u2019s position indicates\nsliding or staying, and a predictionpassesif all object states are correctly predicted.\nCommunicating Vessels.Models predict liquid level redistribution in communicating vessels of diverse shapes and liquid\ncolors. The simulations compute final states based on hydrostatic equilibrium to ensure physical accuracy. As shown in\nFig. 20 illustrates an example of the generated video, showing the starting frame with initial liquid levels (left), an intermedi-\nate redistribution process (middle), and the final frame at equilibrium (right). The evaluation prompt,\u201cA connected container\n(communicating vessel) with two vertical tubes filled with liquid. The liquid level in both tubes changes slowly and smoothly\nuntil it reaches equilibrium, where the levels remain still. The motion should appear smooth and continuous, with no splash-\ning or turbulence\u201d, guides the assessment. Performance is measured using a mask-based method that computes the height\nFigure 19. An illustration of the block sliding task, showing the starting frame with objects on an inclined surface (left), an intermediate\nsliding motion (middle), and the final frame showing which objects slid down (right).\ndifference between predicted and ground truth levels in each vessel. A prediction is consideredpassedif the equilibrated\nliquid positions are level.\nFigure 20. An illustration of the communicating vessels task, showing the starting frame with initial liquid levels (left), an intermediate\nredistribution stage (middle), and the final frame at equilibrium (right).\nTemperature-Induced Deformation.Models predict material (ice) changes under temperature variations across scenarios,\nwith temperature ranges varying from -20\u00b0C to 100\u00b0C. Material transformations are created using physics-based rendering\nand simulation, with each scenario specifying specific temperature and material properties to ensure deterministic outcomes.\nFig. 21 illustrates an example of the generated video showing ice melting, with the starting frame showing solid ice at low\ntemperature (left), an intermediate melting stage (middle), and the final frame showing the completely melted state (right).\nThe evaluation prompt is:\u201cObserve the material at the initial temperature. Predict how it will change (melt, expand, contract,\ndeform) when the temperature changes to the target value. Show the final state. \u201dVLM-based evaluation uses Gemini-2.5-\nPro [7] to assess whether the predicted material state matches expected physical behavior (e.g., ice should melt above 0\u00b0C,\nmetal should expand when heated), and a predictionpassesif the VLM confirms correct physical transformation.\nFigure 21. An illustration of the temperature-induced deformation task using an ice melting example, showing the starting frame with solid\nice (left), an intermediate melting stage (middle), and the final frame with completely melted ice/water (right).\nTable 3. Complete pass@5 results for all models across all 13 tasks in V-ReasonBench. Tasks are grouped by their reasoning dimensions.\nDimension Task Seedance-1.0-Lite Vidu-Q2 Kling-2.5-Turbo-Pro Veo-3.1 Hailuo-02 Sora-2\nStructured\nProblem-SolvingArithmetic Operation 0.00 56.00 0.00 42.00 90.00 100.00\nCode Execution 0.00 0.00 2.22 11.11 31.11 53.33\nSudoku 0.00 0.00 0.00 0.00 18.00 50.00\nTic-Tac-Toe 3.33 3.33 26.67 63.33 46.67 90.00\nSpatial\nCognitionShape Fitting 21.43 10.34 21.43 32.14 46.43 35.71\nVisual Symmetry 0.00 3.33 3.33 26.67 10.00 26.67\nColor Connection 0.00 0.00 20.00 10.00 60.00 60.00\nPattern-based\nInferenceSequence Completion 0.00 10.00 10.00 5.00 15.00 0.00\nAnalogy Solving 0.00 10.00 0.00 0.00 10.00 20.00\nRule Following 0.00 24.00 0.00 20.00 40.00 48.00\nPhysical\nDynamicsBlock Sliding 20.00 20.00 10.00 20.00 10.00 20.00\nCommunicating Vessels 0.00 10.00 0.00 10.00 20.00 0.00\nTemperature-Induced Deformation 80.00 90.00 60.00 90.00 100.00 60.00\n8. Complete Task-Level Results\nTab. 3 and Fig. 22 present the detailed pass@5 performance of all evaluated models across each of the 13 reasoning tasks in V-\nReasonBench. This granular breakdown complements the dimension-level aggregation reported in the main paper, revealing\ntask-specific strengths and weaknesses for each video generation model.\n9. Limitation of VLM-Based Evaluation\nFig. 23, 24, 25 provide additional failure cases across multiple reasoning tasks. These examples highlight recurring issues\nin grid-structured and spatially dense scenes, where VLMs frequently misread fine-grained details such as cell boundaries,\nobject adjacency, and subtle geometric relations.\n10. More Reasoning Patterns Demonstrations\nTo provide a clearer view of the structural deviation patterns discussed in the paper, we include additional example predictions\nof Seedance-1.0-Lite and Vidu-Q2 from the Tic-Tac-Toe task. These cases further illustrate how models differ in their\nhandling of minimalist scenes in the video reasoning process that requires strict structural fidelity.\nTaken together, these two examples in Fig. 26 and 27 show a consistent pattern in how Seedance-1.0-Lite handles simple\nboard-based reasoning tasks. In both cases, the model starts from a clean tic-tac-toe target board and generates a richer, more\nrealistic scene: in the first example, the board appears in a cartoon setting with characters and logo-like icons; in the second,\nthe tic-tac-toe layout is mapped onto a chessboard surrounded by chess pieces and tokens. The model roughly keeps the 3\u00d73\ngrid region and the idea of a board game, but it replaces several cells with new symbols or removes parts of the board entirely.\nThese changes are harmless from a storytelling point of view, but they break the strict structural requirements of the task.\nThe final frames no longer contain the exact X/O pattern specified by the ground truth, so the predictions fail under our\ngrid-based evaluation. These two cases illustrate how the model\u2019s preference for visually rich, realistic scenes can conflict\nwith reasoning tasks that depend on precise symbolic and spatial accuracy.\nAcross these four tic-tac-toe examples, Seedance-1.0-Lite and Vidu-Q2 display the same tendency discussed in the main\npaper. When the input is a simple board on a plain background, the models transform it into a cartoon scene, a chessboard\narrangement, or a heavily textured abstract image, instead of preserving the intended X/O pattern. Although they maintain a\nrough sense of a central board, the exact cell-level structure required for correct reasoning is lost.\nThis pattern is closely related to the models\u2019 training data, which mostly consists of open-domain videos that emphasize\nrich visuals, motion, and diverse scenes, while containing very few clean, diagram-like examples. As a result, the models\ninterpret minimal inputs as incomplete and replace them with visually complex content. However, video reasoning tasks\nrequire the opposite: models must treat simple scenes as complete and maintain precise spatial and symbolic structure. This\nindicates that training data and objectives for reasoning should place stronger weight on structure-preserving generation\nrather than visually complex outputs.\nArithmetic Operation\nCode Execution\nSudoku\nTic-Tac-Toe\nShape Fitting\nVisual Symmetry\nColor Connection Sequence CompletionAnalogy SolvingRule FollowingBlock SlidingCommunicating\nVesselsTemperature-Induced\nDeformation\nSeedance-1.0-Lite\nVidu-Q2Kling-2.5-Turbo-Pro\nVeo-3.1Hailuo-02\nSora-2Figure 22. Summary of V-ReasonBench performance across video models. The figure illustrates how six video generation models perform\non 13 reasoning tasks, with scores rescaled within each dimension to enable direct comparison.\n11. More Hallucination in Video Reasoning\nInputModel PredictionGround TruthVLM: The prediction is incorrectbecause it is not a perfect reflection. For example, the blue square in the input at row 3, column 1 should be reflected to row 3, column 16,but this square is missing in the prediction image.\nV-ReasonBench: Score= 1.00Pass = TrueEvaluation\nFigure 23. Example failure case fromVisual Symmetrytask. Evaluation prompt for VLM: \u201cYou are given three images: an input image,\na ground-truth image, and a prediction image. The input image shows an original colored pattern. The correct transformation is: instantly\nreflect the entire pattern across its central vertical axis while preserving all existing colors and shapes exactly. No other modifications are\nallowed. Your task is to determine whether the prediction is correct. The prediction is correct only if two conditions are met: (1) The\npredicted pattern is a proper reflection of the input pattern across the central vertical axis, with no changes to colors, shapes, or structure\nother than mirroring. (2) The predicted pattern exactly matches the reflected pattern shown in the ground-truth image. This includes correct\nplacement of every cell, color consistency, and no unintended alterations.\u201d\nInputModel PredictionGround TruthVLM: The prediction fails the second condition. While the predicted lower-right grid is correct, the prediction image incorrectly modifies the example grids in the top row, which should have remained identical to the input and ground-truth images. The problem requires that only the lower-right grid changes.\nV-ReasonBench: Score= 1.00Pass = TrueEvaluation\nFigure 24. Example failure case fromRule Followingtask. Evaluation prompt for VLM: \u201cYou are given three images: an input image, a\nground-truth image, and a prediction image. All images contain several example grids and a lower-right grid. Only the lower-right grid\nis supposed to change. Your task is to judge whether the prediction is correct. The prediction is correct only if two conditions are met:\n(1) The predicted lower-right grid follows the same transformation rule demonstrated by the example grids in the input image. (2) The\npredicted lower-right grid visually and structurally matches the lower-right grid in the ground-truth image, including filled cells, empty\ncells, colors, and spatial arrangement. All other grids must remain identical across images.\u201d\nInputModel PredictionGround TruthVLM: The prediction is physically correct and visually matches the ground-truth image. The block correctly slides down the slope and comes to rest on the horizontal plane at the same final position shown in the ground-truth, which is consistent with the physical dynamics derived from the provided parameters.\nV-ReasonBench: Score= 0.83Pass = FalseEvaluation\nFigure 25. Example failure case fromBlock Slidingtask. Evaluation prompt for VLM: \u201cYou are given three images: an input image,\na ground-truth image, and a prediction image. The input image shows a rigid rectangular block resting at the top of an incline. The\nground-truth image shows the final resting position of the block after sliding down the slope under ideal planar contact, given the annotated\nparameters: slope lengthL slope , slope angle\u03b8, friction coefficients\u00b5 slope and\u00b5 plane , block length and height, and gravitational accel-\nerationg= 9.81m/s. Air resistance, elastic collisions, and secondary impacts at the slope foot are ignored. Your task is to determine\nwhether the prediction is physically correct. The prediction is correct only if the following conditions are satisfied: (1) The predicted final\nresting position of the block is consistent with the physical dynamics implied by the slope geometry and friction parameters. The block\nmust appear at the location where it should come to rest after sliding, without penetrating surfaces or hovering. (2) The predicted final\nframe visually matches the ground-truth image in block position, orientation, and contact location on either the slope or the flat plane.\nColors, shapes, and scene layout must remain unchanged except for the block\u2019s final location. (3) No extraneous motion or unintended\nscene modifications are introduced.\u201d\n\u2026\u2026\u2026\u2026\u2026\u2026\nFigure 26. Given an initial board (left), Seedance-1.0-Lite generates a stylized video in which the board is embedded in a cartoon scene\nwith characters and additional logos overlaid on several cells (right). Although the overall 3\u00d73 grid and most X/O placements are preserved,\nthe added icons and altered symbols change the board configuration, causing the prediction to fail under strict grid-based evaluation.\n\u2026\u2026\u2026\u2026\u2026\u2026\nFigure 27. Starting from a simple target tic-tac-toe board (left), the model generates a chessboard scene in which the 3\u00d73 grid is roughly\naligned but many cells are filled with chess pieces and tokens instead of the required X and O marks. These changes alter the intended\nboard configuration, so the prediction does not satisfy the 100% cell-wise match required by the grid-based evaluation.\n\u2026\u2026\n\u2026\u2026\n\u2026\u2026\nFigure 28. Starting from a simple initial board (left), the model generates a video in which the tic-tac-toe grid is placed on a colorful, highly\ntextured background. Although the final frame roughly keeps the 3\u00d73 grid and most X/O positions, some cells are partially overwritten by\nvisual effects and altered marks.\n\u2026\u2026\u2026\u2026\u2026\u2026\nFigure 29. Starting from a defined initial board (left), the model generates frames in which strong abstract textures rapidly cover the scene.\nThe tic-tac-toe grid and marks become barely visible and are eventually replaced by a large digit and background pattern.",
      "summary": "The paper introduces V-ReasonBench, a comprehensive benchmark suite designed to systematically evaluate the reasoning abilities of generative video models across four critical dimensions. By testing six state-of-the-art models, the study highlights significant differences in reasoning skills and provides insights into model limitations and behaviors.",
      "key_contributions": [
        "Proposes V-ReasonBench, a unified benchmark for evaluating video reasoning across structured problem-solving, spatial cognition, pattern-based inference, and physical dynamics.",
        "Constructs the benchmark using both synthetic and real-world image sequences with answer-verifiable, reproducible, and scalable tasks.",
        "Performs extensive evaluation of six leading video generation models, revealing dimension-specific strengths and weaknesses.",
        "Compares video models to strong image models and analyzes common hallucination patterns and the impact of video duration on reasoning."
      ],
      "methodology": "The authors developed a diverse set of reasoning tasks spanning four dimensions, sourced from synthetic and real-world sequences, and systematically evaluated multiple state-of-the-art video and image models on these tasks.",
      "results": "The evaluation demonstrates clear differences in reasoning performance across models and reasoning dimensions, uncovers common hallucination behaviors, and shows that video duration influences multi-frame reasoning capabilities.",
      "keywords": [
        "video generation",
        "reasoning benchmark",
        "structured problem-solving",
        "spatial cognition",
        "pattern inference",
        "physical dynamics",
        "model evaluation"
      ],
      "status": "new",
      "notes": null,
      "created_at": "2025-11-23T20:38:02.967724",
      "needs_llm_processing": false
    },
    "9": {
      "id": "2511.16666v1",
      "title": "SceneDesigner: Controllable Multi-Object Image Generation with 9-DoF Pose Manipulation",
      "authors": [
        {
          "name": "Zhenyuan Qin",
          "affiliation": null
        },
        {
          "name": "Xincheng Shuai",
          "affiliation": null
        },
        {
          "name": "Henghui Ding",
          "affiliation": null
        }
      ],
      "abstract": "Controllable image generation has attracted increasing attention in recent years, enabling users to manipulate visual content such as identity and style. However, achieving simultaneous control over the 9D poses (location, size, and orientation) of multiple objects remains an open challenge. Despite recent progress, existing methods often suffer from limited controllability and degraded quality, falling short of comprehensive multi-object 9D pose control. To address these limitations, we propose SceneDesigner, a method for accurate and flexible multi-object 9-DoF pose manipulation. SceneDesigner incorporates a branched network to the pre-trained base model and leverages a new representation, CNOCS map, which encodes 9D pose information from the camera view. This representation exhibits strong geometric interpretation properties, leading to more efficient and stable training. To support training, we construct a new dataset, ObjectPose9D, which aggregates images from diverse sources along with 9D pose annotations. To further address data imbalance issues, particularly performance degradation on low-frequency poses, we introduce a two-stage training strategy with reinforcement learning, where the second stage fine-tunes the model using a reward-based objective on rebalanced data. At inference time, we propose Disentangled Object Sampling, a technique that mitigates insufficient object generation and concept confusion in complex multi-object scenes. Moreover, by integrating user-specific personalization weights, SceneDesigner enables customized pose control for reference subjects. Extensive qualitative and quantitative experiments demonstrate that SceneDesigner significantly outperforms existing approaches in both controllability and quality. Code is publicly available at https://github.com/FudanCVL/SceneDesigner.",
      "arxiv_id": "2511.16666v1",
      "arxiv_url": "http://arxiv.org/abs/2511.16666v1",
      "pdf_url": "https://arxiv.org/pdf/2511.16666v1",
      "published": "2025-11-20T18:59:25Z",
      "updated": "2025-11-20T18:59:25Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "full_text": "SceneDesigner: Controllable Multi-Object Image\nGeneration with 9-DoF Pose Manipulation\nZhenyuan Qin\u2217Xincheng Shuai\u2217Henghui Ding\u2020\nFudan University\nhttps://henghuiding.com/SceneDesigner/\nAbstract\nControllable image generation has attracted increasing attention in recent years,\nenabling users to manipulate visual content such as identity and style. However,\nachieving simultaneous control over the 9D poses (location, size, and orientation)\nof multiple objects remains an open challenge. Despite recent progress, existing\nmethods often suffer from limited controllability and degraded quality, falling short\nof comprehensive multi-object 9D pose control. To address these limitations, we\nproposeSceneDesigner, a method for accurate and flexible multi-object 9-DoF pose\nmanipulation.SceneDesignerincorporates a branched network to the pre-trained\nbase model and leverages a new representation, CNOCS map, which encodes\n9D pose information from the camera view. This representation exhibits strong\ngeometric interpretation properties, leading to more efficient and stable training.\nTo support training, we construct a new dataset,ObjectPose9D, which aggregates\nimages from diverse sources along with 9D pose annotations. To further address\ndata imbalance issues, particularly performance degradation on low-frequency\nposes, we introduce a two-stage training strategy with reinforcement learning,\nwhere the second stage fine-tunes the model using a reward-based objective on\nrebalanced data. At inference time, we proposeDisentangled Object Sampling, a\ntechnique that mitigates insufficient object generation and concept confusion in\ncomplex multi-object scenes. Moreover, by integrating user-specific personaliza-\ntion weights,SceneDesignerenables customized pose control for reference subjects.\nExtensive qualitative and quantitative experiments demonstrate thatSceneDesigner\nsignificantly outperforms existing approaches in both controllability and quality.\n1 Introduction\nControlling the spatial properties of real-world images has been extensively explored, enabling users\nto manipulate the structure of interesting subjects or the overall layout of a scene [ 68,61,24,19,17,\n35,36,6,57]. However, most existing methods are confined to 2D space and often rely on densely\nannotated control maps as input, such as depth images. In contrast, 3D spatial control remains a\nlargely underexplored challenge. Consider, for example, a designer aiming to arrange multiple pieces\nof furniture in a room, each with distinct sizes and orientations, or a user wishing to generate an image\nwhere a pet dog is turned away from the camera, gazing at the landscape ahead. These scenarios\nhighlight the need for generation models that support 3D-aware multi-object control, a capability that\nis both essential for practical applications and insufficiently addressed by current approaches.\nThere have been several preliminary explorations in 3D-aware controllable generation [ 40,34,30,\n18,8,26,50]. For example, LOOSECONTROL [ 6] employs 3D bounding boxes for controlling the\n\u2217Equal Contribution\n\u2020Henghui Ding (henghui.ding@gmail.com) is the corresponding author with the Institute of Big Data, College\nof Computer Science and Artificial Intelligence, Fudan University, Shanghai, China.\n39th Conference on Neural Information Processing Systems (NeurIPS 2025).arXiv:2511.16666v1  [cs.CV]  20 Nov 2025\nAn astronaut walking on Mars\nA corgi dog on beach\nA Jeep on a muddy mountain road at sunset\nA rabbit and a turtleon the grassA bear riding motorbike on the ruinsPanda, dog, and eagle in forest\u2026in a lavender field... on moss in forest\u2026on the old book(a) Single-object 9D Pose Control\n(b) Multi-object Scenario(c) Customization\nFigure 1: 9D pose control results of theSceneDesigner. The figures show the applications in single-\nobject, multi-object, and customization scenarios, exhibiting high quality, flexibility and fidelity.\nlocation and size of the object in 3D space. To enable orientation control, some methods take rotation\nangles around designated axes as input. Among them, Zero-1-to-3 [ 30] infers the novel perspective\nof reference subject, but relies on external inpainting tools [ 46] to create a complex background.\nContinuous 3D Words [ 8] and its following work [ 40] are constrained by unrealistic visual style and\nlimited control over object quantity and pose diversity. More recently, ORIGEN [ 34] introduces\nprecise orientation control but is restricted by its dependence on a one-step generative model, which\nhinders compatibility with widely used multi-step frameworks [46, 41, 10].\nTo address the aforementioned challenges, we enhance existing text-to-image models [ 10,46] by\nenabling 9D pose control of multiple objects within the same scene, as shown in Fig. 1. To support\nthis goal, we first construct a new dataset,ObjectPose9D, which provides 9D pose annotations\nacross diverse real-world scenarios. To buildObjectPose9D, we begin with the publicly available\nOmniNOCS dataset [ 21], which offers accurate pose annotations but is limited in object and back-\nground diversity. To overcome this limitation, we further annotate the large-scale MS-COCO dataset\n[27] with 9D poses to expand the variety of visual concepts and scene types. Specifically, we employ\nMoGe [ 60] and Orient Anything [ 62] to estimate 3D bounding boxes with orientations. All collected\nimages and pose annotations are then carefully checked and manually refined by human annotators\nto ensure data quality. Together, these efforts yieldObjectPose9D, a diverse and richly annotated\ndataset for training image generation models with flexible multi-object 9D pose control.\nA Stormtrooperstands beside a motorcycle, inside a futuristic hangar. 1. Pose Control2. Control Signals3. Image GenerationPrompt\nCNOCS Maps\nFigure 2: Overview ofSceneDesigner.Then,how can 9D poses be efficiently en-\ncoded for controllable image generation?\nSome previous studies [ 8,40] project rota-\ntion angle into textural space and combine\nit with text embeddings. However, this\nway struggles to capture fine-grained ori-\nentation and precise spatial positioning.\nInspired by the success of ControlNet-like\narchitectures [ 68,70,43,36] in structural\ncontrol, LOOSECONTROL [ 6] adopts 3D bounding boxes for 3D-aware guidance. However, this\nrepresentation lacks orientation information. For example, a single bounding box may ambiguously\ncorrespond to either a front- or back-facing object, limiting controllability and reliability. To address\nthis, we build upon the Normalized Object Coordinate System (NOCS) [ 54] to better encode geo-\nmetric properties. Nevertheless, a traditional NOCS map requires a precise 3D shape of each object\ncategory, which is user-unfriendly and often difficult to acquire. To overcome this, we introduce the\nCuboid NOCS (CNOCS) map to only consider a cuboid shape for general purpose. This simplifica-\ntion retains essential geometric cues while supporting category-agnostic pose encoding. Moreover,\n2\nCNOCS map allows for flexible variants and generalizes well across object categories. Besides, to\naddress the imbalanced pose distribution in real-world data, we introduce a two-stage training strategy\nwith reinforcement learning. In the first stage, the model is trained onObjectPose9Dto learn basic\npose controllability. In the second stage, it is fine-tuned to improve performance on low-frequency\nobject poses by maximizing our proposed reward function under a balanced distribution. During\ninference, we applyDisentangled Object Sampling, a technique designed to mitigate concept fusion\nand insufficient generation in complex multi-object scenes. Furthermore, by integrating user-specific\npersonalized weights, our method enables customized pose control for user-provided reference\nsubjects. Within our framework, users can freely operate the cuboid-shape meshes in 3D space as\ndemonstrated in Fig. 2, identifying the 3D location, size, and orientation of each object.\nOur main contributions are:1)We introduceObjectPose9D, a dataset with rich real-world scenes and\ncomprehensive 9D pose annotations, facilitating effective training of pose-controllable generation\nmodels.2)We proposeSceneDesigner, a framework that supports multi-object 9D pose control. It\nleverages our proposed pose representation, CNOCS map, which encodes 3D properties of objects\nfrom the camera view using a coarse cuboid abstraction. We further introduce a two-stage training\nstrategy with reinforcement learning to mitigate data imbalance and enhance generalization. At\ninference time,Disentangled Object Samplingis employed to address insufficient or entangled object\ngeneration, while user-specific weights enable personalized pose control.3)Extensive qualitative and\nquantitative experiments demonstrate thatSceneDesignersignificantly outperforms previous methods\nin both single- and multi-object scenarios, achieving high fidelity and controllability.\n2 Related Works\nControllable generation:Controllable image generation [ 68,70,43,36,66,25,58,47,13,2,4,22,\n37,29,56,49] enables fine-grained user control over visual content. Among them, ControlNet [ 68]\nand its following works [ 43,70] introduce a branched network to process the geometry guidance,\nresulting in the image with high structure fidelity. Other methods [ 2,4] like DreamBooth [ 47]\nand Textual Inversion [ 13] learn new concepts given by users, endowing the text-to-image (T2I)\nmodels with customization capability. Despite these efforts, controlling the 9D poses of objects\nstill faces challenges. Most methods [ 24,61,64] can only handle 2D bounding boxes, which are\ninsufficient to represent 3D properties. Recently, LOOSECONTROL [ 6] lifts this condition to 3D\nspace, but struggles to represent precise orientation. A group of methods [ 40,8] leverage precise\npose annotations from a synthetic dataset, and receive the rotation angles from users. However,\nthey are constrained by limited controllability in intricate pose and multi-object scenarios, and poor\ngeneralization in in-the-wild images. The recent work ORIGEN [ 34] optimizes initial noise to\nmaximize the constructed reward function. However, it cannot localize objects in designated area.\nAdditionally, it relies on one-step generation models, hampering its compatibility with general models.\nCollectively, there are currently no approaches that can efficiently control the 9D poses of multiple\nobjects in image generation. Beyond the image domain, recent advances in video generation have\nexplored 3D-aware controllable synthesis. 3DTrajMaster [ 12] focuses on multi-entity trajectory\ncontrol through MLP-encoded pose representations, while FMC [ 50] enables 6D pose control for\ncameras and objects. Meanwhile, CineMaster [ 57] extends the LOOSECONTROL [ 6] paradigm to\nvideo synthesis, providing intuitive cinematic control.\n3D-aware image editing:There is a growing body of methods [ 69,30,59,67,39,22,14] that\nfocus on 3D-aware image editing. Among them, 3DIT [ 33] was trained on collected synthetic data,\nmanipulating the object by translation or rotation. Similarly, Zero-1-to-3 [ 30] can generate novel\nviews of the reference subject given by users. NeuralAssets [ 63] learns object-centric representations\nthat enable disentangled control over object appearance and pose in neural rendering pipelines.\nDifferent from these methods, Diffusion Handles [ 39] leverages the point clouds estimated from the\ninput image, and guides the sampling process through transformed points and depth-conditioned\nControlNet [ 68]. 3DitScene [ 69] reconstructs the 3D Gaussian Splatting [ 20] of the scene, and\nmanipulates the 3D Gaussians of the designated object through arbitrary 3D-aware operations.\nAligning the generation model with human preference:Based on the success of reinforcement\nlearning in the field of natural language processing [ 38,45], some studies [ 11,7] make efforts to align\nthe generated image with human preference. Some methods [ 65,42] like ReFL [ 65] maximizes the\nproposed reward score through backpropagation. Other RL-based methods like DDPO [ 7] leverage\n3\npolicy gradient algorithm for finetuning. Different from these methods, Diffusion-DPO [ 53] performs\nsupervised learning on constructed preference dataset.\n3 Method\n3.1 Preliminaries\nGenerative models:The generative models are designed to transform samples from a simple noise\ndistribution into data from the target distribution. Diffusion-based methods [ 51,52,15] learn a noise\nprediction model, and remove the estimated noise from noisy image during sampling. From another\nperspective, Flow matching [ 3,28,32] achieves the goal by training a neural network to model the\nvelocity field, moving the noise to data along straight trajectories.\nNormalized Object Coordinate System (NOCS):To estimate 6D poses and sizes of multiple objects,\nNOCS [ 54] constructs the correspondences between pixels and normalized coordinates. It represents\nall objects in a normalized space while maintaining consistent category-level orientation. Then, the\nmethod constructs NOCS map, an RGB image representing each pixel\u2019s normalized coordinates,\nwhich is efficient for training the prediction model.\n3.2 Overview\nOur goal is to equip T2I models [ 46,41,10] with the controllability in 9D poses of multiple objects.\nFor clarity, we only present our flow-based implementation [ 10]. It\u2019s worth noting that our method can\nalso be applied to diffusion models [ 46,41]. Formally, given the text prompt cpdepicting the visual\ncontent that contains a set of objects {obji}No\ni=1, and their 9D poses {li, si, oi}No\ni=1, where li, si, oi\nrepresent the 3D location, size, and orientation from the camera view, respectively. The generation\nmodel aims to create the final image xthat minimizes \u03a3No\ni=1\u0000\nDls(x,obji, li, si)+D o(X(x,obji), oi)\u0001\n,\nwhere Dlsindicates the inaccuracy of location and size in image space. Xinfers the object\u2019s\norientation and Dois a discriminant function that calculates the discrepancy of input and estimated\norientations. For simplicity, we denote the {li, si, oi}No\ni=1as{Pi}No\ni=1. Therefore, our task is to learn a\nconditional generation modelp \u03b8(x|cp,{Pi}No\ni=1)parameterized by\u03b8that minimizes:\nEt,(x,c p,{Pi}No\ni=1)\u223cD,\u03f5\u223cN(0,I)\u0014\r\r\rv\u03b8\u0010\nxt, t, cp,{Pi}No\ni=1\u0011\n\u2212(\u03f5\u2212x)\r\r\r2\u0015\n,(1)\nwhere v\u03b8models velocity field, the data are sampled fromObjectPose9D, denoted as D, and xt=\n(1\u2212t)x+t\u03f5 . In following sections, Sec. 3.3 introduces our CNOCS map that encodes multi-object\n9D poses {Pi}No\ni=1. Then, Sec. 3.4 details the construction pipeline of our datasetObjectPose9D.\nFinally, Sec . 3.5 presents our two-stage training strategy for enhancing the alignment with input\ncondition, andDisentangled Object Samplingtechnique used at inference time.\n3.3 CNOCS Map: Effective Representation of 9-DoF poses\nFigure 3: Illustration of CNOCS map.To encode the 9D poses of general objects,i.e.,\n{Pi}No\ni=1, a straightforward method is to project\nthe location, size, and orientation to embeddings,\nrespectively, while integrating the features into\nnetwork through cross- or self-attention mecha-\nnism [ 8,40,61,24,55,23]. In contrast, control\nmaps used in ControlNet-like methods [ 68,43]\noffer an alternative approach for encoding 9D\npose information. This spatial representation\nprovides stronger structural constraints com-\npared to direct projection methods. Our subse-\nquent experimental results (Sec. 4) demonstrate\nthat this map-based representation is more ef-\nfective than direct embedding approaches, there-\nfore, we adopt this encoding strategy for our\nframework.\n4\nWe leverage the idea from NOCS [ 54] and propose CNOCS map to preserve the object\u2019s 3D properties,\nexhibiting strong geometry interpretation. A preliminary idea is to obtain the 3D bounding boxes\nof objects and render them in depth-sorted order depending on their distances from the camera\nview, which have already been explored in recent works [ 6]. Although this kind of representation\ncan perceive the location and size of objects, it is viewpoint-dependent and insufficient to present\nprecise orientation. Inspired by NOCS, it\u2019s necessary to establish correspondence between pixels\nindexed by (u, v) and their associated points on the object\u2019s surface, while encoding the point\nbased on its coordinate in object space. However, NOCS has to access the precise 3D shape of\neach category, leading to cumbersome user input and hindering its application. In contrast, our\nCNOCS map alleviates the issues by using a cuboid shape inherited from 3D bounding boxes, while\nformalizing the encoding process to achieve better generality. The process of constructing CNOCS\nmap is illustrated in Fig. 3 and formalized in Eq. 2.\nxc\nu,v, yc\nu,v, zc\nu,v,oi=Intersect(u, v,{bbox i}No\ni=1),\nxo\nu,v, yo\nu,v, zo\nu,v=projoi\nc2o(xc\nu,v, yc\nu,v, zc\nu,v),\nxno\nu,v, yno\nu,v, zno\nu,v=normalize(xo\nu,v, yo\nu,v, zo\nu,v,bbox oi),\nCNOCS[u, v] =f(xno\nu,v, yno\nu,v, zno\nu,v),(2)\nwhere \u201c Intersect \u201d operation finds the associated camera space point (xc\nu,v, yc\nu,v, zc\nu,v)of[u, v] -indexed\npixel on surface of 3D bounding box belonging to the oi-th object. Specifically, the oi-th object\noccludes other entities along the view ray from the pixel. Next, projoi\nc2otransforms the coordinate\nfrom camera space to object space based on the pose of oi-th object. The \u201c normalize \u201d operation then\nmaps the values to [\u22121,1] using the side lengths of 3D bounding box. Finally, we assign the feature\ncalculated by the encoding function fto[u, v] -indexed pixel in CNOCS map. There are many choices\nforf, leading to different variants such as: 1) Constant function. We can simply assign a predefined\nvector for any point, such as Euler angles. This variant is named C-CNOCS map. 2) Identity function.\nSimilar to NOCS [ 54], we can directly use the coordinate (xno\nu,v, yno\nu,v, zno\nu,v)as point embedding.\nUnlike the original NOCS map, the points are located on surface of 3D bounding box instead of\nthe precise shape determined by CAD model. This variant is named I-CNOCS map. 3) Spherical\nharmonic function. (xno\nu,v, yno\nu,v, zno\nu,v)can be further transformed into the form (\u03b8no\nu,v, \u03d5no\nu,v, rno\nu,v)in\nspherical coordinate system. Then, we can construct a series of Laplace\u2019s spherical harmonics\nYm\nl(\u03b8no\nu,v, \u03d5no\nu,v)depending on a user-defined degree as point embedding, where l, m represent indices\nof degree and order, respectively. This variant is named S-CNOCS map. Based on empirical results\n(Sec. 4), we use I-CNOCS map as the pose representation since it is simple and effective.\nOrientation EstimationPoint Cloud Estimation3DBoxEstimation9DPoses\nInput Image\nFigure 4: Annotation pipeline of 9D poses in MS-COCO [27].\n3.4 ObjectPose9D Dataset\nThere are currently no existing datasets suitable for learning multi-object 9D pose control. Although\nsome public datasets like Objectron [ 1] and OmniNOCS [ 21] contain object pose annotations, they\nare constrained by limited object categories and scene diversity. Therefore, we select several subsets\nfrom OmniNOCS as the base of our dataset, which cover common objects and scenarios in indoor and\nstreet scenes. Furthermore, we resort to the COCO dataset [ 27] to enlarge the data distribution and\nimprove the model generalization ability. The annotation pipeline of 9D poses in COCO dataset [ 27]\nis shown in Fig. 4 and consists of the following steps:\n1.Selection of objects. First, we obtain the suitable objects from each image using the following\ncriteria:1)The area of object mask must be limited within predefined lower and upper bounds,\n5\nexcluding the objects that are too small or large;2)The orientation of the objects should be\nunambiguous. Categories with inherent orientation ambiguity, such as \"bottle\", are excluded.\n2.Estimation of orientation and 3D bounding boxes. Next, we obtain the orientations and 3D\nbounding boxes of suitable objects to construct CNOCS map. First, Orient Anything [ 62] is\nused to infer the object orientation and we filter the objects with low prediction confidence. In\nparallel, we get the initial geometry estimation (point clouds) of the scene via an advanced 3D\nreconstruction method [ 60], and identify the points belonging to the object based on the mask,\nwhile discarding the points that are too far from centroid by a threshold. Finally, 3D bounding\nboxes can be derived using the point clouds and predicted orientation, which tightly enclose the\nobject points.\nTo ensure data quality, human annotators manually filter out low-quality samples and refine inaccurate\nannotations. Based on the estimated 3D bounding boxes and orientations, we then construct the\nCNOCS map representation using Eq. 2. Besides, Multimodal Large Language Model (MLLM) [ 5]\nis also employed to generate descriptive captions for each image, enriching the dataset with aligned\ntextual information. These steps together yield the final dataset,ObjectPose9D. Further details on\ndataset statistics and construction procedures are provided in Sec. A.1.\n3.5 SceneDesigner: Multi-object 9-DoF Pose Control\nWith the constructed datasetObjectPose9D, we proceed to train our proposed method (SceneDesigner)\nfor 9D pose control of multiple objects. The details about training and inference are introduced below.\nLearning for 9-DoF pose control:We simply introduce ControlNet-like [ 68] branched network\ninto the base model as overall architecture, which receives CNOCS map that encodes {Pi}No\ni=1, text\nprompt cp, and the noisy latent in the current step. v\u03b8is learned in two stages. In the first stage,\nthe parameters from the branched network are optimized using Eq. 1, learning the basic 9D pose\ncontrollability. However, the learned model exhibits inferior performance on low-frequency poses\nfromObjectPose9D, which is caused by imbalanced pose distribution. For example, the model fails\nto generate the back views of most animals due to biases from datasets [ 21,27]. Therefore, we resort\nto the technique from RLHF (Reinforcement Learning from Human Feedback). For aligning the\nobject pose with the condition, we aim to maximize the introduced reward function below:\nrls(x, c p,{Pi}No\ni=1) = \u03a3No\ni=1\u0000\n1\u2212D ls(x,obji, li, si)\u0001\n,\nro(x, c p,{Pi}No\ni=1) = \u03a3No\ni=1\u0000\n1\u2212D o(X(x,obji), oi)\u0001\n,\nr(x, c p,{Pi}No\ni=1) =\u03b3r ls(x, c p,{Pi}No\ni=1) +\u03bbr o(x, c p,{Pi}No\ni=1),(3)\nwhere higher rlsrepresents better accuracy of location and size. For rls, we use the advanced\ndetection model [ 31] to estimate the 2D bounding box, and compute its Intersection over Union\n(IoU) with the projected one from 3D bounding box. On the other hand, roassesses the orientation\nprecision. For X, we crop the object area and feed it to Orient Anything [ 62]. Then, Docalculates the\nKL divergence between the target distribution and the estimated distribution from Orient Anything.\nThe final reward ris calculated by combining rls, rothrough weighting factors \u03b3, \u03bb. Consequently,\nthe objective of the second stage is to minimize:\nE(cp,{Pi}No\ni=1)\u223cB,x\u223cp \u03b8(\u00b7|cp,{Pi}No\ni=1)h\n\u2212\u03b2r(x, c p,{Pi}No\ni=1) +L priori\n,(4)\nwhere text prompts and CNOCS maps are sampled from constructed balanced data distribution\nB, and Lprior is calculated as Eq. 1 for stabilizing the training [ 65]. However, naively training\nwith Eq. (4) leads to huge GPU memory consumption for backpropagation throughout multi-step\ndenoising process, which is infeasible. To reduce memory footprint, we leverage randomized\ntruncated backpropagation and gradient checkpointing like in AlignProp [ 42], while feeding the\ncoarse image estimated from intermediate step to the reward function instead of the clean one. The\nwhole pipeline is presented in Algorithm 2. Through RL finetuning, the model outperforms in pose\nalignment with the input condition. More details about RL finetuning are provided in Sec. A.2.\nInference:After two-stage training, the model can effectively control the 9D pose of arbitrary object.\nHowever, the model fails to associate each object with its pose from CNOCS map in multi-object\nscenarios, since there is no restriction in constructing the correspondences. Furthermore, insufficient\ngeneration and attribute leakage often occur in generating multiple concepts. Therefore, we propose\n6\nAlgorithm 1:Algorithm pipeline ofDisentangled Object Sampling.\nInput:Initial noise \u03f5sampled from N(0,I) ; text prompt cpconsisting of entity names {obji}No\ni=1;CNOCS\nmap for each object{P i}No\ni=1; CNOCS map of the whole sceneP global ; sampling stepT;\nx0=\u03f5;\nfortin{0,1. . . , T\u22121}do\nvt=v\u03b8(xt, t, cp,Ptotal);\nxt+1=xt+vtdt;\nforiin{1, . . . , N o}do\nObtain object maskM ifrom CNOCS mapP i;\nvi\nt=v\u03b8(xt, t,obji,Pi);/\u2217it can be computed in parallel withv t\u2217/\nxi\nt+1=xt+vi\ntdt;\nxt+1= (1\u2212M i)xt+1+M ixi\nt+1;\nx=x T;\nReturn:The generated imagex.\nDisentangled Object Samplingto alleviate the challenges, and the process is shown in Algorithm 1.\nEssentially, we combine multiple noisy latents at each denoising step using region masks, where\neach latent is sampled based on global or object-specific condition. ThroughDisentangled Object\nSampling, the model is able to match each pose from CNOCS map with corresponding object.\nFurthermore, we can also load the personalized weights given by users and perform customized pose\ncontrol of reference subjects.\n4 Experiment\nImplementation details:The proposedSceneDesigneris based on Stable Diffusion 3.5 [ 10], training\nwith 6 NVIDIA A800 80G GPUs. We use AdamW optimizer with an initial learning rate of 5e\u22126.\nThe resolution is set to 512\u00d7512 with 48 batch size. In the first stage, the parameters \u03b8from the\nintroduced ControlNet are updated for 45K iterations in the proposedObjectPose9D. For the second\nstage, we further fine-tune the model with RL objective for 5K iterations. More details about training\nare provided in Sec. A.2. During inference, we only inject the conditions in initial 15steps during\nsampling with20denoising steps.\nValidation details:For validation metrics, we use mean Intersection over Union ( mIoU ) and spatial\naccuracy Acclsfor assessing the precision of location and size. Specifically, we use Grounding\nDINO [ 31] to detect generated objects. Acclsis calculated as\u03a3N\ni=1I(IoU i>0.6)\nN, where Iis indicator\nfunction and Nis total number of test cases. Similar to ORIGEN [ 34], we use the following\ntwo metrics for orientation evaluation:1) Abs.Err calculates the absolute error of azimuth angles\n(in degrees) between the input condition and the estimated one from Orient Anything [ 62] and\n2)Acc.@22.5\u25e6measures the accuracy with 22.5\u25e6tolerance. Furthermore, CLIP [ 44] is used to\nestimate the text-image alignment and FID presents visual quality. Specifically, we randomly sample\nthe reference images from LAION [ 48] to calculate FID. In addition, a user study is also conducted\nto complement the evaluation based on human preferences. For validation dataset, we introduce\ntwo benchmarks to assess the model performance in pose control of single-object and multi-object\nscenarios, namedObjectPose-SingleandObjectPose-Multi, which are obtained by estimating the\n9D poses from validation part of COCO [ 27] as in Sec. 3.4. Among them,ObjectPose-Singleis\nfurther divided intoObjectPose-Single-FrontandObjectPose-Single-Backfor assessing the orientation\naccuracy in front- and back-facing scenarios, containing 247 and 156 samples, respectively. Besides,\nObjectPose-Multiincludes 229 cases.\n4.1 Comparisons with State-of-the-Art Methods\nEvaluation in single-object generation:This experiment evaluates the capability of single-object\npose control. Although Zero-1-to-3 [ 30] exhibits considerable orientation controllability, it depends\non the reference image from users and exhibits poor generalization in real-world images. Furthermore,\nsince the codes of other relevant methods [ 34,40] were not open-sourced at the time of our experiment,\nthey are also not discussed in our experiment. Consequently, we choose LOOSECONTROL (LC) [ 6]\nand Continuous 3D Words (C3DW) [ 8] as compared T2I methods due to their abilities for 3D-\n7\nTable 1: Quantitative evaluation of pose alignment in multiple benchmarks.\nBenchmark MethodLocation&Size Alignment Orientation Alignment\nAccls(%)\u2191mIoU (%)\u2191 Abs.Err\u2193Acc@22.5\u25e6(%)\u2191\nObjectPose-Single-FrontC3DW [8] 2.02 19.61 50.01 60.32\nLOOSECONTROL [6] 23.89 27.12 87.26 23.08\nSceneDesigner(ours) 50.20 57.21 13.23 89.47\nObjectPose-Single-BackLOOSECONTROL 24.36 30.49 132.26 7.05\nSceneDesigner(ours) 52.56 60.66 17.47 83.33\nObjectPose-MultiLOOSECONTROL 14.85 22.58 147.42 4.80\nSceneDesigner(ours) 47.16 52.16 23.14 80.79\nTable 2: Comparisons of visual quality and text alignment.\nC3DW [8] LOOSECONTROL [6]SceneDesigner(Ours)\nFID\u2193 67.39 37.89 24.91\nCLIP\u2191 0.267 0.293 0.345\nA dog plays on the beach\nA soldier on the desertA green sofa against brick wall\noursLCC3DW(a) Single-object Pose Control(b) Multi-object Pose ControlA catand a teddy bearA turtleand a penguinA boat, an eagle, and a planeA cat, a car, and a sofa\nAn elephant on the grassland\nN/AN/ACondition\n12\n12121233\nN/AN/AN/A\nFigure 5: Evaluation of 9D pose control in single- and multi-object scenarios.SceneDesigner\noutperforms other methods in both fidelity and quality under various pose conditions.\naware control, while converting the pose condition into their required formats. As shown in the left\nof Fig. 5, the compared methods exhibit limited ability to control the location, size, and orientation\nsimultaneously. Although LC [ 6] can control the location and size of the object effectively, the results\npresent arbitrary orientation with poor quality and fidelity. On the other hand, C3DW is suffered\nby invariant image layout and unrealistic visual content. Besides, it can only process the front 180\u25e6\nrange of azimuths. As a result, the method lacks the controllability in more complicated orientations\n(e.g., back-facing case in column 4 of Fig. 5), and is unable to locate the object with a designated size.\nIn contrast,SceneDesigneroutperforms in both fidelity and quality. The quantitative results in Tab. 1\nalso demonstrate the superior performance of our method in pose alignment. As demonstrated by\nAcclsandmIoU metrics in Tab. 1,SceneDesigneroutperforms LC in controlling spatial location\nand size of the object, while C3DW exhibits poor precision since the objects are typically centered\nin the image. For orientation, C3DW achieves higher performance than LC, since the latter does\nnot encode the orientation properties. However it cannot generate back-facing object, and suffers\nfrom the low-quality contents as indicated by the FID metric in Tab. 2. On the other hand, our\nmethod achieves considerable performances in both front- and back-facing settings with the help of\nefficient representation and the two-stage training strategy, while outperforming in both quality and\ntext alignment.\nEvaluation in multi-object generation:We also consider the pose control of multiple objects in the\nsame scene. Since C3DW can only handle single object generation, LC is chosen as the compared\nmethod in this setting. As shown in the right of Fig. 5, it demonstrates poor performance in both\n8\nTable 3: Ablation studies.\nBenchmark SettingLocation&Size Alignment Orientation Alignment\nAccls(%)\u2191mIoU (%)\u2191 Abs.Err\u2193Acc@22.5\u25e6(%)\u2191\nObjectPose-Singlew/o MS-COCO [27] 41.69 50.07 74.89 24.32\nw/o RL finetuning 43.18 50.32 43.85 52.36\nw/ C-CNOCS map 40.45 49.86 37.86 73.70\nw/ Pose embedding 32.51 40.73 49.65 47.15\nprompt only 12.90 14.32 88.43 25.31\nSceneDesigner(ours) 51.12 58.55 14.87 87.10\nObjectPose-Multiw/oDOS 36.68 45.91 42.92 59.39\nSceneDesigner(ours) 47.16 52.16 23.14 80.79\nTable 4: Human preferences in quality, pose fidelity, and text-to-image alignment.\nMethod C3DW [8] LOOSECONTROL [6]SceneDesigner\nImage quality 0.47 0.64 0.96\nLocation fidelity 0.05 0.88 0.98\nSize fidelity 0.02 0.82 0.96\nOrientation fidelity 0.62 0.390.91\nText-to-image alignment 0.63 0.510.94\norientation and semantic fidelity, while suffering from insufficient generation and concept confusion.\nAs a result, the method cannot precisely identify the concept inside each 3D bounding box. In\ncomparison,SceneDesignerleveragesDisentangled Object Samplingduring inference to mitigate the\nentangled generation, while maintaining the pose controllability. The quantitative results from Tab. 1\nalso demonstrate thatSceneDesignermaintains performance comparable to single-object scenarios\nwhen handling multiple objects, outperforming other methods in all metrics.\nUser Study:We additionally conduct a user study to assess the methods in the following aspects:\nimage quality,pose fidelityandtext-to-image alignment. Tab. 4 demonstrates the evaluation results\nfrom human beings, where the scores are normalized and the higher value indicates better performance.\nSpecifically, we employed 20 volunteers to evaluate outcomes from each method. For each score, we\naverage the results and normalize it by the maximum value.\nw/o RL Finetuningw/o MS-COCOOursw/o DOSA wolfand a pig\nw/ C-CNOCS MapOursA rabbit in the forest\nRabbit, catand dogOursw/o DOS12312\nFigure 6: Ablation studies. The left indicates the effects ofObjectPose9D, two-stage training strategy\nand I-CNOCS map. The right examples show the impact ofDisentangled Object Sampling(DOS).\n4.2 Ablation Studies\nWe conduct comprehensive ablation studies to validate the effectiveness of each component in our\nproposedSceneDesigner. The quantitative results are summarized in Tab. 3.\nPose Conditioning:To validate our proposed CNOCS map, we compare it with three alternative\nconditioning strategies: 1) a C-CNOCS map that assigns constant Euler angles to the object region;\n2) a variant that directly injects 9D pose embeddings via attention; and 3) a training-free baseline\n(prompt only) that converts the pose into a textual description based on templates. As shown in Tab. 3,\nall three baselines underperform, confirming the superiority of our representation.\n9\nDataset and Training Strategy:We then analyze the impact of our dataset and two-stage training\nstrategy. First, we train a model using only OmniNOCS data. As shown in Fig. 6 (left) and Tab. 3,\nthe limited categories in OmniNOCS lead to poor generalization on unseen classes (e.g., rabbit) and\na significant performance drop. Second, we evaluate the model checkpoint from the first stage, prior\nto RL finetuning. This model fails to generate back-facing objects, and its orientation accuracy is\nconsiderably lower. These comparisons confirm the importance of both our diverseObjectPose9D\nand the RL finetuning stage.\nMulti-Object Generation withDOS:For multi-object generation scenarios, we assess the effect\nof the proposedDOS. The results in the right part of Fig. 6 and Tab. 3 highlight its effectiveness in\nmitigating concept confusion and ensuring each object correctly corresponds to its specified pose in\nthe CNOCS map.\n5 Limitations and Impacts\n5.1 Limitations\nAlthoughSceneDesignerachieves high-fidelity pose control, it cannot control the precise shape\nof the object. Furthermore, the performance in the multi-object scenario is constrained by the\ninherent capability of the base model. As mentioned in previous literature, increasing the number of\nsemantic concepts in text prompt exacerbates insufficient generation and attribute leakage. While our\nDisentangled Object Samplingtechnique mitigates this issue, it introduces additional computation.\nTherefore, we will explore how to enhance the alignment with conditions in multi-object generation\nwhile maintaining computational efficiency in our future work.\n5.2 Social Impacts\nPositive societal impacts:SceneDesignersupports users with ability in multi-object 9D pose control\nwithout extensive resources or professional equipment, while providing customized pose control\nof user-provided subjects. This capability proves particularly valuable for applications like virtual/\naugmented reality and product design, where spatial control is essential.\nPotential negative societal impacts:Multi-object 9D pose control raises concerns about potential\nmisuse for generating deceptive content in sensitive areas such as political manipulation and social\nmedia, where inaccurate poses could spread misinformation. Without proper safeguards, such as\ndetection methods, ethical guidelines, and public awareness, this technique could be exploited to\nundermine the trust in digital media.\nMitigation strategies:Developing and adhering to strict ethical guidelines for multi-object 9D\npose control technologies helps mitigate misuse risks. This includes enforcing usage restrictions for\nsensitive generation and ensuring transparency in generated outputs through traceability measures.\n6 Conclusion\nWe introduceSceneDesignerthat achieves multi-object 9D pose control within the same scene.\nOur key insight is introducing CNOCS map to encode the 9D pose of general objects, preserving\nthe 3D properties and exhibiting high geometry interpretation. This representation accelerates the\nconvergence and facilitates to provide a user-friendly interface for creating 3D-aware conditions. For\nlearning the newly added control modules, we resort to the public dataset with comprehensive 9D\npose annotations, while enhancing the diversity through annotating large-scale datasets. To alleviate\nthe poor performance in low-frequency poses caused by imbalanced data distribution, we introduce\na two-stage training process, where the second stage maximizes the proposed reward function to\nenhance the alignment of object pose with input conditions. During inference, ourDisentangled\nObject Samplingassociates each object with the corresponding pose condition, avoiding the confusion\nin multi-object generation. Furthermore, our method can also achieve customized pose control of\nreference subjects given by users. Finally, qualitative and quantitative experiments demonstrate that\nour method outperforms existing methods in both single- and multi-object scenarios.\nAcknowledgement.This project was supported by the National Natural Science Foundation of China\n(NSFC) under Grant No. 62472104.\n10\nReferences\n[1]Adel Ahmadyan, Liangkai Zhang, Artsiom Ablavatski, Jianing Wei, and Matthias Grundmann. Objectron:\nA large scale dataset of object-centric videos in the wild with pose annotations. InCVPR, 2021.\n[2]Yuval Alaluf, Elad Richardson, Gal Metzer, and Daniel Cohen-Or. A neural space-time representation for\ntext-to-image personalization.ACM TOG, 2023.\n[3]Michael S Albergo, Nicholas M Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: A unifying\nframework for flows and diffusions.arXiv, 2023.\n[4]Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-Or, and Dani Lischinski. Break-a-scene:\nExtracting multiple concepts from a single image. InSIGGRAPH, 2023.\n[5]Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie\nWang, Jun Tang, et al. Qwen2. 5-vl technical report.arXiv, 2025.\n[6]Shariq Farooq Bhat, Niloy J. Mitra, and Peter Wonka. LOOSECONTROL: lifting controlnet for generalized\ndepth conditioning. InSIGGRAPH, 2024.\n[7]Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models\nwith reinforcement learning.arXiv, 2023.\n[8]Ta-Ying Cheng, Matheus Gadelha, Thibault Groueix, Matthew Fisher, Radomir Mech, Andrew Markham,\nand Niki Trigoni. Learning continuous 3d words for text-to-image generation. InCVPR, 2024.\n[9]Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson,\nUwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding.\nInCVPR, 2016.\n[10] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M\u00fcller, Harry Saini, Yam Levi,\nDominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution\nimage synthesis. InICML, 2024.\n[11] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mo-\nhammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for fine-tuning\ntext-to-image diffusion models.NeurIPS, 2023.\n[12] Xiao Fu, Xian Liu, Xintao Wang, Sida Peng, Menghan Xia, Xiaoyu Shi, Ziyang Yuan, Pengfei Wan,\nDi Zhang, and Dahua Lin. 3dtrajmaster: Mastering 3d trajectory for multi-entity motion in video generation.\nInICLR, 2025.\n[13] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel\nCohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In\nICLR, 2023.\n[14] Shuting He, Peilin Ji, Yitong Yang, Changshuo Wang, Jiayi Ji, Yinglin Wang, and Henghui Ding. A\nsurvey on 3d gaussian splatting applications: Segmentation, editing, and generation.arXiv preprint\narXiv:2508.09977, 2025.\n[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. InNeurIPS, 2020.\n[16] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. LoRA: Low-rank adaptation of large language models.ICLR, 2022.\n[17] Minghui Hu, Jianbin Zheng, Daqing Liu, Chuanxia Zheng, Chaoyue Wang, Dacheng Tao, and Tat-Jen\nCham. Cocktail: Mixing multi-modality controls for text-conditional image generation. InNeurIPS, 2023.\n[18] Yuzhong Huang and Fred Morstatter. Orientdream: Streamlining text-to-3d generation with explicit\norientation control. InICASSP, 2025.\n[19] Zeyinzi Jiang, Chaojie Mao, Yulin Pan, Zhen Han, and Jingfeng Zhang. SCEdit: Efficient and controllable\nimage diffusion generation via skip connection editing. InCVPR, 2024.\n[20] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00fchler, and George Drettakis. 3d gaussian splatting for\nreal-time radiance field rendering.ACM Trans. Graph., 2023.\n[21] Akshay Krishnan, Abhijit Kundu, Kevis-Kokitsi Maninis, James Hays, and Matthew Brown. OmniNOCS:\nA unified nocs dataset and model for 3d lifting of 2d objects. InECCV, 2024.\n11\n[22] Nupur Kumari, Grace Su, Richard Zhang, Taesung Park, Eli Shechtman, and Jun-Yan Zhu. Customizing\ntext-to-image diffusion with object viewpoint control. InSIGGRAPH, 2024.\n[23] Pengxiang Li, Kai Chen, Zhili Liu, Ruiyuan Gao, Lanqing Hong, Guo Zhou, Hua Yao, Dit-Yan Yeung,\nHuchuan Lu, and Xu Jia. TrackDiffusion: Tracklet-conditioned video generation via diffusion models. In\nWACV, 2023.\n[24] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and\nYong Jae Lee. GLIGEN: open-set grounded text-to-image generation. InCVPR, 2023.\n[25] Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, Ming-Ming Cheng, and Ying Shan. PhotoMaker:\nCustomizing realistic human photos via stacked ID embedding.CVPR, 2023.\n[26] Ziye Li, Hao Luo, Xincheng Shuai, and Henghui Ding. AnyI2V: Animating any conditional image with\nmotion control. InICCV, 2025.\n[27] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r,\nand C. Lawrence Zitnick. Microsoft COCO: common objects in context. InECCV, 2014.\n[28] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for\ngenerative modeling.arXiv, 2022.\n[29] Chang Liu, Xiangtai Li, and Henghui Ding. Referring image editing: Object-level image editing via\nreferring expressions. InCVPR, pages 13128\u201313138, 2024.\n[30] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl V ondrick.\nZero-1-to-3: Zero-shot one image to 3d object. InICCV, 2023.\n[31] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li,\nJianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set\nobject detection. InECCV, 2024.\n[32] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer\ndata with rectified flow.arXiv, 2022.\n[33] Oscar Michel, Anand Bhattad, Eli VanderBilt, Ranjay Krishna, Aniruddha Kembhavi, and Tanmay Gupta.\nOBJECT 3DIT: Language-guided 3d-aware image editing. InNeurIPS, 2023.\n[34] Yunhong Min, Daehyeon Choi, Kyeongmin Yeo, Jihyun Lee, and Minhyuk Sung. ORIGEN: Zero-shot 3d\norientation grounding in text-to-image generation.arXiv, 2025.\n[35] Sicheng Mo, Fangzhou Mu, Kuan Heng Lin, Yanli Liu, Bochen Guan, Yin Li, and Bolei Zhou. FreeControl:\nTraining-free spatial control of any text-to-image diffusion model with any condition. InCVPR, 2024.\n[36] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2I-\nAdapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In\nAAAI, 2024.\n[37] Weili Nie, Sifei Liu, Morteza Mardani, Chao Liu, Benjamin Eckart, and Arash Vahdat. Compositional\ntext-to-image generation with dense blob representations. InICML, 2024.\n[38] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\nhuman feedback.NeurIPS, 2022.\n[39] Karran Pandey, Paul Guerrero, Matheus Gadelha, Yannick Hold-Geoffroy, Karan Singh, and Niloy J. Mitra.\nDiffusion handles enabling 3d edits for diffusion models by lifting activations to 3d. InCVPR, 2024.\n[40] Rishbuh Parihar, Vaibhav Agrawal, Sachidanand VS, and R Venkatesh Babu. Compass Control: Multi\nobject orientation control for text-to-image generation. InCVPR, 2025.\n[41] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna,\nand Robin Rombach. SDXL: improving latent diffusion models for high-resolution image synthesis.arXiv,\n2023.\n[42] Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, and Katerina Fragkiadaki. Aligning text-to-image\ndiffusion models with reward backpropagation.arXiv, 2023.\n12\n[43] Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang, Juan Carlos Niebles,\nCaiming Xiong, Silvio Savarese, Stefano Ermon, Yun Fu, and Ran Xu. UniControl: A unified diffusion\nmodel for controllable visual generation in the wild. InNeurIPS, 2023.\n[44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning\ntransferable visual models from natural language supervision. InICML, 2021.\n[45] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn.\nDirect preference optimization: Your language model is secretly a reward model.NeurIPS, 2023.\n[46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution\nimage synthesis with latent diffusion models. InCVPR, 2022.\n[47] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dream-\nbooth: Fine tuning text-to-image diffusion models for subject-driven generation. InCVPR, 2023.\n[48] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush\nKatta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. LAION-400M: open dataset of clip-filtered\n400 million image-text pairs.arXiv, 2021.\n[49] Xincheng Shuai, Henghui Ding, Xingjun Ma, Rongcheng Tu, Yu-Gang Jiang, and Dacheng Tao. A survey\nof multimodal-guided image editing with text-to-image diffusion models.arXiv, 2024.\n[50] Xincheng Shuai, Henghui Ding, Zhenyuan Qin, Hao Luo, Xingjun Ma, and Dacheng Tao. Free-Form\nMotion Control: Controlling the 6d poses of camera and objects in video generation. InICCV, 2025.\n[51] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In\nNeurIPS, 2019.\n[52] Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable approach to\ndensity and score estimation. InUAI, 2019.\n[53] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano\nErmon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference\noptimization. InCVPR, 2024.\n[54] He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, and Leonidas J Guibas. Normal-\nized object coordinate space for category-level 6d object pose and size estimation. InCVPR, 2019.\n[55] Jiawei Wang, Yuchen Zhang, Jiaxin Zou, Yan Zeng, Guoqiang Wei, Liping Yuan, and Hang Li. Boximator:\nGenerating rich and controllable motions for video synthesis.arXiv, 2024.\n[56] Mengyu Wang, Henghui Ding, Jianing Peng, Yao Zhao, Yunpeng Chen, and Yunchao Wei. CharaConsist:\nFine-grained consistent character generation. InICCV, 2025.\n[57] Qinghe Wang, Yawen Luo, Xiaoyu Shi, Xu Jia, Huchuan Lu, Tianfan Xue, Xintao Wang, Pengfei Wan,\nDi Zhang, and Kun Gai. Cinemaster: A 3d-aware and controllable framework for cinematic text-to-video\ngeneration. InSIGGRAPH, 2025.\n[58] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, and Anthony Chen. InstantID: Zero-shot identity-\npreserving generation in seconds.arXiv, 2024.\n[59] Ruicheng Wang, Jianfeng Xiang, Jiaolong Yang, and Xin Tong. Diffusion models are geometry critics:\nSingle image 3d editing using pre-trained diffusion priors. InECCV, 2024.\n[60] Ruicheng Wang, Sicheng Xu, Cassie Dai, Jianfeng Xiang, Yu Deng, Xin Tong, and Jiaolong Yang.\nMoGe: Unlocking accurate monocular geometry estimation for open-domain images with optimal training\nsupervision. InCVPR, 2025.\n[61] Xudong Wang, Trevor Darrell, Sai Saketh Rambhatla, Rohit Girdhar, and Ishan Misra. Instancediffusion:\nInstance-level control for image generation. InCVPR, 2024.\n[62] Zehan Wang, Ziang Zhang, Tianyu Pang, Chao Du, Hengshuang Zhao, and Zhou Zhao. Orient Anything:\nLearning robust object orientation estimation from rendering 3d models.arXiv, 2024.\n[63] Ziyi Wu, Yulia Rubanova, Rishabh Kabra, Drew A. Hudson, Igor Gilitschenski, Yusuf Aytar, Sjoerd van\nSteenkiste, Kelsey R. Allen, and Thomas Kipf. Neural Assets: 3d-aware multi-object scene synthesis with\nimage diffusion models. InNeurIPS, 2024.\n13\n[64] Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian Zhang, Yefeng Zheng, and Mike Zheng\nShou. Boxdiff: Text-to-image synthesis with training-free box-constrained diffusion. InICCV, 2023.\n[65] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong.\nImagereward: Learning and evaluating human preferences for text-to-image generation.NeurIPS, 2023.\n[66] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. IP-Adapter: Text compatible image prompt adapter\nfor text-to-image diffusion models.arXiv, 2023.\n[67] Jiraphon Yenphraphai, Xichen Pan, Sainan Liu, Daniele Panozzo, and Saining Xie. Image Sculpting:\nPrecise object editing with 3d geometry control. InCVPR, 2024.\n[68] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion\nmodels. InICCV, 2023.\n[69] Qihang Zhang, Yinghao Xu, Chaoyang Wang, Hsin-Ying Lee, Gordon Wetzstein, Bolei Zhou, and Ceyuan\nYang. 3DitScene: Editing any scene via language-guided disentangled gaussian splatting.arXiv, 2024.\n[70] Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, and Kwan-Yee K.\nWong. Uni-ControlNet: All-in-one control to text-to-image diffusion models. InNeurIPS, 2023.\n14\nA Technical Appendices and Supplementary Material\nOverview:The supplementary includes the following sections:\n\u2022A.1.Details of dataset.\n\u2022A.2.Details of method.\n\u2022A.3.More comparisons and analyses.\n\u2022A.4.More ablation studies.\nA.1 Details of Dataset\nAs the base of our datasetObjectPose9D, we select Objectron [ 1] and Cityscapes [ 9] subsets from\nOmniNOCS [ 21], sampling around 110,000 images with comprehensive pose annotations of interesting instances.\nFurthermore, we enlarge the category diversity and scene variations through introducing additional data from\nMS-COCO [ 27], and leverage the approach in Sec. 3.4 to obtain pose annotations for suitable objects, obtaining\naround 65,000 samples. Concretely, we select objects whose sizes range from 10% to 70% of the image size,\nand further exclude those with prediction confidence [ 62] below 0.8. For estimation of 3D bounding boxes,\nthe farthest 10% of point clouds from the object centroid are discarded. In addition, Qwen2.5-VL-7B [ 5] is\nused to describe the visual content. To maintain high-quality training data, human annotators further filter\nthe low-quality images and rectify the inaccurate annotation. Finally,ObjectPose9Dcontains totally 125,486\ntraining data.\nA.2 Details of Method\nDifferences with related methods:There exists significant differences between our method and related\napproaches.1) Controllability of 9D poses.To the best of our knowledge,SceneDesigneris the first method\nthat can achieve multi-object 9D pose control. For example, LOOSECONTROL [ 6] that receives 3D bounding\nboxes is unaware of object orientation, and the results are significantly influenced by model priors. Other\nmethods [ 40,8] like Continuous 3D Words [ 8](C3DW) can only control the front 180\u25e6azimuth angles, and lack\nflexibility in scene composition, typically producing rigid arrangements. The recent work ORIGEN [ 34] is able\nto achieve intricate orientation control, but limited in controlling the location and size.2) Dataset and training\nstrategy.The high quality training samples and efficient learning strategy makeSceneDesigneroutperform other\nmethods in terms of quality and fidelity. LOOSECONTROL with single-stage learning performs poor alignment\nwith conditions in some complicated scenarios. Methods [ 40,8] trained on synthetic dataset exhibit inferior\ngeneralization ability in generating real-world images, thus having poor image quality. The training-free method\nORIGEN optimizes the initial noise to maximize the introduced reward function, aligning the object orientation\nwith input condition. However, it is only applicable to one-step generative models, hindering its application.\nDetails about RL finetuning:Constructing a balanced pose distribution for each object category from existing\ndata sources [ 27,48] is impractical. Therefore, RL finetuning discussed in Sec. 3.5 is introduced to alleviate\nthe challenge. In our implementation, we set \u03b3and\u03bbin Eq. (3) to 0and1, respectively, since our experiment\nshows that Lprior effectively maintains control over both location and size. \u03b2in Eq. (4) is set to 5e\u22123to\navoid overfitting. The dataset Bfrom Eq. (4) constructs balanced pose distribution for each considered object\ncategory. An ideal approach is to randomly place multiple cuboids of arbitrary sizes and orientations in the\nspace, while generating corresponding CNOCS maps. However, our experiments demonstrate that this leads to\nslow convergence. Consequently, we only consider single-object scenarios. In practice, the model trained under\nthis setting demonstrates strong generalization capability in multi-object scenarios throughDisentangled Object\nSamplingtechnique. We get totally 20,000 CNOCS maps with different location, size, and orientation. Besides,\nwe create 1200 descriptions of interesting object categories (e.g.animal) by querying MLLM [ 5]. Then, we\ncan independently sample the text prompt and CNOCS map to construct balanced pose distributions for rich\nobject categories. The pipeline of RL finetuning is illustrated in Algorithm 2. We optimize the whole parameters\nfrom ControlNet with truncation length Kset to 2. The denoising steps are sampled from uniform distribution\nU(T min, Tmax), whereT min, Tmaxare set to6and16.\nA.3 More Comparisons and Analyses\nMore qualitative results:To illustrate the flexibility of our method in 9D pose control, Fig. 7 presents the results\nunder various pose conditions. As shown in the figures, the poses of the generated objects exhibit strong fidelity\nwith conditions. Besides, Fig. 8 and Fig. 9 present more qualitative comparisons in single- and multi-object\nscenarios with intricate pose conditions. These results also demonstrate thatSceneDesigneroutperforms other\nmethods in terms of fidelity and image quality. Fig. 10 further presents more qualitative results of our method in\nmulti-object generation. We also provide examples in Fig. 11 to present the model\u2019s capability of customized\npose control. Specifically, we use images from DreamBooth [ 47] and learn LoRA [ 16] weights for each subject.\n15\nA trendy cartoon bear near waterfall\nA bronze horse statue in excavated historical site\nA monkey sitting on a fruit-filled cave\nA capybara stands by river\nA rhino covered with moss stands against clock tower\nA tiger on a snowy taiga forest\nFigure 7: 9D pose control under various pose conditions.\nA teddy bear sitting on top of suitcases\nA dog sits by fenceA toy astronaut bear stands among cookiesA white SUV on a winding gravel roadAn orange motorcycle parked in desert\noursLCC3DWN/ACondition\nFigure 8: Evaluation of 9D pose control in single-object scenarios.\n16\nA boyand a girlsit facing the water\nA Minionand a catface each other on a streetA personstands beside a white carA catand a dognear cherry blossom treesIron Man stands on a whaleover the ocean\noursLCCondition1212121212Figure 9: Evaluation of 9D pose control in multi-object scenarios.\nPenguin, polar bear and boyin snowy landscapeHulk, Spider Manand Deadpoolin city ruinsGolden cat, shoe, and cameraon tableCat, sofa, and carin grassy fieldTurtleand penguinon a map, planein the sky123123123123123\nFigure 10: More generation results ofSceneDesignerin multi-object scenarios.\n17\nAlgorithm 2:Algorithm pipeline of RL finetuning.\nInput:The proposed datasetObjectPose9DDand constructed datasetBfor RL finetuning; truncation\nlengthK; the range of denoising steps[T min, Tmax]during sampling; the number of training epochsN E;\nweighting factor\u03b2of reward function;\nforepochin{1. . . , N E}do\nGet samples fromDand update the network parameters\u03b8through Eq. (1);\nSample the CNOCS map that encodes{P i}No\ni=1, andc pfromB;\nObtain the initial noise\u03f5sampled fromN(0,I);\nObtain the sampling stepsT 1from uniform distributionU(T min, Tmax);\nSample the stepT 0from uniform distributionU(T 1\u2212K, T 1\u22121), which begins gradient calculation;\nx0=\u03f5;\nfortin{0, . . . , T 0\u22121}do\nno grad:x t+1=xt+v\u03b8(xt, t, cp,{Pi}No\ni=1)dt;\nfortin{T 0, . . . , T 1\u22121}do\nwith grad:x t+1=xt+v\u03b8(xt, t, cp,{Pi}No\ni=1)dt;\n\u02c6x=xT1\u2212T1\u03f5\n1\u2212T 1;\nCalculate the gradient towards\u2212\u03b2r(\u02c6x, c p,{Pi}No\ni=1)defined in Eq. (3) and update the\u03b8;\nReturn:The network parameters\u03b8.\nTable 5: Ablation studies.\nSettingLocation&Size Alignment Orientation Alignment\nAccls(%)\u2191mIoU (%)\u2191 Abs.Err\u2193Acc@22.5\u25e6(%)\u2191\nw/oL prior 26.55 31.76 28.79 76.18\nSceneDesigner(ours) 51.12 58.55 14.87 87.10\nA.4 More Ablation Studies\nWe conduct additional experiments to demonstrate the impacts of critical techniques from our method. Beyond\nthe settings discussed in the main paper, we further validate the effectiveness of Lprior loss in Eq. (4). Tab. 5\nand qualitative results in Fig. 12 indicate that the model learned without Lprior suffers from reward hacking\nand quality degradation under the same training configuration. Furthermore, Fig. 13 shows the impacts of our\nDisentangled Object Sampling(DOS).\n18\n\u2026intheforest\u2026inthemeadow\u2026near the river\n\u2026intheroof\u2026onthestone\u2026in snowy field\nFigure 11: Customized 9D pose control.\nw/o RL Finetuningw/ C-CNOCS Mapw/o MS-COCOOursw/o \ud835\udc3f!\"#$\"A gorilla in meteor craterA sheep in beach\nFigure 12: Ablation study inObjectPose9D, two-stage training strategy and I-CNOCS map. The\nmodel learned withoutL prior suffers from overfitting and quality degradation.\nA carand a bus1212Oursw/o DOSCat, a duckand a carOursw/o DOS3\nFigure 13: Ablation study inDisentangled Object Sampling(DOS). Our method alleviates the\ninsufficient generation and concept fusion with the help ofDOS.\n19",
      "summary": "SceneDesigner is a novel method for controllable multi-object image generation, enabling precise manipulation of each object's 9-DoF pose (location, size, orientation) within a scene. The approach introduces a new pose representation and training strategies to achieve superior controllability and image quality compared to prior methods.",
      "key_contributions": [
        "Introduces SceneDesigner, a method for accurate and flexible multi-object 9-DoF pose manipulation in image generation.",
        "Proposes the CNOCS map, a novel representation encoding 9D pose information from the camera view for improved geometric interpretability.",
        "Constructs ObjectPose9D, a new dataset with diverse images and 9D pose annotations to support training.",
        "Develops a two-stage training strategy with reinforcement learning to address data imbalance and improve performance on rare poses.",
        "Presents Disentangled Object Sampling and user-specific personalization for enhanced object control and customization."
      ],
      "methodology": "SceneDesigner augments a pre-trained base model with a branched network and CNOCS map representation, trains on the ObjectPose9D dataset using a two-stage reinforcement learning approach, and applies Disentangled Object Sampling at inference for robust multi-object pose control.",
      "results": "Experiments show that SceneDesigner achieves significantly better controllability and image quality for multi-object 9-DoF pose manipulation than existing methods, as demonstrated by both qualitative and quantitative evaluations.",
      "keywords": [
        "controllable image generation",
        "multi-object manipulation",
        "9-DoF pose",
        "CNOCS map",
        "reinforcement learning",
        "dataset construction",
        "personalization"
      ],
      "status": "new",
      "notes": null,
      "created_at": "2025-11-23T20:38:07.300234",
      "needs_llm_processing": false
    },
    "10": {
      "id": "2511.16662v1",
      "title": "TriDiff-4D: Fast 4D Generation through Diffusion-based Triplane Re-posing",
      "authors": [
        {
          "name": "Eddie Pokming Sheung",
          "affiliation": null
        },
        {
          "name": "Qihao Liu",
          "affiliation": null
        },
        {
          "name": "Wufei Ma",
          "affiliation": null
        },
        {
          "name": "Prakhar Kaushik",
          "affiliation": null
        },
        {
          "name": "Jianwen Xie",
          "affiliation": null
        },
        {
          "name": "Alan Yuille",
          "affiliation": null
        }
      ],
      "abstract": "With the increasing demand for 3D animation, generating high-fidelity, controllable 4D avatars from textual descriptions remains a significant challenge. Despite notable efforts in 4D generative modeling, existing methods exhibit fundamental limitations that impede their broader applicability, including temporal and geometric inconsistencies, perceptual artifacts, motion irregularities, high computational costs, and limited control over dynamics. To address these challenges, we propose TriDiff-4D, a novel 4D generative pipeline that employs diffusion-based triplane re-posing to produce high-quality, temporally coherent 4D avatars. Our model adopts an auto-regressive strategy to generate 4D sequences of arbitrary length, synthesizing each 3D frame with a single diffusion process. By explicitly learning 3D structure and motion priors from large-scale 3D and motion datasets, TriDiff-4D enables skeleton-driven 4D generation that excels in temporal consistency, motion accuracy, computational efficiency, and visual fidelity. Specifically, TriDiff-4D first generates a canonical 3D avatar and a corresponding motion sequence from a text prompt, then uses a second diffusion model to animate the avatar according to the motion sequence, supporting arbitrarily long 4D generation. Experimental results demonstrate that TriDiff-4D significantly outperforms existing methods, reducing generation time from hours to seconds by eliminating the optimization process, while substantially improving the generation of complex motions with high-fidelity appearance and accurate 3D geometry.",
      "arxiv_id": "2511.16662v1",
      "arxiv_url": "http://arxiv.org/abs/2511.16662v1",
      "pdf_url": "https://arxiv.org/pdf/2511.16662v1",
      "published": "2025-11-20T18:59:03Z",
      "updated": "2025-11-20T18:59:03Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "full_text": "TriDiff-4D: Fast 4D Generation through Diffusion-based Triplane Re-posing\nEddie Sheung1, Qihao Liu1, Wufei Ma1, Prakhar Kaushik1, Jianwen Xie2, Alan Yuille1\n1Johns Hopkins University2Lambda Inc\nFigure 1. TriDiff-4D is a novel 4D generative pipeline that enables high-quality, controllable 4D avatar generation from text using diffusion-\nbased triplane re-posing. By explicitly modeling 3D structure and motion priors within the diffusion model, learned from large-scale 3D\nand motion datasets, it produces anatomically accurate, motion-consistent, dynamic, and visually coherent avatars that generate 14 frames\nof 3D object sequences in just 36 seconds on a single H100 GPU.\nAbstract\nWith the increasing demand for 3D animation, generating\nhigh-fidelity, controllable 4D avatars from textual descrip-\ntions remains a significant challenge. Despite notable ef-\nforts in 4D generative modeling, existing methods exhibit\nfundamental limitations that impede their broader appli-\ncability, including temporal and geometric inconsistencies,\nperceptual artifacts, motion irregularities, high computa-\ntional costs, and limited control over dynamics. To ad-\ndress these challenges, we propose TriDiff-4D, a novel 4D\ngenerative pipeline that employs diffusion-based triplane\nre-posing to produce high-quality, temporally coherent 4D\navatars. Our model adopts an auto-regressive strategy to\ngenerate 4D sequences of arbitrary length, synthesizing\neach 3D frame with a single diffusion process. By explic-\nitly learning 3D structure and motion priors from large-\nscale 3D and motion datasets, TriDiff-4D enables skeleton-\ndriven 4D generation that excels in temporal consistency,\nmotion accuracy, computational efficiency, and visual fi-delity. Specifically, TriDiff-4D first generates a canoni-\ncal 3D avatar and a corresponding motion sequence from\na text prompt, then uses a second diffusion model to an-\nimate the avatar according to the motion sequence, sup-\nporting arbitrarily long 4D generation. Experimental re-\nsults demonstrate that TriDiff-4D significantly outperforms\nexisting methods, reducing generation time from hours to\nseconds by eliminating the optimization process, while sub-\nstantially improving the generation of complex motions with\nhigh-fidelity appearance and accurate 3D geometry.\n1. Introduction\nThe demand for realistic, expressive, and controllable\n3D avatars is rapidly growing across a range of do-\nmains, including gaming, and virtual and augmented reality\n(VR/AR). However, creating digital avatars that combine\nhigh visual fidelity with fine-grained control over movement\nand pose remains a complex challenge. Existing 3D/4D\ngeneration methods often depend on (multi-view) 2D priors\n1arXiv:2511.16662v1  [cs.CV]  20 Nov 2025\n[41] [46] [51], leading to a difficult trade-off between ap-\npearance quality and 3D geometric accuracy. A well-known\nissue in such approaches is the Janus problem [29], where\ngeometric inconsistencies across viewpoints can cause ar-\ntifacts such as characters displaying multiple faces when\nviewed from different angles. Building on the challenges of\n3D generation, 4D generation introduces additional com-\nplexity by requiring 3D content to evolve coherently over\ntime, demanding high motion realism and temporal con-\nsistency. For example, due to the inherent difficulty of\nsynchronizing motion patterns with structural deformations,\nexisting methods often suffer from temporal inconsistencies\nand unrealistic non-rigid deformations, commonly referred\nto as the \u201cjelly effect\u201d (as illustrated in Fig. 2) [33]. More-\nover, achieving realistic motion and visual detail remains\ndifficult, as it requires balancing geometric consistency and\ntexture fidelity across frames [2].\nTo resolve these fundamental difficulties, current 4D\ngeneration methods often rely on complex intermediate rep-\nresentations such as detailed meshes [23] or keypoint struc-\ntures [3], which are time-consuming and difficult to ob-\ntain and manipulate. A substantial body of research lever-\nages pre-trained image or video diffusion models as priors\nto guide 4D content creation [35]. They typically employ\nScore Distillation Sampling (SDS) to optimize various 4D\nrepresentations, including NeRF [26], 3D Gaussian Splat-\nting [38], or deformable meshes, as seen in methods like\n4D-fy [2] and 4Dynamic [45]. However, they are usually\nconstrained to low-resolution outputs and simple motions,\nprimarily due to the quality of 2D generations, computa-\ntional inefficiencies, and the scarcity of high-quality 4D\ntraining data [14]. Notably, \u201cJelly effect\u201d and \u201cJanus prob-\nlem\u201d persist as major challenges in these approaches, sig-\nnificantly degrading the animation quality and limiting their\npractical applicability.\nTo address these limitations, we propose TriDiff-4D, a\nTriplane Re-posingDiffusion model for4Dgeneration. In-\nstead of relying on computationally expensive optimiza-\ntion loops with 2D image and video priors using SDS\nloss, TriDiff-4D efficiently encodes 3D object and motion\nknowledge, and reduces the generation time from hours\nto seconds. In addition, our model addresses key limi-\ntations of existing methods by combining an efficient tri-\nplane representation with direct diffusion-based reposing.\nThis integration ensures frame-to-frame temporal coher-\nence, eliminates the Janus problem across viewpoints, and\npreserves anatomical correctness throughout complex mo-\ntion sequences. Furthermore, unlike approaches that rely\non mesh deformation and video-to-4D techniques, we con-\ndition directly on 3D skeletons within the triplane feature\nspace. This shift in conditioning dimensionality enables us\nto maintain volumetric consistency across all viewpoints,\neffectively eliminating the jelly-like wobbling effect.Specifically, our proposed TriDiff-4D begins by gener-\nating an initial static 3D avatar represented by triplane fea-\ntures, directly derived from a textual description specifying\nthe object\u2019s category and appearance. Simultaneously, a se-\nquence of skeleton poses is generated from a separate tex-\ntual description specifying the desired motion with a text-\nto-motion models, renowned for generating expressive and\ncomplex motions. After that, a novel diffusion-based re-\nposing mechanism is used to generate a sequence of ani-\nmated triplane features. More precisely, this module takes\nthe initial avatar\u2019s triplane features and the generated skele-\nton sequence as conditional inputs, iteratively transforming\nthe pose information within the latent space for each frame.\nThis ensures that the avatar accurately aligns with each tar-\nget skeleton pose while maintaining appearance consistency\nthroughout the motion. Finally, the re-posed sequence of\ntriplane features are decoded into a dynamic, temporally\nsmooth, and view-consistent 4D avatar. Our framework\nsupports flexible rendering options, with compatibility for\nboth Neural Radiance Fields (NeRF) and Gaussian Splat-\nting models as decoders, allowing users to select the optimal\nrendering approach based on their specific requirements.\nTo demonstrate its effectiveness, we compare TriDiff-4D\nwith leading 4D avatar generation methods across gener-\nation speed, qualitative comparisons, and user preference\nstudy. The results demonstrate that our method delivers\nsuperior visual quality, natural motion, accurate geometry,\nand overall appeal, while being significantly more compu-\ntationally efficient than existing approaches. Quantitatively,\nTriDiff-4D achieves a user preference score of 79.59% ver-\nsus 20.41% for the previous open-source state-of-the-art\nmodel [31], and reduces generation time from 10 minutes\nto just 0.6 minutes.\nIn summary, our main contributions are as follows:\n\u2022 We propose TriDiff-4D, a cohesive text-to-4D generation\npipeline that directly synthesizes high-quality 3D avatars\nwith realistic motion in a single forward pass within sec-\nonds.\n\u2022 We introduce a novel diffusion-based model for re-posing\n3D avatars by modifying triplane features based on gen-\nerated skeleton sequences, ensuring temporal coherence\nand anatomical accuracy across complex motions.\n\u2022 We learn 3D structure and motion priors directly from\nlarge-scale assets and address key limitations of existing\nmethods by explicitly conditioning 4D generation on our\ngenerated 3D avatars and skeletal motion. This approach\neliminates view inconsistencies, temporal flickering, and\nthe \u2018jelly-like wobbling effect\u2019 commonly seen in com-\npeting methods.\n2. Related Work\nText-to-3D Generation.The generation of static 3D-\nobjects from textual prompts has seen significant progress.\n2\nFigure 2.Running motion comparison [32] (left) and our method (right). The baseline exhibits unrealistic geometric stretching,\nparticularly evident in limb elongation during dynamic movements, while our method maintains consistent proportional geometry and\nstructural integrity throughout the motion sequence.\nDirect-3D [18] addresses critical challenges in text-to-3D\ngeneration by introducing a triplane feature generation ap-\nproach for 3D model creation, which provided the initial\ntriplane features for our proposed pipeline. Its architecture\nfeatures a disentangled triplane diffusion module for sep-\narate geometry and color generation, followed by a NeRF\nauto-decoder. Another foundational work, EG3D [4], in-\ntroduced an efficient geometry-aware 3D Generative adver-\nsarial Network(GAN) that utilizes a hybrid explicit-implicit\ntriplane architecture, demonstrating the efficacy of triplane\nfor 3D generation. And many other approaches leverage\nthe popular Score Distillation Sampling(SDS), like one of\nthe popular methods DreamFusion [29] which distill knowl-\nedge from pre-trained 2D text-to-image diffusion models\ninto 3D representations. Also, some methods use multi-\nview generation for high-resolution 3D content creation like\nLGM [37]. Overall, the research for 3D generation has\nshifted towards directly generating comprehensive 3D rep-\nresentations, which prioritizes more efficient and stream-\nlined methodology that enable more direct translation of in-\nput descriptions into high-fidelity 3D assets [21].\nMotion Generation and Skeleton-based Animation.\nEarly implementations of motion generation rely on dif-\nfusion model, such as MDM and SinMDM [7, 30, 39],\nuse transformer-based architecture and operate by denois-\ning from sampled motion sequences. This process allows\nthe model to generate diverse and realistic human motions\nconditioned on various inputs such as text prompts or action\nlabels. Furthermore, MoMask [6] is a leading framework\nfor text-driven motion generation. It employs a hierarchical\nquantization scheme to represent human motion as discrete\ntokens and uses bidirectional transformers for generation,\nwhich achieves a high-fidelity result.\nBeyond text-to-motion, there are numerous studies\nworking on general human pose estimation and motion gen-\neration from various inputs [8, 19, 42, 47]. Several methods\nspecifically focus on using skeleton conditioning to con-\ntrol avatar animation. For instance, Animate Anyone [8],\na novel framework that leverages a UNet-based architec-ture to generate photorealistic human animations. The sys-\ntem takes skeleton sequences as input and produces high-\nfidelity 2D videos that precisely follow the skeletal mo-\ntion while preserving the appearance of the reference sub-\nject. This approach effectively bridges the gap between mo-\ntion control and visual rendering, enabling detailed charac-\nter animation from simple pose sequences. Furthermore,\nDreamWaltz-G [9] integrates skeleton controls from 3D\nhuman templates [10] into 2D diffusion models using a\nskeleton-guided score distillation strategy, combined with\na hybrid 3D gaussian avatar representation for expressive\nanimation.\n4D generation.4D generation has been an active re-\nsearch area as the demand for 3D scenes or avatars is in-\ncreasing. 4D-fy [2] is a key temporary method that must\nbe considered. It utilizes a hybrid score distillation sam-\npling(SDS) [3] , like some 3D-aware text-to-3D gener-\nation models [29, 36, 43], and optimize a 4D radiance\nfield. This approach aims to balance the appearance qual-\nity, 3D structure and motions. Another important approach\nis video sequence based 4D generation such as Dream-\nGaussian4D [31] and 4DGen [44] [40]. These frameworks\nemerged as a notable contribution addressing critical limi-\ntations observed in prior 4D content creation pipelines that\npredominantly relied on text-to-image and text-to-video dif-\nfusion models. [49]\n3. Preliminaries\nLatent Diffusion Model with Triplane Representation.\nDiffusion models [7] are a class of generative models that\nlearn to transform a simple noise distribution into a complex\ndata distribution through an iterative denoising process. La-\ntent diffusion models [34] move this process from pixel\nspace to the latent space of a Variational Autoencoder [13].\nHere, we follow previous work [18] and consider the diffu-\nsion process in the triplane Space [4].\nSpecifically, we consider the triplane representation of\nthe geometric and color information, denotedF geo/color,0\n3\nof the input data, and refer to it simply asF 0in this section.\nIn the forward diffusion process, Gaussian noise is incre-\nmentally added toF 0:\nq(Ft|Ft\u22121) =N\u0010\nFt;p\n1\u2212\u03b3 tFt\u22121, \u03b3tI\u0011\n,(1)\nwhereF trepresents the noisy triplane representation at time\nstept, and\u03b3 tis a predefined noise schedule witht\u2208(0,1).\nWith reparameterization, this process allows samplingF tin\na closed form:\nFt=\u221a\u00af\u03b1tF0+\u221a\n1\u2212\u00af\u03b1 t\u03f5,(2)\nwhere\u00af\u03b1 t=Qt\ni=1(1\u2212\u03b3 i)and\u03f5\u223c N(0,I). The diffusion\nmodel, parameterized by\u0398, learns to reverse this noising\nprocess by takingF tas input and reconstructing the clean\ndata with the objective:\nL=\u2225\u03f5\u2212\u03f5 \u0398(Ft, t, c)\u22252\n2,(3)\nwherecis a condition to guide the denoising process.\n4. Methodology\nOur key insight is that 4D avatar generation can be effi-\nciently achieved by explicitly separating 3D structure mod-\neling from motion control, then recombining them through\ndiffusion-based reposing. As illustrated in Fig. 3, our\nTriDiff-4D operates through three main stages: we first\ngenerate a high-quality static 3D avatar using a triplane\ndiffusion model (Section 4.1), simultaneously encode the\ntarget motion into a format compatible with our diffusion\narchitecture (Section 4.2), and finally unify these compo-\nnents through our novel diffusion-based reposing mecha-\nnism (Section 4.3) that transforms the static avatar accord-\ning to the motion guidance while preserving appearance\nconsistency throughout the animation sequence.\n4.1. Initial 3D Avatar Generation\nWe employ a triplane-based diffusion model that gener-\nates high-quality 3D assets from text prompts. Inspired by\nDIRECT-3D [18], we decompose triplane features into sep-\narate geometry and color components to facilitate training\nstability and enable independent optimization of structural\nand appearance characteristics.\nThe generated featuresT init= (Finit\ng, Finit\nc)comprise\nthree feature maps corresponding to theXY,XZ, andY Z\ncoordinate planes, with each plane containingCfeature\nchannels. At resolution characterized byH\u00d7W, the ge-\nometry triplaneFinit\ng\u2208R3\u00d7C\u00d7H\u00d7Wencodes structural in-\nformation while the color triplaneFinit\nc\u2208R3\u00d7C\u00d7H\u00d7Wcap-\ntures appearance details. These triplane features undergo re-\nshaping toR3\u00b7C\u00d7H\u00d7Wfor convolutional processing before\nbeing decoded through an auto-decoder to generate final 3D\nrenderings. The processed featuresT initsubsequently func-\ntion as conditioning inputs for the diffusion-based reposing\nframework described in Section 4.3.4.2. Skeleton Encoding for Pose Guidance\nOur diffusion-based reposing framework requires effective\nskeleton representations to guide the generation of properly\nposed triplane features. While 3D skeletons naturally cap-\nture the target pose information needed for character ani-\nmation, their structurally complex nature presents compat-\nibility challenges when used directly with diffusion archi-\ntectures. This section introduces our skeleton encoding ap-\nproach that systematically transforms 3D skeletal data into\na format specifically designed to condition diffusion models\nwhile carefully preserving the spatial relationships essential\nfor accurate triplane feature reposing.\nMotion Generation.We integrated a transformer model\nto generate our motions following works like MoMask [6]\nwhich demonstrates state-of-the-art performance in synthe-\nsizing diverse and realistic movements and implemented a\npose transfer pipeline to adapt the motion generation frame-\nwork to our dataset. The pipeline enables us to work with\n3D skeleton that is naturally represented as a set of ver-\ntices (joints) and edges (bones), as well as the 3D coordi-\nnates of the vertices. Although this representation encodes\nrich 3D information, it is not readily compatible with diffu-\nsion models, particularly when used as a conditioning sig-\nnal to guide triplane feature generation. Hence we propose\nto pre-process 3D skeletons into 2D triplane skeleton en-\ncodings, which enable efficient and effective pose guidance\n(see Fig. 4).\nSkeleton encoding.Given the generated 3D skeleton,\nwe process three 2D skeleton encodings each from a differ-\nent orthogonal view,i.e., XY , XZ, and YZ. We project the\n3D joints to the 2D plane and produce two maps that pre-\nserve key skeleton information: (1) theoccupancy mapis a\nbinary map indicating the presence of joints and bones when\nprojected to the 2D plane, and (2) theindex mapencodes\nthe normalized joint index for regions occupied by joints\nand bones. Specifically, for each location(u, v)in the 2D\nlattice, the occupancy and index maps are represented by\nAlthough we are representing the 3D skeleton with 2D\nmaps, the 2D skeleton embeddings preserve critical spatial\nrelationships between joints and bones. As shown in Fig. 4,\nthis approach maintains the structural integrity of the skele-\nton across multiple orthogonal views (XY , XZ, YZ), ensur-\ning that the complete 3D pose information is retained.\nCompatibility with triplane diffusion models.2D\nskeleton encoding is naturally compatible with the triplane\ndiffusion models. They can be directly used as conditioning\nsignals or concatenated with noisy latent to guide the gener-\nation process. This design avoids the need for an additional\nencoding module to process the structured 3D data.\nComputational efficiency.Converting complex 3D\nskeleton data into a set of 2D projections significantly re-\nduces computational complexity compared to processing\nfull volumetric representations. This efficiency is critical\n4\nFigure 3.Method overview.Given a prompt, we generate a 4D mesh through a three-step process: (1)Triplane generation, which\ntransforms text descriptions into 3D representations; (2)Skeleton generation, which generates 3D motion sequences from the text; and (3)\nDiffusion-based reposing, which integrates these components to produce animated 3D avatars with precise pose control.\nFigure 4.Visualization of our triplane skeleton encoding approach.Each row visualizes the same pose projected onto three orthogonal\nplanes: XY (top), XZ (middle), and YZ (bottom). For each projection, we display three feature channels, repeated to match the triplane\nfeature dimensionality:occupancy maps, which highlight the structural presence of joints and bones, andindex maps, where brightness\nvariations represent normalized joint indices.\nfor real-time applications and allows the diffusion model to\nprocess pose information without prohibitive computational\ndemands.\nMoccupancy (u, v) =(\n1,if(u, v)within joint/bone projection\n0,otherwise\nMindex(u, v) =\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f3i\nN\u22121,if(u, v)is projection of jointi\ni+j\n2(N\u22121),if(u, v)within bone(i, j)projection\n0,otherwise\n(4)whereNis the total number of joints,i, j\u2208 {0, . . . , N\u22121}are\njoint indices, and(i, j)denotes the connecting bone.\n4.3. Diffusion-based Avatar Re-posing\nThe goal of the re-posing module is to generate re-posed tri-\nplane features given initial triplane featuresFinit\ngeoandFinit\ncolor\nand skeleton representationSt\nencat timet.\nFt\ngeo, Ft\ncolor=RePose\u0000\nFinit\ngeo, Finit\ncolor, St\nend\u0001\n(5)\n5\nWe implement the re-posing module as a conditional dif-\nfusion model with a U-Net architecture. Specifically, two\ntypes of conditioning methods are considered.\nDirect concatenation.We concatenate the diffusion\nmodel latent with the initial triplane features and the skele-\nton representation, and pass the combined input through a\nconvolutional layer followed by the U-Net backbone for de-\nnoising. This direct integration enables spatial alignment\nbetween the avatar features and skeletal structure, provid-\ning a strong geometric prior for the denoising process.\nCross-attention conditioning.In the U-Net, we convert\nthe image-like condition (triplane appearance and skele-\nton) onto sequential tokens, allowing the diffusion model\nto discover meaningful relationships between spatial posi-\ntions while integrating the conditional information. After\nprocessing, it reshapes the enhanced representation back\nto spatial format and adds it to the original input through\na residual connection. It acts as a bridge that precisely\nmaps the target skeleton onto the character features and en-\nables high quality, detailed reposing while preserving the\ncharacter\u2019s original identity and appearance. These mod-\nules use attention mechanisms across multiple resolutions\nto create dynamic relationships between the input features\nand conditional information. With this transformer archi-\ntecture, TriDiff-4D achieves superior pose transfer through\nprecise joint-to-feature alignment and reduces reposing ar-\ntifacts such as limb distortion and geometry collapse during\nextreme poses [31], and result shown in Fig. 5.\n5. Experiments\n5.1. Experimental Setup\nDatsets.Given the scarcity of comprehensive 4D datasets,\nwe strategically selected and combined two complementary\nresources to address our research needs. For character rep-\nresentation, we utilized the RaBit dataset [24], which com-\nprises 1,500 unique character models including both human\nand anthropomorphic animal figures. This dataset provides\nSMPL-like parametric models that enable precise manipu-\nlation of pose through joint rotation and skeleton config-\nurations. This structure allows for consistent deformation\nacross diverse character morphologies.\nTo ensure robust motion diversity and anatomically re-\nalistic movement patterns, we incorporated the AMASS\ndataset [25] [5], which offer an extensive collection of high-\nfidelity human motion sequences captured from real sub-\njects. These datasets provided the necessary variation in\nmovement dynamics to train our models while maintain-\ning biomechanical plausibility across different action cat-\negories. Our approach is specifically designed to be com-\npatible with existing motion generation frameworks without\nrequiring additional training, enabling efficient implemen-\ntation with minimal computational resources. This compat-Table 1.Inference speed and iteration requirements.Our\nmethod enables significantly faster generation without iterative op-\ntimization.\nMethod Time Iterations\nMA V3D [35] 6.5 hr 12k\nAnimate124 [48] - 20k\nConsistent4D [11] 2.5 hr 10k\n4D-fy [2] 23 hr 120k\nDream-in-4D [50] 10.5 hr 20k\nAYG [15] - 20k\nDG4D (Image-to-4D GS) [31] 6.5 mins 0.7k\nDG4D + Texture Refinement 10 mins 0.75k\nTriDiff-4D (Ours)0.6 mins 1\u22c6\n\u22c6Single forward pass, no optimization required.\nTable 2. Quantitative comparison on the Consistent4D benchmark.\nMethod LPIPS\u2193CLIP\u2191FVD\u2193\nConsistent4D 0.16 0.87 1133.44\n4DGen 0.13 0.89 -\nSTAG4D 0.13 0.91 992.21\nDG4D 0.16 0.87 -\nEfficient4D [27] 0.14 0.92 -\nL4GM 0.12 0.94 691.87\nTriDiff-4D (Ours) 0.13 0.94 626.29\nibility significantly reduces the operational costs associated\nwith deploying advanced motion synthesis systems in pro-\nduction environments.\nEvaluation.We evaluate our method against state-of-the-\nart approaches for 4D avatar generation and animation,\nfocusing on both computational efficiency and animation\nquality. Our baseline comparisons include optimization-\nbased methods such as MA V3D [35], Consistent4D [11],\nand DreamGaussian4D [31], which represent diverse ap-\nproaches to 4D content creation. We also measure percep-\ntual quality (LPIPS), semantic alignment (CLIP), and tem-\nporal coherence (FVD) using the Consistent4D benchmark\nin Tab. 2.\nImplementation details.We use the Adam optimizer [12]\nwith a learning rate of1e\u22124for the diffusion model. The\nlearning rate schedule includes a linear warm-up over the\nfirst 500 iterations and a step decay that reduced the rate\nby half after 500K iterations. Our model operates on 128\n\u00d7128 resolution inputs and is trained for 2M iterations on\na dataset comprising 1.5K characters, each with 22 poses\nwhich contains total of 33K training samples. More experi-\nmental details in supplementary\n6\nFigure 5.Complex motion generation with TriDiff-4D.Our model is capable of generating extreme pose transitions and complex motions\nwhile preserving consistent geometry and appearance.\nFigure 6.Comparison with MA V3D.Our method generates more realistic characters with improved visual quality, enhanced pose articu-\nlation, and consistent appearance throughout the animation sequence. Baseline results are obtained from the official project page.\n5.2. State-of-the-Art Text-to-4D Generation\nWe compare our model with other 4D generation models\nin Tab. 1, which demonstrates the substantial computational\nefficiency of our approach. Our method generates a com-\nplete 14 frame animation sequence in merely 0.6 minutes,\nrepresenting a significant reduction in processing time com-\npared to existing techniques that require hours of computa-\ntion. This efficiency stems from our non-iterative pipeline\ndesign, which requires only single pass rather than thou-\nsands of optimization iterations.\nBeyond computation advantages, our approach main-\ntains superior geometric consistency during animation\nshown in Fig. 2 that preserves structural integrity during\npose changes, whereas alternative approaches have a jelly-\nlike wobbling effect. The effect is enabled by our triplane\nrepresentation (Fig. 7) which maintains consistent feature\ninformation across frames.\nFinally, we conduct a user preference study the results\nfrom DreamGaussian4D and TriDiff-4D, where each gen-\nerated animation was reviewed by 14 participants. Partic-ipants are asked to select the best one in three evaluation\naxes: geometry consistency, motion consistency and over-\nall preference. The results are presented in Tab. 8, where\nTriDiff-4D demonstrates a clear advantage across all eval-\nuation criteria, with users strongly preferring our approach\nfor its superior visual quality and motion coherence.\n5.3. Ablation Studies\nTo evaluate the impact of key design decisions in our\nTriDiff-4D framework, we conduct a series of ablation ex-\nperiments to examine both the skeleton encoding method\nand attention resolution.\nSkeleton Encoding Method.We compare our index-\nbased skeleton encoding method with a heatmap-based ap-\nproach (Fig. 9). The fundamental difference between these\napproaches lies in how skeleton information is represented\nspatially. Our index-based method provides precise skeletal\nstructures Fig. 7 through normalized joint indices and con-\nnections, which creates a sharp localization of object skele-\nton structure. In contrast, the heatmap-based approach di-\n7\nFigure 7.Triplane feature visualization.These frames illustrate the integration of the skeleton features with avatar appearance features,\nprojected onto the same plane across sequential time steps. The visualization shows that the generated features align precisely with the\ntarget skeletal structure, confirming that our approach effectively produces geometry that adheres to skeletal guidance while preserving\nconsistent appearance attributes.\nMotion Cons. Geometry Cons. Overall Preference020406080100User preferenceTriDiff-4D (Ours) DG4D\nFigure 8.User preference study results comparing TriDiff-\n4D with DreamGaussian4D.Participants consistently preferred\nTriDiff-4D across all evaluation criteria.\nFigure 9.Different skeleton encoding approaches.Left: Index-\nbased skeleton encoding, where joints are represented as discrete\npoints across three orthogonal planes. Right: The alternative\nheatmap-based encoding that provides a smoother, continuous rep-\nresentation of joint locations.\nFigure 10.Visualization of ablation studies.(1) Top: Index-\nbased skeleton full configuration. (2) Mid: Heatmap-based skele-\nton with full configuration. (3) Bottom: Index-based skeleton with\nlower attention resolution and head number.\nminishes the connection information between joints, result-\ning in less defined skeleton constraints during the diffusion\nprocess. Experimental results demonstrate that our index-based approach achieves better pose accuracy and superior\nappearance quality. While using the heatmap-based encod-\ning, the diffusion model struggles to maintain structural in-\ntegrity during complex animations, leading to noticeable vi-\nsual artifacts including holes in the character\u2019s body surface.\nAttention Mechanism.We reduce the attention reso-\nlution from (32, 16, 8) to (4, 2, 1) and decrease attention\nheads from 4 to 1. Despite a 5% reduction in memory us-\nage, this configuration produces significantly degraded re-\nsults (Fig. 10). With the same training iterations, the lower\nattention model fails to learn proper reposing patterns, re-\nsulting in severe geometry distortions and loss of character\ndetails. These results confirm that maintaining sufficient at-\ntention resolution is critical for high-fidelity animation gen-\neration, demonstrating that our configuration strikes an op-\ntimal balance between quality and efficiency. Despite the\ntheoretical computational advantage of lower resolution at-\ntention, we observe that training with this configuration is\nactually slower in practice. This result occurs because the\nreduction of spatial resolution prevents the model from ef-\nficiently extracting and focusing on the most relevant fea-\ntures for pose transfer. The lower resolution attention maps\nbecome too coarse to capture important local relationships\nbetween triplane skeleton structure and geometry. Further-\nmore, the reduced number of attention heads severely limits\nthe model\u2019s ability to simultaneously attend to different as-\npects of the animation process, such as maintaining overall\ncharacter structure while positioning individual limbs.\n6. Conclusion\nIn this paper, we present TriDiff-4D, a novel approach for\ngenerating high-quality 4D avatars through diffusion-based\ntriplane reposing. Our method addresses critical limitations\nof existing 4D generation techniques, regarding temporal\nconsistency, motion accuracy, visual fidelity, and computa-\ntional efficiency combining a specialized diffusion model\nwith the triplane representation and skeleton-guided condi-\ntioning. Notably, our approach achieves precise skeleton-\nbased pose control while reducing generation time, enabling\nsignificantly faster workflow and real-time applications pre-\nviously unattainable with existing methods.\n8\nAppendix\nA. Implementation Details\nA.1. Initial Static Avatar Generation\nOur dataset comprises 1,500 unique characters, each ren-\ndered in multiple poses to create a diverse training set of\n3,000 initial configurations. Through extensive experimen-\ntation, we discovered that initializing with a T-pose consis-\ntently yields superior performance and faster training con-\nvergence. While our model can process avatars in any initial\npose, non-T-pose initializations generally require additional\ntraining iterations to achieve comparable reposing quality.\nA.2. Training\nWe employ a two-stage training approach. The static avatar\ngeneration model is trained for 200,000 iterations, while\nthe reposing model undergoes a more extensive 2M itera-\ntions on 33K poses derived from our character dataset. Both\nmodels are trained on 4 NVIDIA H100 GPUs.\nFor the triplane feature representation, we set the num-\nber of channels C = 6 and train the diffusion model with\n1,000 diffusion steps using a linear noise schedule. To opti-\nmize efficiency, we implement a progressive reconstruction\nstrategy:\nFor the initial avatar model:\n\u2022 15 reconstructions per iteration for the first 50,000 itera-\ntions\n\u2022 3 reconstructions per iteration until 100,000 iterations\n\u2022 1 reconstruction per iteration for all remaining iterations\nFor the reposing model:\n\u2022 30 reconstructions per iteration for the first 50,000 itera-\ntions\n\u2022 15 reconstructions per iteration until 300,000 iterations\n\u2022 5 reconstructions per iteration until 800,000 iterations\n\u2022 1 reconstruction per iteration for the final training phase\nThis progressive schedule significantly accelerates train-\ning while maintaining output quality by allocating more\ncomputational resources during early learning phases when\nthe model requires more intensive refinement.\nB. Limitations\nOur current model does not simulate cloth dynamics and\nonly generate human avatars due to the scarcity of appro-\npriate 4D datasets that include realistic cloth behavior. This\nlimitation stems directly from the broader challenge in the\nfield: the lack of comprehensive training data that captures\nhow garments fold, wrinkle, and respond to character move-\nment during animation. Without such specialized datasets,\nour model cannot learn the complex physical interactions\nbetween clothing and body motion, particularly evident in\ncharacters with loose or flowing garments where these dy-\nnamics are most noticeable.Moreover, our framework presently employs standard\ndiffusion models. The exploration of more advanced ap-\nproaches, such as Flow Matching [1, 16, 20, 22] and en-\nhanced architectures [17, 28], is left to future work.\nC. Ethics and Broader Impact\nOur 4D avatar generation research raises ethical consider-\nations that we actively address. While our technology fo-\ncuses on non-photorealistic characters rather than realistic\nhuman representations, we strongly condemn any use of our\ntechnology to create deceptive content intended to harm in-\ndividuals, groups, or organizations, or to deliberately spread\nmisinformation in public discourse.\nOur research on diffusion-based reposing has poten-\ntial applications beyond graphics research. The ability to\nrapidly transform avatar poses through feature-space modi-\nfications could enhance virtual reality experiences, person-\nalized avatars and real-time applications where traditional\noptimization approaches are prohibitively slow.\nD. Safeguards\nGiven the capabilities of TriDiff-4D, it is essential to estab-\nlish appropriate safeguards against potential misuse. Our\ntraining dataset deliberately excludes photorealistic human\ncharacters, focusing instead on stylized figures, which in-\nherently limits the model\u2019s ability to generate deceptive\ncontent that could be used for harmful purposes.\n9\nFigure 11. Additional results using TriDiff-4D\n10\nReferences\n[1] Michael S Albergo and Eric Vanden-Eijnden. Building nor-\nmalizing flows with stochastic interpolants.arXiv preprint\narXiv:2209.15571, 2022. 9\n[2] Sherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gordon\nWetzstein, Leonidas Guibas, Peter Wonka, Sergey Tulyakov,\nJeong Joon Park, Andrea Tagliasacchi, and David B Lin-\ndell. 4d-fy: Text-to-4d generation using hybrid score dis-\ntillation sampling. InProceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n7996\u20138006, 2024. 2, 3, 6\n[3] Zenghao Chai, Chen Tang, Yongkang Wong, and Mohan\nKankanhalli. Star: Skeleton-aware text-based 4d avatar gen-\neration with in-network motion retargeting, 2024. 2, 3\n[4] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,\nBoxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J\nGuibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient\ngeometry-aware 3d generative adversarial networks. InPro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 16123\u201316133, 2022. 3\n[5] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji,\nXingyu Li, and Li Cheng. Generating diverse and natural 3d\nhuman motions from text. InProceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 5152\u20135161, 2022. 6\n[6] Chuan Guo, Yuxuan Mu, Muhammad Gohar Javed, Sen\nWang, and Li Cheng. Momask: Generative masked model-\ning of 3d human motions. InProceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 1900\u20131910, 2024. 3, 4\n[7] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models.Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 3\n[8] Li Hu. Animate anyone: Consistent and controllable image-\nto-video synthesis for character animation. InProceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 8153\u20138163, 2024. 3\n[9] Yukun Huang, Jianan Wang, Ailing Zeng, Zheng-Jun Zha,\nLei Zhang, and Xihui Liu. Dreamwaltz-g: Expressive 3d\ngaussian avatars from skeleton-guided 2d diffusion.arXiv\npreprint arXiv:2409.17145, 2024. 3\n[10] Yujiao Jiang, Qingmin Liao, Zhaolong Wang, Xiangru Lin,\nZongqing Lu, Yuxi Zhao, Hanqing Wei, Jingrui Ye, Yu\nZhang, and Zhijing Shao. Smplx-lite: A realistic and driv-\nable avatar benchmark with rich geometry and texture anno-\ntations. In2024 IEEE International Conference on Multime-\ndia and Expo (ICME), pages 1\u20136. IEEE, 2024. 3\n[11] Yanqin Jiang, Li Zhang, Jin Gao, Weiming Hu, and Yao\nYao. Consistent4d: Consistent 360\u00b0 dynamic object gen-\neration from monocular video. InThe Twelfth International\nConference on Learning Representations, 2024. 6\n[12] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.arXiv preprint arXiv:1412.6980,\n2014. 6\n[13] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. InInternational Conference on Learning Rep-\nresentations, 2014. 3[14] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution\ntext-to-3d content creation. InIEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2023. 2\n[15] Huan Ling, Seung Wook Kim, Antonio Torralba, Sanja Fi-\ndler, and Karsten Kreis. Align your gaussians: Text-to-4d\nwith dynamic 3d gaussians and composed diffusion models.\nInProceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 8576\u20138588, 2024. 6\n[16] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximil-\nian Nickel, and Matt Le. Flow matching for generative mod-\neling.arXiv preprint arXiv:2210.02747, 2022. 9\n[17] Qihao Liu, Zhanpeng Zeng, Ju He, Qihang Yu, Xiaohui\nShen, and Liang-Chieh Chen. Alleviating distortion in image\ngeneration via multi-resolution diffusion models and time-\ndependent layer normalization.Advances in Neural Infor-\nmation Processing Systems, 37:133879\u2013133907, 2024. 9\n[18] Qihao Liu, Yi Zhang, Song Bai, Adam Kortylewski, and\nAlan Yuille. Direct-3d: Learning direct text-to-3d generation\non massive noisy 3d data. InProceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 6881\u20136891, 2024. 3, 4\n[19] Qihao Liu, Ju He, Qihang Yu, Liang-Chieh Chen, and Alan\nYuille. Revision: High-quality, low-cost video generation\nwith explicit 3d physics modeling for complex motion and\ninteraction.arXiv preprint arXiv:2504.21855, 2025. 3\n[20] Qihao Liu, Xi Yin, Alan Yuille, Andrew Brown, and Mannat\nSingh. Flowing from words to pixels: A noise-free frame-\nwork for cross-modality evolution. InProceedings of the\nComputer Vision and Pattern Recognition Conference, pages\n2755\u20132765, 2025. 9\n[21] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl V ondrick. Zero-1-to-\n3: Zero-shot one image to 3d object. InProceedings of\nthe IEEE/CVF international conference on computer vision,\npages 9298\u20139309, 2023. 3\n[22] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow\nstraight and fast: Learning to generate and transfer data with\nrectified flow.arXiv preprint arXiv:2209.03003, 2022. 9\n[23] Matthew Loper, Naureen Mahmood, Javier Romero, Ger-\nard Pons-Moll, and Michael J. Black. SMPL: A skinned\nmulti-person linear model.ACM Trans. Graphics (Proc.\nSIGGRAPH Asia), 34(6):248:1\u2013248:16, 2015. 2\n[24] Zhongjin Luo, Shengcai Cai, Jinguo Dong, Ruibo Ming,\nLiangdong Qiu, Xiaohang Zhan, and Xiaoguang Han. Ra-\nbit: Parametric modeling of 3d biped cartoon characters\nwith a topological-consistent dataset. InProceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 12825\u201312835, 2023. 6\n[25] Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Ger-\nard Pons-Moll, and Michael J. Black. AMASS: Archive of\nmotion capture as surface shapes. InInternational Confer-\nence on Computer Vision, pages 5442\u20135451, 2019. 6\n[26] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\n11\nthesis.Communications of the ACM, 65(1):99\u2013106, 2021.\n2\n[27] Zijie Pan, Zeyu Yang, Xiatian Zhu, and Li Zhang. Effi-\ncient4d: Fast dynamic 3d object generation from a single-\nview video, 2025. 6\n[28] William Peebles and Saining Xie. Scalable diffusion models\nwith transformers. InProceedings of the IEEE/CVF inter-\nnational conference on computer vision, pages 4195\u20134205,\n2023. 9\n[29] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall. Dreamfusion: Text-to-3d using 2d diffusion.arXiv\npreprint arXiv:2209.14988, 2022. 2, 3\n[30] Sigal Raab, Inbal Leibovitch, Guy Tevet, Moab Arar, Amit H\nBermano, and Daniel Cohen-Or. Single motion diffusion.\narXiv preprint arXiv:2302.05905, 2023. 3\n[31] Jiawei Ren, Liang Pan, Jiaxiang Tang, Chi Zhang, Ang Cao,\nGang Zeng, and Ziwei Liu. Dreamgaussian4d: Genera-\ntive 4d gaussian splatting.arXiv preprint arXiv:2312.17142,\n2023. 2, 3, 6\n[32] Jiawei Ren, Kevin Xie, Ashkan Mirzaei, Hanxue Liang, Xi-\naohui Zeng, Karsten Kreis, Ziwei Liu, Antonio Torralba,\nSanja Fidler, Seung Wook Kim, and Huan Ling. L4gm:\nLarge 4d gaussian reconstruction model. InAdvances in\nNeural Information Processing Systems, 2024. 3\n[33] Jiawei Ren, Kevin Xie, Ashkan Mirzaei, Hanxue Liang, Xi-\naohui Zeng, Karsten Kreis, Ziwei Liu, Antonio Torralba,\nSanja Fidler, Seung Wook Kim, and Huan Ling. L4gm:\nLarge 4d gaussian reconstruction model, 2024. 2\n[34] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj \u00a8orn Ommer. High-resolution image\nsynthesis with latent diffusion models. InProceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 3\n[35] Uriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual,\nIurii Makarov, Filippos Kokkinos, Naman Goyal, Andrea\nVedaldi, Devi Parikh, Justin Johnson, and Yaniv Taigman.\nText-to-4d dynamic scene generation.arXiv:2301.11280,\n2023. 2, 6\n[36] Boshi Tang, Jianan Wang, Zhiyong Wu, and Lei Zhang. Sta-\nble score distillation for high-quality 3d generation.arXiv\npreprint arXiv:2312.09305, 2023. 3\n[37] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang,\nGang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian\nmodel for high-resolution 3d content creation. InEuropean\nConference on Computer Vision, pages 1\u201318. Springer, 2024.\n3\n[38] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang\nZeng. Dreamgaussian: Generative gaussian splatting for ef-\nficient 3d content creation. InThe Twelfth International Con-\nference on Learning Representations, 2024. 2\n[39] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir,\nDaniel Cohen-Or, and Amit H Bermano. Human motion dif-\nfusion model.arXiv preprint arXiv:2209.14916, 2022. 3\n[40] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and\ndiverse text-to-3d generation with variational score distilla-\ntion, 2023. 3[41] Yiming Xie, Chun-Han Yao, Vikram V oleti, Huaizu Jiang,\nand Varun Jampani. Sv4d: Dynamic 3d content generation\nwith multi-frame and multi-view consistency, 2025. 2\n[42] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan,\nJia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng\nShou. Magicanimate: Temporally consistent human im-\nage animation using diffusion model. InProceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 1481\u20131490, 2024. 3\n[43] Runjie Yan, Kailu Wu, and Kaisheng Ma. Flow score dis-\ntillation for diverse text-to-3d generation.arXiv preprint\narXiv:2405.10988, 2024. 3\n[44] Yuyang Yin, Dejia Xu, Zhangyang Wang, Yao Zhao,\nand Yunchao Wei. 4dgen: Grounded 4d content gen-\neration with spatial-temporal consistency.arXiv preprint\narXiv:2312.17225, 2023. 3\n[45] Yu-Jie Yuan, Leif Kobbelt, Jiwen Liu, Yuan Zhang, Pengfei\nWan, Yu-Kun Lai, and Lin Gao. 4dynamic: Text-to-4d gen-\neration with hybrid priors.arXiv preprint arXiv:2407.12684,\n2024. 2\n[46] Yifei Zeng, Yanqin Jiang, Siyu Zhu, Yuanxun Lu, Youtian\nLin, Hao Zhu, Weiming Hu, Xun Cao, and Yao Yao. Stag4d:\nSpatial-temporal anchored generative 4d gaussians, 2024. 2\n[47] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models. In\nProceedings of the IEEE/CVF international conference on\ncomputer vision, pages 3836\u20133847, 2023. 3\n[48] Yuyang Zhao, Zhiwen Yan, Enze Xie, Lanqing Hong, Zhen-\nguo Li, and Gim Hee Lee. Animate124: Animating one im-\nage to 4d dynamic scene.arXiv preprint arXiv:2311.14603,\n2023. 6\n[49] Yuyang Zhao, Zhiwen Yan, Enze Xie, Lanqing Hong, Zhen-\nguo Li, and Gim Hee Lee. Animate124: Animating one im-\nage to 4d dynamic scene, 2024. 3\n[50] Yufeng Zheng, Xueting Li, Koki Nagano, Sifei Liu, Otmar\nHilliges, and Shalini De Mello. A unified approach for text-\nand image-guided 4d scene generation. InCVPR, 2024. 6\n[51] Yufeng Zheng, Xueting Li, Koki Nagano, Sifei Liu, Karsten\nKreis, Otmar Hilliges, and Shalini De Mello. A unified ap-\nproach for text- and image-guided 4d scene generation, 2024.\n2\n12",
      "summary": "TriDiff-4D introduces a diffusion-based triplane re-posing pipeline for fast and high-fidelity 4D avatar generation from text prompts. The method achieves temporally coherent, controllable, and visually accurate 4D sequences with significantly reduced computational cost.",
      "key_contributions": [
        "Proposes a novel diffusion-based triplane re-posing framework for 4D avatar generation.",
        "Implements an auto-regressive strategy enabling arbitrarily long 4D sequence synthesis.",
        "Explicitly learns 3D structure and motion priors from large-scale datasets for skeleton-driven generation.",
        "Eliminates the need for expensive optimization, drastically reducing generation time.",
        "Demonstrates superior temporal consistency, motion accuracy, and visual fidelity compared to existing methods."
      ],
      "methodology": "TriDiff-4D first generates a canonical 3D avatar and motion sequence from a text prompt, then applies a second diffusion model to animate the avatar according to the motion sequence, using an auto-regressive approach for each frame.",
      "results": "TriDiff-4D outperforms prior methods by producing high-fidelity, temporally consistent 4D avatars with complex motions, while reducing generation time from hours to seconds.",
      "keywords": [
        "4D generation",
        "diffusion models",
        "triplane representation",
        "avatar animation",
        "temporal consistency",
        "motion synthesis",
        "text-to-4D"
      ],
      "status": "new",
      "notes": null,
      "created_at": "2025-11-23T20:38:10.484928",
      "needs_llm_processing": false
    },
    "11": {
      "id": "2511.16659v1",
      "title": "PartUV: Part-Based UV Unwrapping of 3D Meshes",
      "authors": [
        {
          "name": "Zhaoning Wang",
          "affiliation": null
        },
        {
          "name": "Xinyue Wei",
          "affiliation": null
        },
        {
          "name": "Ruoxi Shi",
          "affiliation": null
        },
        {
          "name": "Xiaoshuai Zhang",
          "affiliation": null
        },
        {
          "name": "Hao Su",
          "affiliation": null
        },
        {
          "name": "Minghua Liu",
          "affiliation": null
        }
      ],
      "abstract": "UV unwrapping flattens 3D surfaces to 2D with minimal distortion, often requiring the complex surface to be decomposed into multiple charts. Although extensively studied, existing UV unwrapping methods frequently struggle with AI-generated meshes, which are typically noisy, bumpy, and poorly conditioned. These methods often produce highly fragmented charts and suboptimal boundaries, introducing artifacts and hindering downstream tasks. We introduce PartUV, a part-based UV unwrapping pipeline that generates significantly fewer, part-aligned charts while maintaining low distortion. Built on top of a recent learning-based part decomposition method PartField, PartUV combines high-level semantic part decomposition with novel geometric heuristics in a top-down recursive framework. It ensures each chart's distortion remains below a user-specified threshold while minimizing the total number of charts. The pipeline integrates and extends parameterization and packing algorithms, incorporates dedicated handling of non-manifold and degenerate meshes, and is extensively parallelized for efficiency. Evaluated across four diverse datasets, including man-made, CAD, AI-generated, and Common Shapes, PartUV outperforms existing tools and recent neural methods in chart count and seam length, achieves comparable distortion, exhibits high success rates on challenging meshes, and enables new applications like part-specific multi-tiles packing. Our project page is at https://www.zhaoningwang.com/PartUV.",
      "arxiv_id": "2511.16659v1",
      "arxiv_url": "http://arxiv.org/abs/2511.16659v1",
      "pdf_url": "https://arxiv.org/pdf/2511.16659v1",
      "published": "2025-11-20T18:58:39Z",
      "updated": "2025-11-20T18:58:39Z",
      "categories": [
        "cs.CV",
        "cs.CG",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "full_text": "PartUV: Part-Based UV Unwrapping of 3D Meshes\nZHAONING WANG,Hillbot Inc., USA\nXINYUE WEI,University of California San Diego, USA and Hillbot Inc., USA\nRUOXI SHI,University of California San Diego, USA and Hillbot Inc., USA\nXIAOSHUAI ZHANG,Hillbot Inc., USA\nHAO SU,University of California San Diego, USA and Hillbot Inc., USA\nMINGHUA LIU,Hillbot Inc., USA\nFig. 1. We propose PartUV, a novel part-based UV unwrapping method for 3D meshes. Unlike traditional approaches that rely solely on local geometric priors\nand often produce over-fragmented charts, PartUV combines learned part priors with geometric cues to generate a small number of part-aligned charts. We\nevaluate our method on four diverse datasets\u2014PartObjaverseTiny (man-made) [Yang et al .2024], Trellis (AI-generated) [Xiang et al .2024], ABC (CAD) [Koch\net al.2019], and Common Shapes [Jacobson and contributors 2023]\u2014and show that it produces significantly less fragmented UV mappings while maintaining\nlow distortion on par with baseline methods. Leveraging part-aware charts also enables applications such as generating one atlas per part.\nAuthors\u2019 Contact Information: Zhaoning Wang, Hillbot Inc., USA, zhaoning.eric.wang@\ngmail.com; Xinyue Wei, University of California San Diego, USA and Hillbot Inc., USA,\nxiwei@ucsd.edu; Ruoxi Shi, University of California San Diego, USA and Hillbot\nInc., USA, r8shi@ucsd.edu; Xiaoshuai Zhang, Hillbot Inc., USA, x@hillbot.ai; Hao Su,\nUniversity of California San Diego, USA and Hillbot Inc., USA, haosu@ucsd.edu;\nMinghua Liu, Hillbot Inc., USA, m@hillbot.ai.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nSA Conference Papers \u201925, Hong Kong, Hong KongUV unwrapping flattens 3D surfaces to 2D with minimal distortion, often\nrequiring the complex surface to be decomposed into multiple charts. Al-\nthough extensively studied, existing UV unwrapping methods frequently\nstruggle with AI-generated meshes, which are typically noisy, bumpy, and\npoorly conditioned. These methods often produce highly fragmented charts\nand suboptimal boundaries, introducing artifacts and hindering downstream\ntasks. We introduce PartUV, a part-based UV unwrapping pipeline that\ngenerates significantly fewer, part-aligned charts while maintaining low dis-\ntortion. Built on top of a recent learning-based part decomposition method\nPartField, PartUV combines high-level semantic part decomposition with\nnovel geometric heuristics in a top-down recursive framework. It ensures\n\u00a92025 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-2137-3/2025/12\nhttps://doi.org/10.1145/3757377.3763843\nSA Conference Papers \u201925, December 15\u201318, 2025, Hong Kong, Hong Kong.arXiv:2511.16659v1  [cs.CV]  20 Nov 2025\n2\u2022Zhaoning Wang, Xinyue Wei, Ruoxi Shi, Xiaoshuai Zhang, Hao Su, and Minghua Liu\neach chart\u2019s distortion remains below a user-specified threshold while min-\nimizing the total number of charts. The pipeline integrates and extends\nparameterization and packing algorithms, incorporates dedicated handling\nof non-manifold and degenerate meshes, and is extensively parallelized for ef-\nficiency. Evaluated across four diverse datasets\u2014including man-made, CAD,\nAI-generated, and Common Shapes\u2014PartUV outperforms existing tools and\nrecent neural methods in chart count and seam length, achieves comparable\ndistortion, exhibits high success rates on challenging meshes, and enables\nnew applications like part-specific multi-tiles packing. Our project page is\nat https://www.zhaoningwang.com/PartUV.\nCCS Concepts:\u2022Shape modeling \u2192Shape analysis;\u2022Mesh models \u2192\nMesh geometry processing;Parametrization;Geometric algorithms.\nACM Reference Format:\nZhaoning Wang, Xinyue Wei, Ruoxi Shi, Xiaoshuai Zhang, Hao Su, and Minghua\nLiu. 2025. PartUV: Part-Based UV Unwrapping of 3D Meshes. InSIGGRAPH\nAsia 2025 Conference Papers (SA Conference Papers \u201925), December 15\u201318,\n2025, Hong Kong, Hong Kong.ACM, New York, NY, USA, 14 pages. https:\n//doi.org/10.1145/3757377.3763843\n1 Introduction\nUV unwrapping projects the 3D surface of a mesh onto a 2D plane,\nassigning every 3D surface point a corresponding 2D UV coordinate.\nThis step is fundamental in 3D -content -creation pipelines because\nit enables detailed surface information-such as material properties\n(e.g., base -color, roughness, normal maps), along with auxiliary\nmaps like ambient occlusion and displacement\u2014to be efficiently\nstored and manipulated in 2D space.\nA principal component of UV unwrapping is surface parame-\nterization, which flattens the 3D surface while trying to preserve\ngeometric properties such as angles and areas. For meshes with\ncomplex geometry, flattening the entire surface into a single 2D\nchart introduces large distortion. Consequently, chart segmentation\n(or seam cutting) is typically used to divide the mesh into multiple\ncharts along strategically placed seams, allowing each chart to be\nflattened with reduced distortion. Finally, UV packing arranges the\nresulting charts within the unit square (the UV atlas) to maximize\ntexture-space use.\nAlthough UV unwrapping has been studied extensively, exist-\ning methods are typically tuned for well-behaved meshes, such as\nthose created by professional 3D artists. They often fail on more\ncomplex, AI -generated meshes. Such meshes, typically extracted\nfrom neural -field isosurfaces (e.g., via marching cubes [Lorensen\nand Cline 1998]), tend to have bumpy surfaces, many small trian-\ngles, and poor geometric quality (e.g., disconnected components\nor holes). On such data, existing methods may time -out or return\nextremely fragmented atlases in which a single chart holds only\none or a handful of triangles. This extreme fragmentation hampers\ntexture painting and editing, introduces texture -bleed and baking\nor rendering artifacts at chart boundaries, and burdens downstream\napplications with an unwieldy number of charts.\nSome recent approaches mitigate fragmentation [Srinivasan et al .\n2024; Zhang et al .2024; Zhao et al .2025] by representing the UV\nmapping using a neural field and optimizing such a field for each 3D\nshape from scratch. While these methods can effectively control the\nnumber of charts generated, they typically run for more than thirty\nminutes and still exhibit noticeable distortion. Other methods [Liet al.2018; Poranne et al .2017] jointly optimize seam length and\ndistortion but are likewise computationally expensive. Moreover,\nsome existing approaches like [L\u00e9vy et al .2002; Sorkine et al .2002;\nZhou et al .2004] segment charts or identify seams using heuristics\nbased on local geometric properties, rather than leveraging the\nconcept of geometric or semantic parts. This can lead to unintuitive\nor suboptimal chart boundaries that split semantically coherent\nregions across multiple charts, further complicating downstream\ntasks such as texture authoring, part-based editing, and semantic-\naware rendering.\nIn this paper, we introduce PartUV, a part-based UV unwrap-\nping pipeline for 3D meshes that generates UV mappings with a\nsmall number of part-aligned charts while maintaining low distor-\ntion, as well as robust and efficient processing\u2014typically completed\nwithin a few to several tens of seconds. PartUV builds on a recent\nlearning -based method, PartField [Liu et al .2025], which produces\na hierarchical part tree for the input mesh. PartUV also proposes\nseveral novel geometric heuristics that further decompose simple\nlocal parts into charts that can be flattened with minimal distortion.\nCombining high -level semantic decomposition from PartField with\nfine-grained geometric cuts, PartUV employs a top -down recursive\nsearch that minimizes chart count while keeping each chart\u2019s distor-\ntion below a user -specified threshold. PartUV uses established sur-\nface parameterization algorithms (e.g., ABF++ [Sheffer et al .2005])\nfor chart flattening and proven packing algorithms for optimal atlas\nlayout. To ensure high speed and robustness, the pipeline incorpo-\nrates extensive parallelization and acceleration strategies, as well as\ndedicated handling of non-manifold and degenerate cases.\nBy explicitly incorporating semantic priors, our approach yields\nseveral key benefits. First, semantics improve decomposition by\nreducing excessive reliance on local geometry, which often causes\nover-segmentation and long runtimes. Second, semantic cues pre-\nserve the coherence of object parts, preventing chart boundaries\nfrom cutting through meaningful regions (e.g., across the flat surface\nof a TV screen, as in Figure 7, or a human face), thereby facilitating\nediting and rendering tasks. Third, semantic grouping naturally\nsupports better chart packing strategies, allowing related charts\nto be organized together\u2014either within a single atlas (top right\nof Figure 1) or across multiple atlases (bottom right of Figure 1).\nThis enhances organizational clarity and simplifies the process of\nlocating and editing related charts. Finally, seams guided by seman-\ntic boundaries tend to fall in perceptually unobtrusive locations,\nimproving the visual quality of textured models.\nWe evaluate PartUV on four datasets spanning man-made, AI-\ngenerated, CAD models, and common 3D shapes (e.g., Stanford\nBunny, XYZ Dragon), and compare it against widely used UV un-\nwrapping tools such as xatlas [Young 2019], Blender [Commu-\nnity [n.d.]], and Open3D [Zhou et al .2018a], as well as the recent\nneural-based methods [Srinivasan et al .2024]. As shown in Fig-\nure 1, our method decomposes input meshes into significantly fewer\ncharts\u2014also resulting in shorter seam lengths\u2014while maintaining\nlow angular and area distortion comparable to baseline methods. The\nincorporation of explicit part priors not only helps the segmented\ncharts better align with part boundaries but also enables new appli-\ncations, such as packing semantic-aligned parts into separate atlases.\nMoreover, PartUV maintains a high success rate, handles a wide\nSA Conference Papers \u201925, December 15\u201318, 2025, Hong Kong, Hong Kong.\nPartUV: Part-Based UV Unwrapping of 3D Meshes\u20223\nrange of meshes (including large, complex, and non-manifold ones),\nand processes each mesh quickly\u2014typically within a few to several\ntens of seconds.\n2 Related Work\n2.1 Mesh Parameterization\nDisregarding mesh decomposition, mesh parameterization\u2014the pro-\ncess of mapping a 3D mesh to a 2D domain while minimizing various\ndistortion metrics (e.g., isometric, conformal, equiareal)\u2014is a funda-\nmental operation in UV unwrapping [Rabinovich et al .2017; Sawh-\nney and Crane 2017; Sch\u00fcller et al .2013]. Many methods directly\noptimize UV coordinates based on mesh connectivity by solving\nlinear or nonlinear systems under fixed or free boundary conditions.\nThese include barycentric embeddings like Tutte\u2019s embedding [Tutte\n1963], Laplacian eigen-decomposition techniques [Belkin and Niyogi\n2003; Mullen et al .2008; Taubin 1995], and distortion-minimization\nmethods [Ben-Chen et al .2008; Desbrun et al .2002; Hormann and\nGreiner 2000; Ray and L\u00e9vy 2003; Sander et al .2001; Su et al .2016;\nYan et al .2005; Yueh et al .2019], such as Least Squares Conformal\nMaps (LSCM) [L\u00e9vy et al .2002]. Some adopt local/global strate-\ngies that combine per-triangle transformations (e.g., rotations) with\nglobal stitching [Alexa et al .2023; Liu et al .2008; Sorkine and Alexa\n2007]. In contrast, Angle-Based Flattening (ABF) methods [Sheffer\nand De Sturler 2000; Sheffer and de Sturler 2001; Sheffer et al .2005;\nZayer et al .2007] optimize triangle angles before converting them\ninto UVs, typically offering better angle preservation than LSCM\nbut at higher computational cost due to nonlinear solvers. In this\npaper, we adopt ABF for base unfolding, integrating it with various\nmesh decomposition strategies.\n2.2 Mesh Segmentation for UV Unwrapping\nMesh segmentation is a key step in UV unwrapping, aiming to divide\na complex 3D mesh into simpler patches, or charts, each of which\ncan be flattened with minimal distortion. Existing approaches can be\nbroadly categorized into four main strategies. Top-down methods re-\ncursively subdivide the mesh, often using spectral analysis [Liu and\nZhang 2007a,b; Pothen et al .1990] or cuts along feature lines [Zhang\net al.2005] and high-curvature regions [Lavou\u00e9 et al .2005]. Bottom-\nup aggregation [Bhargava et al .2025; Julius et al .2005; Kalvin and\nTaylor 1996; L\u00e9vy et al .2002; Pulla et al .2001; Sorkine et al .2002;\nTakahashi et al .2011; Yamauchi et al .2005; Zhou et al .2004] grows\ncharts from small seeds (e.g., triangles), merging them based on\nplanarity or distortion bounds to maximize chart size under quality\nconstraints. Cut optimization [Carr et al .2006; Gu et al .2002; Li\net al.2018; Pietroni et al .2009; Poranne et al .2017; Sander et al .\n2003; Takahashi et al .2011] focuses on identifying optimal seam\nnetworks that minimize seam length, reduce flattening distortion, or\nbetter suit specific parameterization goals. Clustering-based meth-\nods [Katz and Tal 2003; Roy 2023] group mesh elements based on\ngeometric similarity to form coherent segments. Commonly used\ntools such as xatlas [Young 2019], Blender [Community [n.d.]], and\nOpen3D [Zhou et al .2018a] primarily adopt bottom-up aggregation\nstrategies that rely solely on local geometric properties. In contrast,PartUV integrates high-level semantic part priors from a learning-\nbased module with novel local geometric decomposition strategies\nvia a top-down recursive tree search.\n2.3 Learning-Based Parameterization and Segmentation\nRepresenting UV mappings with neural networks has attracted\nattention due to their differentiability and composability. Neural\nSurface Maps [Morreale et al .2021] pioneered this direction by\nlearning mappings across collections of surfaces. Several approaches\njointly optimize UV parameterization and 3D reconstruction from\nmulti-view images using cycle-consistency and distortion-based\nlosses [Das et al .2022; Xiang et al .2021]. Others, such as NUVO [Srini-\nvasan et al .2024], solve for parameterizations directly from sampled\n3D points. More recent methods [Zhang et al .2024; Zhao et al .2025]\ndesign neural networks to emulate physical operations, including\nface cutting, UV deformation, and unwrapping. However, all the\naforementioned techniques typically rely on per-shape optimization,\nmaking them computationally expensive and often requiring tens\nof minutes per shape. A few feedforward models address specific\ntasks more efficiently, such as intra-category texture transfer [Chen\net al. 2022] and low-distortion patch selection [Liu et al. 2023].\nTo enhance UV chart decomposition, we explore the integration\nof semantic part priors. Traditional learning-based part segmen-\ntation methods [Jiang et al .2020; Qian et al .2022; Vu et al .2022]\nare restricted to closed-set categories due to the limited scale of\npart-annotated datasets [Mo et al .2019; Yi et al .2016]. Recent ap-\nproaches [Guo et al .2024; He et al .2024; Xu et al .2023, 2024; Yang\net al.2023; Yin et al .2024; Zhou et al .2018a] lifting priors from 2D\nvision to 3D models [Kirillov et al .2023; Li et al .2022] for open-\nworld 3D part segmentation but rely on per-shape optimization,\nleading to slow runtimes and noise sensitivity. In contrast, a re-\ncent method, PartField [Liu et al .2025], introduces a feedforward\nmodel that learns part-aware feature fields for fast, hierarchical 3D\npart decomposition. While semantic priors intuitively benefit UV\nunwrapping, na\u00efve integration can yield suboptimal results. Par-\ntUV addresses this with a novel top-down strategy that interleaves\nsemantic segmentation and geometric flattening.\n3 Method\n3.1 Overview\nFor a 3D meshM=(\ud835\udc49,\ud835\udc39) , PartUV decomposes the mesh faces into\na small collection of disjoint charts:\n\ud835\udc39=\ud835\udc3e\u00d8\n\ud835\udc58=1\ud835\udc36\ud835\udc58,with\ud835\udc36 \ud835\udc56\u2229\ud835\udc36\ud835\udc57=\u2205 (\ud835\udc56\u2260\ud835\udc57),(1)\nwhere eachchart \ud835\udc36\ud835\udc58is a connected subset of faces. We utilize an\nAngle-Based Flattening algorithm(ABF++), to flatten each chart onto\na 2D plane, yielding mappings:\n\ud835\udf19\ud835\udc58:\ud835\udc36\ud835\udc58\u2212\u2192R2,(2)\nso that each vertex \ud835\udc63\ud835\udc56\u2208\ud835\udc49receives a corresponding 2D UV coordi-\nnateu\ud835\udc56=\ud835\udf19\ud835\udc58(\ud835\udc63\ud835\udc56)\u2208R2, determined by the chart it belongs.\nThe primary challenge lies in decomposing the mesh into charts\n{\ud835\udc36\ud835\udc58}, a task essentially equivalent to identifying optimal seams for\nmesh segmentation. Many state-of-the-art methods, such as xat-\nlas and Blender\u2019s Smart UV, rely on geometric heuristics\u2014such as\nSA Conference Papers \u201925, December 15\u201318, 2025, Hong Kong, Hong Kong.\n4\u2022Zhaoning Wang, Xinyue Wei, Ruoxi Shi, Xiaoshuai Zhang, Hao Su, and Minghua Liu\nFig. 2.Pipeline of PartUV.Given a mesh M, we first leverage the learning-based method PartField to predict a part-aware feature field. By applying a\nclustering algorithm to this field, we obtain a hierarchical part treeTfor the input meshM. We then recursively traverse the part treeTstarting from the\nroot node. For each visited node P, we apply two novel geometry-based strategies to segment the corresponding part mesh into multiple sets of charts \ud835\udc9e.\nEach chart in the set is then flattened using the ABF algorithm [Sheffer et al. 2005], and its distortion is evaluated. If the distortion exceeds a user-specified\nthreshold\ud835\udf0f, we recursively traverse the left and right children of the part tree T. Otherwise, we adopt the segmented charts and their corresponding UV\nmappings for the part mesh.\nbottom-up greedy chart expansion based on face normals or distor-\ntion metrics\u2014to generate these charts. However, these strategies\ntypically produce overly fragmented charts and unintuitive or sub-\noptimal boundaries, often splitting semantically coherent regions\nacross multiple charts.\nIn contrast, PartUV adopts a coarse-to-fine, two-stage strategy for\nmesh decomposition. At a high level, it employs a recent learning-\nbased method called PartField (introduced in Section 3.2) to parti-\ntion mesh faces into semantically coherentparts: \ud835\udc39=\u00d0\ud835\udc40\n\ud835\udc5a=1\ud835\udc43\ud835\udc5a,\nwhere each part \ud835\udc43\ud835\udc58exhibits relatively simple geometry\u2014for instance,\ncylindrical limbs, spherical toes. Subsequently, we introduce two\ngeometry-based heuristics (detailed in Section 3.4) to further seg-\nment each part into a small set ofcharts: \ud835\udc43\ud835\udc5a=\u00d0\ud835\udc41\ud835\udc5a\n\ud835\udc5b=1\ud835\udc36\ud835\udc5a,\ud835\udc5b, ensuring\neach resulting chart \ud835\udc36\ud835\udc5a,\ud835\udc5bcan be flattened onto a 2D plane with\nminimal distortion.\nTo accomplish this, PartUV employs a top-down recursive de-\ncomposition search (detailed in Section 3.3) that minimizes the total\nnumber of generated charts while ensuring the distortion for each\nchart remains below a user-specified threshold \ud835\udf0f. Since exhaustive\ndecomposition searches can incur substantial computational over-\nhead, we introduce acceleration and parallelization techniques for\nefficiency. To ensure a complete and robust pipeline, Section 3.5 also\ndescribes additional preprocessing and postprocessing procedures,\nincluding handling non-manifold and multi-component meshes, as\nwell as performing UV packing.\n3.2 Preliminary: PartField\nPartField [Liu et al .2025] trains a feed -forward neural network that\ntakes a 3D mesh as input and predicts a continuous, part-based fea-\nture field encoded as a triplane. By leveraging extensive contrastive\nlearning on part-labeled 3D data and large -scale unlabeled 3D data\nwith 2D pseudo part labels, PartField learns general hierarchical\nconcepts of semantic and geometric parts. For any 3D point, we\ncan obtain its high -dimensional part feature by interpolating thetriplane representation. Points whose features are similar\u2014as mea-\nsured by cosine similarity\u2014are therefore more likely to belong to\nthe same part.\n3.3 Top-Down Recursive Tree Search\nAfter obtaining the hierarchical part-based feature field using Part-\nField, we first compute a representative part feature for each trian-\ngular face by uniformly sampling \ud835\udc603D points within the face and\naveraging their corresponding point features. Next, we construct a\nhierarchical part tree Tusing agglomerative clustering [Johnson\n1967] on these face features. In this hierarchical tree structure, the\nleaf nodes represent individual triangular faces, and the root node\nencompasses the entire mesh.\nA straightforward approach to mesh decomposition might in-\nvolve using PartField to directly generate a fixed number of parts\nand attempting to flatten each to 2D individually. Alternatively,\none could adaptively traverse Tfrom the root downward, checking\nwhether each node\u2019s corresponding geometry can be flattened into a\n2D chart with minimal distortion. However, we observe that relying\nsolely on parts generated by PartField often leads to suboptimal\nresults. This limitation arises because PartField primarily focuses\non semantic or coarse geometric partitioning and is less effective\nfor finer-scale, UV-related decompositions\u2014such as accurately seg-\nmenting cylindrical or spherical regions into charts that minimize\ndistortion. To address this challenge, PartUV leverages PartField\nprimarily for high-level semantic decomposition into geometrically\nsimpler subparts and employs two geometry-based heuristics (elabo-\nrated in Section 3.4) to further divide these semantically meaningful\nparts into smaller charts amenable to low-distortion flattening. The\nsearch algorithm interleaves these two strategies.\nFormally, PartUV employs a top-down recursive decomposition\nsearch\u2014detailed in Algorithm 1\u2014to optimally balance the chart\ncount against distortion constraints. Given a subtree node P(ini-\ntially the root node of the tree T), a distortion threshold \ud835\udf0f, and a\nSA Conference Papers \u201925, December 15\u201318, 2025, Hong Kong, Hong Kong.\nPartUV: Part-Based UV Unwrapping of 3D Meshes\u20225\nAlgorithm 1PartTreeSearch\nRequire:P (a PartField subtree node), distortion threshold \ud835\udf0f, chart\nbudget\ud835\udc35\nEnsure: AUVChartSet \ud835\udc9e={\ud835\udc36 1,\ud835\udc362,\u00b7\u00b7\u00b7,\ud835\udc36\ud835\udc58}forP\u2019s mesh, whose\ncharts\ud835\udc36\ud835\udc56have distortion\u2264\ud835\udf0fand whose total chart count \ud835\udc58is\n\u2264\ud835\udc35; return\u22a5if no such set exists.\n1:procedurePartTreeSearch(P,\ud835\udf0f,\ud835\udc35)\n2:if\ud835\udc35<1then\u22b2budget exhausted\n3:return\u22a5\n4:end if\n5:H 1\u2190GenCandidatesH1(P.\ud835\udc5a\ud835\udc52\ud835\udc60\u210e,10)\u22b2Heuristic1\n6:for all\ud835\udc9e\u2208H 1do\u22b2\ud835\udc9eis one of candidate\ud835\udc48\ud835\udc49\ud835\udc36\u210e\ud835\udc4e\ud835\udc5f\ud835\udc61\ud835\udc46\ud835\udc52\ud835\udc61\n7: ParameterizeABF( \ud835\udc9e)\u22b2flatten and compute distortion\n8:end for\n9:ifmin \ud835\udc9e\u2208H 1\ud835\udc9e.dist>\ud835\udf0fthen\u22b2no admissible H1 candidate\n10:\ud835\udc3f\u2190PartTreeSearch(P.\ud835\udc59\ud835\udc52\ud835\udc53\ud835\udc61_\ud835\udc50\u210e\ud835\udc56\ud835\udc59\ud835\udc51,\ud835\udf0f,\u221e)\n11:\ud835\udc45\u2190PartTreeSearch(P.\ud835\udc5f\ud835\udc56\ud835\udc54\u210e\ud835\udc61_\ud835\udc50\u210e\ud835\udc56\ud835\udc59\ud835\udc51,\ud835\udf0f,\u221e)\n12:return\ud835\udc3f\u2295\ud835\udc45\u22b2merge chart sets from the subtrees\n13:else\n14:H 2\u2190{GenCandidateH2(P.\ud835\udc5a\ud835\udc52\ud835\udc60\u210e,\ud835\udf0f)}\u22b2Heuristic2\n15:ParameterizeABF(H 2[0])\n16:S\u2190\b\n\ud835\udc9e\u2208H 1\u222aH 2|\ud835\udc9e.dist\u2264\ud835\udf0f\u2227NoOverlap(\ud835\udc9e)\t\n17:\ud835\udc9e best\u2190argmin \ud835\udc9e\u2208S\ud835\udc9e.count\n18:\u22b2 Recurse to determine whether a better solution exists\n19:\ud835\udc35\u2032\u2190min(\ud835\udc35,\ud835\udc9e best.count\u22121)\n20:\ud835\udc3f\u2190PartTreeSearch(P.\ud835\udc59\ud835\udc52\ud835\udc53\ud835\udc61,\ud835\udf0f,\ud835\udc35\u2032\u22121)\n21:\ud835\udc45\u2190PartTreeSearch(P.\ud835\udc5f\ud835\udc56\ud835\udc54\u210e\ud835\udc61,\ud835\udf0f,\ud835\udc35\u2032\u2212\ud835\udc3f.count)\n22:\ud835\udc9e comb\u2190\ud835\udc3f\u2295\ud835\udc45\n23:\ud835\udc56\ud835\udc60_\ud835\udc63\ud835\udc4e\ud835\udc59\ud835\udc56\ud835\udc51\u2190\ud835\udc9e comb.dist\u2264\ud835\udf0f\u2227NoOverlap(\ud835\udc9e comb)\n24:if\ud835\udc9e comb.count<\ud835\udc9e best.count\u2227\ud835\udc56\ud835\udc60_\ud835\udc63\ud835\udc4e\ud835\udc59\ud835\udc56\ud835\udc51then\n25:\ud835\udc9e best\u2190\ud835\udc9e comb\n26:end if\n27:return\ud835\udc9e best\n28:end if\n29:end procedure\nchart budget \ud835\udc35(initially set to\u221e), the algorithm searches for a valid\nchart decomposition of P\u2019s mesh that satisfies three key conditions:\neach chart has a distortion of at most \ud835\udf0f, no charts overlap in 2D,\nand the total number of charts does not exceed \ud835\udc35. If no feasible\ndecomposition exists, the algorithm returns failure (\u22a5).\nSpecifically, the algorithm begins by generating candidate chart\ndecompositions for node P\u2019s mesh using a primary geometric heuris-\ntic (Heuristic1). Each candidate chart set \ud835\udc9e={\ud835\udc36\ud835\udc56}, consisting of\nup to\ud835\udc61charts, is flattened using Angle-Based Flattening (ABF), and\nits distortion is evaluated. If none of the candidate decompositions\nwith up to\ud835\udc61charts from Heuristic1 satisfy the distortion constraint\n(i.e.,\ud835\udeffmin>\ud835\udf0f), the algorithm utilizes the PartField tree to divide the\nmesh and recursively searches both the left and right child subtrees\nwithout budget constraints, then merges their respective optimal\nchart sets.\nConversely, if an admissible candidate decomposition is found\nthrough Heuristic1, we further refine it using a secondary heuristic\n(Heuristic2) designed to potentially yield fewer charts. Among thesecandidates, we select the best solution \ud835\udc9ebestwith the minimal chart\ncount that satisfies the distortion and overlap constraints.\nBefore accepting this solution, the algorithm performs a final\ncheck by recursively exploring the left and right child nodes of the\nPartField subtree, using a reduced chart budget ( \ud835\udc35\u2032), which is derived\nfrom the best candidate\u2019s chart count minus one. If the combined\nsolution from this subtree search yields a valid and superior de-\ncomposition (i.e., fewer charts than \ud835\udc9ebest), it replaces the previously\nidentified best solution. The chart budget \ud835\udc35prevents the search from\ngoing too deep. As the search progresses, \ud835\udc35tightens, and recursion\nstops when candidates exceed it. In practice, the search rarely goes\nvery deep.\nBy strategically interleaving hierarchical semantic guidance from\nPartField with fine-grained geometric heuristics and systematic\nrecursive exploration, our proposed decomposition search achieves\nsemantically coherent, distortion-bounded, and notably compact\nmesh parameterizations\u2014characterized by a small number of charts.\n3.4 Geometry-Based Part Decomposition\nIn this section, we introduce two heuristics,NormalandMerge, to\nfurther decompose a part \ud835\udc43\ud835\udc5agenerated by PartField and exhibiting\nsimple geometry. The goal is to divide \ud835\udc43\ud835\udc5ainto multiple charts,\ndenoted as\ud835\udc43\ud835\udc5a=\u00d0\ud835\udc41\ud835\udc5a\n\ud835\udc5b=1\ud835\udc36\ud835\udc5a,\ud835\udc5b, such that each chart can be flattened\nto 2D with low distortion.\nThe first heuristic, referred to asNormal(line 5 in Algorithm 1), is\nbased on face normals. We apply an agglomerative clustering algo-\nrithm [Johnson 1967] to the face normals of \ud835\udc43\ud835\udc5a\u2019s mesh, partitioning\nits triangle faces into charts, where each chart is composed of con-\nnected faces with similar normals. This clustering is performed once\nfor\ud835\udc43\ud835\udc5a, producing\ud835\udc61candidate decompositions with 1 to \ud835\udc61charts (we\nuse\ud835\udc61=10in our experiments). For each candidate decomposition,\nwe apply the Angle-Based Flattening (ABF) algorithm to flatten the\ncharts and evaluate distortion. Since ABF aims to preserve angles,\nwe quantify distortion using an area stretch metric, defined as:\ndistortion(\ud835\udc9e)=max\n\ud835\udc36\u2208\ud835\udc9e\u00a9\u00ad\n\u00ab1\n|\ud835\udc36|\u2211\ufe01\n\ud835\udc53\u2208\ud835\udc36max\u0012\nstretch(\ud835\udc53),1\nstretch(\ud835\udc53)\u0013\n\u00aa\u00ae\n\u00ac,(3)\nwhere\ud835\udc36denotes a single chart composed of multiple connected\nfaces, and \ud835\udc9edenotes the set of all charts in the decomposition. The\nstretch of a triangle face\ud835\udc53is computed as:\nstretch(\ud835\udc53)=area2\ud835\udc37(\ud835\udc53)\narea3\ud835\udc37(\ud835\udc53),\n\u00a9\u00ad\n\u00ab1\n|\ud835\udc36|\u2211\ufe01\n\ud835\udc53\u2032\u2208\ud835\udc36area2\ud835\udc37(\ud835\udc53\u2032)\narea3\ud835\udc37(\ud835\udc53\u2032)\u00aa\u00ae\n\u00ac.(4)\nNote that both PartField and theNormalheuristic employ the ag-\nglomerative clustering algorithm to group faces into parts or charts.\nThe key difference lies in the features used: PartField utilizes learned\nhigh-level part features, while heuristicNormalrelies on low-level\ngeometric face normals. These two strategies are thus consistent in\nspirit and complementary in practice.\nTheNormalheuristic is simple, fast, and often yields satisfactory\nresults. However, we also propose a second, more computationally\nexpensive heuristic, calledMerge(line 14 in Algorithm 1), which\nmay produce decompositions with fewer charts. Given a part \ud835\udc43\ud835\udc5a,\ntheMergeheuristic begins by computing its oriented bounding\nbox (OBB). It then assigns each triangle face a label from 1 to 6,\ncorresponding to the OBB face normal with which the triangle\u2019s\nSA Conference Papers \u201925, December 15\u201318, 2025, Hong Kong, Hong Kong.\n6\u2022Zhaoning Wang, Xinyue Wei, Ruoxi Shi, Xiaoshuai Zhang, Hao Su, and Minghua Liu\nnormal is most closely aligned. Using both face connectivity and\nthese labels, we segment the faces into multiple connected compo-\nnents. These components are then sorted by size (small to large), and\nwe iteratively attempt to merge each component with its adjacent\nneighbors, starting from the one with the longest shared edges. For\neach merge attempt, we temporarily merge two components and\napply ABF to flatten the combined chart. The merge is accepted\nif the resulting chart satisfies the distortion threshold and is free\nof overlaps. This merging process continues until no further valid\nmerges can be made, after which the final chart set is returned. Since\ntheMergeheuristic often begins with many small components and\nperforms multiple ABF calls during its iterative merging process, it\nis significantly more expensive than theNormalheuristic. However,\nit may yield better decompositions with fewer charts. Therefore, we\nonly invoke theMergeheuristic when theNormalheuristic returns\na valid (admissible) decomposition.\n3.5 Runtime Optimization and Pre- and Post-processing\nRuntime OptimizationTo ensure efficiency despite the compu-\ntational cost of recursive decomposition and repeated ABF invo-\ncations, we adopt two key strategies during the decomposition\nprocess. First, we parallelize all recursive calls to the left and right\nsubtrees during the top-down search, allowing the algorithm to ex-\nploit multi-core processing and significantly accelerate the overall\ndecomposition. Second, to avoid the high cost of repeatedly invok-\ning Angle-Based Flattening (ABF) on dense meshes, we employ a\nGPU-accelerated mesh simplification algorithm [Oh et al .2025] to\ngenerate low-resolution approximations of candidate charts. During\nsimplification, the chart boundary is kept fixed to preserve the geo-\nmetric structure relevant to UV mapping. These simplified charts\nare used to estimate distortion metrics quickly (i.e., surrogate dis-\ntortion) during intermediate evaluations. Once the final chart set is\ndetermined, ABF is applied to the original high-resolution mesh to\nproduce accurate UV coordinates and distortion measurements.\nNon-Manifold and Multi-Component MeshesSince the ABF\nalgorithm assumes each input chart is a manifold and connected sur-\nface, additional processing is required when handling non-manifold\nor multi-component meshes. For non-manifold meshes, we detect all\nnon-manifold edges\u2014i.e., edges shared by more than two faces\u2014and\nresolve them by duplicating the shared vertices and splitting each\nsuch edge into \ud835\udc41\u22121distinct edges, where \ud835\udc41is the number of inci-\ndent faces, thereby converting the structure into a manifold form\nsuitable for flattening. For meshes containing multiple connected\ncomponents, we initially proceed with the proposed PartField-guided\nhierarchical decomposition. If a part mesh consists of disconnected\ncomponents, we skip heuristic-based decomposition at that level\nand instead recursively explore the left and right subtrees of the Part-\nField hierarchy. However, if PartField fails to further segment the\nmultiple components after reaching a predefined recursion depth,\nwe fall back to applying the geometric heuristics and ABF flattening\nto each connected component at that level individually, in order to\navoid excessively fragmented decomposition.\nUV PackingWhile our primary focus is on decomposing 3D meshes\ninto charts and generating corresponding low-distortion 2D parame-\nterizations, our method is fully compatible with a variety of existingUV packing algorithms. A distinguishing feature of our approach\nis that chart decompositions are grouped based on semantically\nmeaningful parts, enabling more structured and application-aware\npacking strategies. For example, charts belonging to the same part\ncan be grouped together during packing. Alternatively, part groups\ncan be packed into an arbitrary number of UV atlas squares (e.g.,\nmultiple[0,1]2spaces) with a semantically balanced distribution\nacross atlases. This semantic hierarchy not only improves organi-\nzational clarity but also benefits downstream applications such as\ntexture painting or editing, where charts belonging to the same part\nremain spatially close and are easier to manipulate collectively.\n4 Experiments\n4.1 Implementation Details and Evaluation Setup\nImplementation Details.We train PartField [Liu et al .2025] on\nObjaverse [Deitke et al .2022] using 8 NVIDIA H100 GPUs for 15\ndays. During inference, we sample 10 points per face and aver-\nage their features for face clustering and part tree construction.\nThe core pipeline of PartUV is implemented in C++17, and we use\nUVPackmaster [\u0141ukasz Czy\u017c 2025] for final group-based UV pack-\ning. A distortion threshold of 1.25 is used in all experiments. For\nABF++, we run 5 iterations per call, and follow Blender to set the\ngradient early-stop condition. Mesh simplification is controlled by a\ncurvature-related threshold of 1e-4 and a maximum iteration count\nof 1,000. All parameters are fixed across experiments. All meth-\nods are evaluated on a cluster node with a 96-core Intel\u00aeXeon\u00ae\nPlatinum 8468 CPU and an NVIDIA H100 GPU.\nEvaluation Datasets.To comprehensively evaluate the approaches\nacross diverse mesh sources, qualities, and styles, we use four datasets:\n(a)Common Shapes[Jacobson 2013], a GitHub repository of 24\nwidely used models in graphics (e.g., Bimba, Igea, Stanford Bunny)\nwith provided processed .obj files; (b)PartObjaverseTiny[Yang et al .\n2024], a 200-shape subset of Objaverse [Deitke et al .2022] featur-\ning high-quality, man-made meshes with multiple components and\nsmooth surfaces; (c)ABC[Koch et al .2019], a CAD dataset of me-\nchanical models combining sharp and smooth features\u2014we use\nthe first 100 meshes from its initial batch; and (d)Trellis[Xiang\net al.2024], which includes 114 AI-generated meshes from a recent\n3D diffusion-based generative model. These meshes are typically\nnoisy and geometrically low-quality, posing greater challenges than\nhuman-made counterparts.\nEvaluation Metrics.We evaluate the quality of the generated UV\nmaps from four perspectives: (1)Number of Charts: For each shape,\nwe count the number of charts and report both the average and\nmedian values across the dataset. (2)Seam Length: We compute the\nseam length by summing the lengths of all chart boundary edges,\nwith UV coordinates normalized to a [0, 1] grid. The median value\nis reported across the dataset. (3)Angular (Conformal) Distortion:\nWe compute the cosine between the tangent and bitangent vectors\nof each face. The distortion for a shape is defined as one minus the\naverage cosine across all faces [Srinivasan et al .2024]. We report the\naverage distortion across the dataset. (4)Area (Equiareal) Distortion:\nWe compute botharea distortionandoverall area distortion.Area\ndistortionis defined as in Equation 3, based on the chart with the\nhighest distortion in each shape.Overall area distortionis computed\nSA Conference Papers \u201925, December 15\u201318, 2025, Hong Kong, Hong Kong.\nPartUV: Part-Based UV Unwrapping of 3D Meshes\u20227\nFig. 3.Quantitative comparison between Blender [Community [n.d.]], xatlas [Young 2019], and PartUV.Unlike Blender and xatlas, which rely solely\non local geometric properties for UV unwrapping, PartUV integrates high-level semantic priors with low-level geometric heuristics, enabling part-based chart\ndecomposition. As a result, it produces significantly fewer charts, with boundaries that better align with semantic parts.\nby aggregating all triangles across all charts, calculating individual\ndistortions, and averaging them. For both metrics, we first compute\nper-shape values and then average them across the dataset. All\ntriangle-level distortion values are clipped to a maximum of 10.\nNotably,overall area distortionmay be smoothed by the number of\nfaces, whilearea distortionmore effectively highlights problematic\nregions in the UV maps. We did not report stretch \ud835\udc3f2and\ud835\udc3f\u221e[Sander\net al.2001] since they are per-triangle metrics that can diverge to\ninfinity when triangles flip or have near-zero area\u2014issues that are\ncommon in baselines such as xatlas, making direct comparison less\nmeaningful.4.2 Comparison with Baselines\nBaselines.We compare PartUV with commonly used tools\u2014Blender\u2019s\nSmart UV Project [Community [n.d.]], xatlas [Young 2019], and\nOpen3D\u2014as well as optimization-based methods, including Nuvo [Srini-\nvasan et al. 2024] and OptCuts [Li et al. 2018].\nBlender, xatlas, and Open3D decompose meshes into charts us-\ning bottom-up strategies guided by local geometric priors. Blender\nclusters triangles based on mesh normals and flattens each chart\nusing simple planar projection. xatlas employs a greedy algorithm\nthat balances geometric deviation, UV distortion, and seam cost,\nfollowed by Least Squares Conformal Maps (LSCM) [L\u00e9vy et al .\nSA Conference Papers \u201925, December 15\u201318, 2025, Hong Kong, Hong Kong.\n8\u2022Zhaoning Wang, Xinyue Wei, Ruoxi Shi, Xiaoshuai Zhang, Hao Su, and Minghua Liu\nFig. 4. Open3D not only suffers from a limited success rate but also produces\nresults with large distortion, which may reduce their practical utility.\nFig. 5. Nuvo and OptCuts incur significantly longer optimisation times for\neach shape. Nuvo\u2019s results also exhibit large distortion.\n2002] for flattening. Open3D builds on Microsoft\u2019s UVAtlas [Corpo-\nration 2023; Sander et al .2002; Zhou et al .2004], which uses region\ngrowing guided by the isometric stretch metric and flattens charts\nwith LSCM. Among optimization-based methods, Nuvo [Srinivasan\net al.2024] learns a continuous UV mapping via a neural field by\nminimizing distortion losses with various regularizations. OptCuts\njointly optimizes surface cuts and distortion under a global bijectiv-\nity constraint. Both methods require significantly longer runtimes\nper mesh.\nResults.As shown in Table 1, Blender and xatlas often produce\nover-fragmented charts, whereas PartUV generates UV maps with\nsignificantly fewer charts. For instance, on the Common Shapes\ndataset, PartUV uses only 1/31 as many charts as Blender. Conse-\nquently, it also results in shorter seam lengths. PartUV maintains\nlow levels of both angular and area distortion, while xatlas mayTable 1.Quantitative comparison between Blender [Community\n[n.d.]], xatlas [Young 2019], and PartUV.\nsuccess average median median overall\ndataset method rate # of # of seam angular area area time\n(%) charts\u2193charts\u2193length\u2193distort.\u2191distort.\u2193distort.\u2193(s)\nCommon Shapes\n(24 shapes)Blender 100.0 1360.3 332.5 44.7 0.906 1.172 1.102 0.3\nxatlas 100.0 974.8 301.0 42.9 0.987 1.885 1.504 77.9\nours 100.043.6 15.0 16.80.982 1.404 1.131 54.0\nTrellis\n(114 shapes)Blender 100.0 3352.9 1957.0 94.5 0.921 1.252 1.107 1.1\nxatlas 100.0 1541.6 895.0 91.2 0.984 2.357 1.093 13.1\nours 100.0538.8 221.5 55.90.961 1.300 1.135 41.9\nABC\n(100 shapes)Blender 100.0 305.3 78.0 25.0 0.992 1.122 1.067 0.7\nxatlas 100.0 249.6 56.0 26.5 1.000 1.192 1.030 31.0\nours 100.074.0 18.0 19.80.999 1.175 1.058 43.1\nPart\nObjaverse\nTiny (200 shapes)Blender 100.0 1509.2 647.5 70.2 0.925 1.325 1.115 0.2\nxatlas 100.0 1142.1 491.5 67.0 0.982 1.728 1.286 4.4\nours 100.0463.3 155.5 39.30.954 1.271 1.112 21.0\nTable 2.Quantitative comparison between Open3d [Zhou et al .\n2018b] and PartUV.Note that Open3D has a limited success rate, and\nthe reported numbers are averaged over the easier cases it successfully\ncompletes. Despite this, Open3D still exhibits large distortion.\nsuccess average overall average median median\ndataset method rate angular area area # of # of seam time\n(%) distort.\u2191distort.\u2193distort.\u2193charts\u2193charts\u2193length\u2193(s)\nCommon ShapesOpen3d 79.2 0.852 1.509 1.191 19.8 12.0 11.8 19.8\nours 100.0 0.987 1.281 1.12824.4 12.0 15.1 52.3\nTrellisOpen3d 39.5 0.859 1.931 1.264 79.8 40.0 25.9 24.1\nours 100.0 0.984 1.308 1.14497.9 51.0 27.4 23.7\nABCOpen3d 83.0 0.878 1.459 1.162 15.0 8.0 9.3 17.6\nours 100.0 0.994 1.171 1.06235.0 15.0 18.6 39.6\nPartObjaverse\nTinyOpen3d 52.5 0.799 2.772 1.295 161.5 80.0 25.6 10.0\nours 100.0 0.957 1.254 1.117227.1 91.0 30.6 14.0\nTable 3.Quantitative comparison between Nuvo [Srinivasan et al .\n2024] and PartUV.\nsuccess average overall average\ndataset method rate angular area area # of time\n(%) distortion\u2191distortion\u2193distortion\u2193charts\u2193(s)\nCommon Shapesnuvo 100.0 0.802 2.722 1.940 17.0 2908.8\nours 100.0 0.987 1.281 1.12824.4 52.3\nexhibit large area distortion on certain challenging shapes. Despite\nutilizing a more exhaustive decomposition search and a compu-\ntationally expensive ABF flattening algorithm to achieve higher\nquality, PartUV maintains a runtime comparable to xatlas, typically\ncompleting in tens of seconds. See Figure 3 for a qualitative compar-\nison, where PartUV leverages semantic part information to produce\nchart boundaries that align more closely with object semantics.\nWe report the comparison results with Open3D separately in Ta-\nble 2 due to its failure to complete some shapes within a reasonable\ntime. For example, on the challenging Trellis dataset, it achieves a\nsuccess rate of only 39.5%. In Table 2, we report the average perfor-\nmance only over the easy cases that Open3D successfully processes.\nFor these cases, Open3D achieves a similar number of charts and\nseam lengths compared to our method. However, this comes at the\ncost of significant distortion. For instance, while Blender, xatlas,\nand PartUV all achieve angular distortion scores mostly above 0.95\nacross all datasets, Open3D consistently falls below 0.9 and even 0.8\nin some cases. A similar phenomenon is observed for area distortion.\nPlease refer to Figure 4 and Figure 6 for qualitative examples, where\nOpen3D produces large distortions, rendering the UV mappings less\nsuitable for practical applications.\nSA Conference Papers \u201925, December 15\u201318, 2025, Hong Kong, Hong Kong.\nPartUV: Part-Based UV Unwrapping of 3D Meshes\u20229\nFig. 6. Texture-map comparison among Blender, xatlas, and our PartUV.\nCompared to Nuvo and OptCuts, we observe that although both\nmethods effectively reduce the number of charts, they typically re-\nquire significantly longer optimization times\u2014often exceeding 30\nminutes or even several hours. Moreover, Nuvo leads to substan-\ntially higher distortion, as shown in Table 3 and Figure 5. While\nOptCuts achieves low distortion, its success rate is limited: it pro-\nduces outputs for only 9 out of 24 shapes in the Common Shapes\ndataset. Additionally, neither method incorporates the concept of\nsemantic parts during optimization.\nSee supplementary material for additional qualitative examples\nUV efficiency comparisons, and analysis of Open3D results.\n4.3 Applications and Analysis\nIn this section, we demonstrate the benefits of using our PartUV:\nTexture Editing and Replacement.Since our UV mappings are\nless fragmented, texture maps can be more effectively edited or\nmodified in 2D space. In Figure 7, we show that our UV maps en-\nable clean placement of conference logos, whereas the UV maps\ngenerated by Blender or xatlas fail to do so due to their overly frag-\nmented layouts. Figure 8 further showcases an application where\nthe texture is replaced with various tiling patterns. Because xatlas\nand Blender often produce numerous small charts containing only\na few triangles, noticeable artifacts appear on the mesh surface. In\ncontrast, our method preserves significantly better visual quality.\nFig. 7. Unlike our baselines, which generate over-fragmented UV maps that\nhinder 2D texture editing, PartUV produces significantly fewer charts with\npart-aligned boundaries, enabling more effective 2D operations.\nFig. 8. Our UV map enables easy texture replacement, whereas xatlas /\nBlender maps cause severe artefacts due to over-segmentation.\nTexture Compression.UV maps always require padding between\ncharts. When the UV layout is over-segmented, more padding is\nneeded, which increases the risk of color bleeding. In Figure 10,\nwe demonstrate that reducing the UV map resolution from 1024 to\n128\u2014a common setting in mobile games\u2014results in noticeable color\nbleeding for textures generated by xatlas and Blender. In contrast,\nPartUV is free from such issues.\nMulti-Atlas Wrapping.As shown in Figure 1 and Figure 9, Par-\ntUV supports part-based UV packing. Given the desired number of\natlases, it can automatically extract semantic-meaningful parts and\npack across separate atlases, facilitating downstream applications\nsuch as texture editing.\nAdaptive Threshold Adjustment.PartUV allows users to specify\na distortion threshold \ud835\udf0f, enabling adaptive control over the trade-off\nbetween the number of charts and the distortion in the generated\nUV maps, as shown in Figure 11.\n4.4 Ablation Studies\nPartField.We integrate semantic part priors with geometric heuris-\ntics through an adaptive recursive tree search. A naive way to com-\nbine them is to first use PartField to decompose the shape into a\nfixed set of 20 parts, and then further decompose each part using\nthe two heuristics. As shown in Table 4(a), this naive combination\nresults in high distortion, as some parts may remain too complex to\nSA Conference Papers \u201925, December 15\u201318, 2025, Hong Kong, Hong Kong.\n10\u2022Zhaoning Wang, Xinyue Wei, Ruoxi Shi, Xiaoshuai Zhang, Hao Su, and Minghua Liu\nFig. 9. Our part-based UV unwrapping can pack all charts into a single atlas or multiple atlases by semantic part, aiding tasks such as 2-D editing.\nFig. 10. xatlas and Blender generate over-fragmented UV maps that may\nintroduce color bleeding, especially with low-resolution texture maps (e.g.,\nin mobile games). In contrast, our results are free from such issues.\nTable 4.Ablation study conducted on the Trellis dataset.\nversionaverage median area angular seam\ntime id # charts\u2193# charts\u2193distort.\u2193distort.\u2191length\u2193\na fixed 20 parts 397.02 223.00 2.18 0.9687 66.42 207.75\nbreplace PF feat. with face normal 574.43 259.50 1.28 0.9708 64.46 83.67\nc no merge 763.74 270.50 1.30 0.9607 62.75 38.00\nd no recursion 928.23 237.00 1.31 0.9630 59.17 40.66\ne no distortion surrogate 575.31 216.50 1.24 0.9632 57.88 61.48\nf full 538.81 221.50 1.30 0.9609 57.92 41.88\nbe flattened with low distortion. We also experimented with replac-\ning the PartField features with face normals and then applying the\noriginal recursive tree search. As shown in Table 4(b), this variant\ndoubles the runtime, increases the number of charts, and produces\nchart boundaries that no longer align with semantic parts.\nMerge Heuristic.We propose an innovative geometry-based heuris-\ntic,Merge, for chart decomposition. When this heuristic is removed\nand only theNormalheuristic is used (Table 4(c)), we observe an\nincrease in the number of charts.\nRecursion.As shown in Alg. 1, Line 18, we do not immediately\nreturn the first solution found but continue the search to find poten-\ntially better solutions. When this strategy is removed (Table 4(d)),\nwe observe a significant increase in the number of charts.\nDistortion Surrogate.To accelerate the search, we first simplify\nthe mesh during intermediate iterations and then apply the ABF\nalgorithm to the simplified mesh to compute an approximate dis-\ntortion, which helps guide the search more efficiently. When this\nstrategy is removed (Table 4(e)), we observe an increase in runtime.\nWe show more ablations and visualizations, including compar-\nisons of different flattening algorithms, in the Appendix.\n4.5 95th Percentile Distortion Metrics\nTo evaluate worst-case behavior, we additionally report 95th-percentile\ndistortion metrics and compare them with baseline methods on four\ndatasets. As shown in Table 5, we consider two metrics:distortion\n95th-shape, defined as the 95th-percentile area distortion across all\nshapes in a dataset, anddistortion 95th-chart, which first computes\nthe 95th-percentile area distortion across all charts within a shape\nand then averages the results over all shapes in the dataset. As the\nFig. 11. PartUV lets users set a distortion threshold, flexibly controlling\nboth distortion and chart count.\nTable 5.95th-percentile area distortion metrics across four datasets.\nDataset Metric Blender xatlas ours\nABCdistortion 95th-shape\u2193 1.175 1.726 1.273\ndistortion 95th-chart\u2193 1.093 1.041 1.133\nCommon Shapesdistortion 95th-shape\u2193 1.885 1.504 1.404\ndistortion 95th-chart\u2193 1.139 1.131 1.169\nPartObjaverseTinydistortion 95th-shape\u2193 1.728 1.286 1.271\ndistortion 95th-chart\u2193 1.132 1.079 1.116\nTrellisdistortion 95th-shape\u2193 1.319 4.701 1.442\ndistortion 95th-chart\u2193 1.120 1.099 1.220\ntable shows, our method consistently yields low 95th-percentile\ndistortion values, with a maximum of 1.442, whereas baseline meth-\nods often produce much higher distortions (e.g., 4.701, 1.885, 1.728).\nThese results demonstrate that our approach achieves more robust\nand stable performance under challenging cases.\n5 Discussion on Failure Cases\nOur method struggles with poor input mesh topology. For example,\nit cannot handle 3D meshes containing self-intersections. In such\ncases, the algorithm may recurse deeply in an attempt to resolve\nthe intersections, which can lead to fragmented charts. Moreover,\nwhen input meshes are extremely fragmented\u2014for instance, those\nwith over 1000 components\u2014the unwrapping results also become\nhighly fragmented. In these cases, remeshing may be required before\nUV unwrapping. However, our method performs well on general\nmeshes with few components and does not require meshes to be\nwatertight or manifold.\n6 Conclusion\nIn this paper, we propose PartUV, a novel framework for UV un-\nwrapping that strategically integrates semantic part priors from\nlearning-based methods with two novel geometric heuristics. Par-\ntUV outperforms existing approaches by generating significantly\nfewer charts, low distortion, and chart boundaries that align with\nsemantic parts. We demonstrate the advantages of this pipeline\nthrough several applications.\nSA Conference Papers \u201925, December 15\u201318, 2025, Hong Kong, Hong Kong.\nPartUV: Part-Based UV Unwrapping of 3D Meshes\u202211\nAppendix\nFig. 12. A demonstration of effects from Open3D\u2019s higher distortion results,\ncreating curved (from angular distortion) and uneven (from area distortion)\nstripes.\nTable 6. Efficiency (\u2191) comparison across methods\nours Blender Open3D xatlas\nPart-Objaverse-Tiny 0.606 0.514 0.332 0.666\nTrellis 0.571 0.430 0.242 0.619\nABC 0.568 0.554 0.490 0.646\nCommon Shapes 0.575 0.460 0.461 0.561\nA1 Additional Results and Comparisons\nA1.1 More Qualitative Results\nIn the supplementary material, we provide an HTML file contain-\ning more qualitative and quantitative comparisons between our\nmethod and our baselines. The examples are gathered across the\nfour datasets we used in the main paper. The metrics are computed\nand reported for individual shapes.\nA1.2 Efficiency\nAs we create the UV map with regard to semantic features and\ngroups, it may raise doubts regarding the final efficiency of the\nresulting UV maps. In Table 6, we show the comparison of the\naverage efficiency of valid results between our method and the\nbaselines. In our experiments, we always group the charts from\nthe same parts when packing, and we use UVPackMaster [\u0141ukasz\nCzy\u017c 2025] to get the final packed UV map. We set the \"heuristic\nsearch\" time to 3 seconds for UVPackMaster, while most meshes\nfinish within microseconds. We define efficiency as the total area of\nvalid 2D faces within the normalized0 \u20131UV space. It can be seen\nthat our method does not hurt the overall efficiency, and it remains\ncompetitive compared to our baseline methods.\nA1.3 Analysis and Visualization of Open3D Results\nIn our experiments, particularly on the more challenging meshes,\nOpen3D frequently crashed or exceeded the allotted time budget.\nFig. 13. When using the face normals as agglomerative features for top-\ndown tree construction, the pipeline generates messier results with no\nsemantic alignment.\nWe designate a shape as timed-out when its runtime surpasses 30\nseconds plus three times the longest runtime among other methods\non the current shape.\nEven for the shapes where Open3D successfully produces a UV\nmap, the results still exhibit notable shortcomings. At first glance,\nOpen3D seemingly generates results with chart numbers similar to\nour method, albeit with slightly higher distortion. However, such dif-\nferences in distortion can significantly affect downstream tasks. In\nFig. 12, we show one example from the ABC dataset, which Open3D\ncreates a layout with a similar chart number with ours. However,\nwith 0.2 difference in area distortion and 0.1 difference in angular\ndistortion, the deformation of the texture is clearly noticeable. In\nparticular, when using a striped texture, angular distortion leads\nto curved lines (bottom half of highlighted region), while area dis-\ntortion causes uneven width (top half of highlighted region)\u2014both\nprominently visible in the red-marked rectangle of the figure, not\nto mention the seams that disregard underlying geometric features.\nIn contrast, our results exhibit minimal distortion, maintain the\nregularity of the shape, and feature seams that largely respect the\nunderlying geometric structure.\nA2 Additional Ablation Results\nA2.1 Replacing PartField feature with Face Normals\nIn our main pipeline, we create a top-down tree of faces with ag-\nglomerative clustering of PartField-predicted features. A simpler\nalternative would be replacing such features with face normals. In\nFig. 13, we provide some visual examples of meshes using normal as\nAgglomerative features versus using the PartField-predicted ones.\nIt can be seen that using normal is substantially prone to producing\ncurly shapes, like the cherry on the left. Moreover, though being\nable to predict a similar number of charts under the same heuristic\nsettings, using normal as the agglomerative features leads to way\nless organized and neat UV layouts, as exemplified on the right of\nthe figure.\nSA Conference Papers \u201925, December 15\u201318, 2025, Hong Kong, Hong Kong.\n12\u2022Zhaoning Wang, Xinyue Wei, Ruoxi Shi, Xiaoshuai Zhang, Hao Su, and Minghua Liu\nFig. 14. Visual comparison of with and without the merge heuristics.\nTable 7.Comparison between using ABF++ (Ours) and LSCM (Ours-\nLSCM) in our pipeline.\naverage median area seam time\n# charts\u2193# charts\u2193distort.\u2193length\u2193(s)\nOurs-LSCM 1154.12 363.00 1.27 65.48 58.61\nOurs 538.81 221.50 1.30 57.92 41.88\nA2.2 Without Merge Heuristic\nIn the main manuscript, we propose two heuristics used to unwrap\na part into charts,NormalandMerge. Despite the higher cost of\ncomputation,Mergeoften produces more visually appealing UV\nmaps with fewer charts. As illustrated in Fig. 14, theMergeheuristic\ncan usually unwrap the part in an \"unfolding\" manner, creating\nless number of charts with a neater layout, comparing under the\nsame distortion threshold requirements with using only theNormal\nHeuristics.\nA2.3 Without Normal Heuristic\nWith two heuristics included in the pipeline, one can also opt to\nuse only theMergethroughout the pipeline. We show several ex-\nample results in Fig. 15. Due to its high computational cost, using\nonlyMerge, especially at the beginning, incurs substantial runtime\noverhead. To be precise, theMergeheuristic would compute an ori-\nented bounding box (OBB) and use projection to get initial charts,\nand it could get hundreds of charts from a bumpy input. Trying to\nmerge all of them would introduce significant overhead for such\nbumpy/uneven meshes. For example, on the bimba mesh on the\nright side of the figure, the runtime increases from 147 seconds to\nover 892 seconds. Moreover, due to its bumpy geometry, the merg-\ning process often results in overlapping, leading to an increase in\nthe number of charts. In essence,Mergecould yield better results,\nbut only on simpler shapes. Therefore, it is most effective as a com-\nplement to theNormalheuristic and should be invoked only when\nNormalproduces a sufficiently good result.\nFig. 15. Using onlyMergeheuristic from the beginning would incur more\nruntime, and more fragmented charts on bumpy meshes.\nA2.4 Using LSCM instead of ABF++\nIn our main pipeline, we adopt ABF++ [Sheffer et al .2005] as the\nprimary flattening algorithm due to its fast performance and low-\ndistortion mappings. Most of our baselines utilize LSCM [L\u00e9vy et al .\n2002], which generally offers faster runtimes thanks to its linear\nenergy formulation. However, despite its speed, LSCM generally\nproduces inferior mapping results compared to ABF++. Such sub-\noptimal outputs can trigger additional recursions in our pipeline,\nresulting in increased runtimes and more fragmented UV charts.\nAs shown in Table 7, using LSCM ultimately leads to both higher\nruntime and a larger number of charts on the Trellis dataset.\nSA Conference Papers \u201925, December 15\u201318, 2025, Hong Kong, Hong Kong.\nPartUV: Part-Based UV Unwrapping of 3D Meshes\u202213\nReferences\nMarc Alexa, Daniel Cohen-Or, and David Levin. 2023. As-rigid-as-possible shape\ninterpolation. InSeminal Graphics Papers: Pushing the Boundaries, Volume 2. 165\u2013\n172.\nMikhail Belkin and Partha Niyogi. 2003. Laplacian eigenmaps for dimensionality\nreduction and data representation.Neural computation15, 6 (2003), 1373\u20131396.\nMirela Ben-Chen, Craig Gotsman, and Guy Bunin. 2008. Conformal flattening by\ncurvature prescription and metric scaling. InComputer Graphics Forum, Vol. 27.\nWiley Online Library, 449\u2013458.\nManas Bhargava, Camille Schreck, Marco Freire, Pierre-Alexandre Hugron, Sylvain\nLefebvre, Silvia Sell\u00e1n, and Bernd Bickel. 2025. Mesh Simplification for Unfolding.\nInComputer Graphics Forum, Vol. 44. Wiley Online Library, e15269.\nNathan A Carr, Jared Hoberock, Keenan Crane, and John C Hart. 2006. Rectangular\nmulti-chart geometry images. InSymposium on geometry processing. 181\u2013190.\nZhiqin Chen, Kangxue Yin, and Sanja Fidler. 2022. Auv-net: Learning aligned uv maps\nfor texture transfer and synthesis. InProceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition. 1465\u20131474.\nBlender Online Community. [n.d.]. Blender - a 3D modelling and rendering package.\nhttp://www.blender.org.\nMicrosoft Corporation. 2023. UVAtlas: Chart-based UV Atlas Generation Library.\nhttps://github.com/microsoft/UVAtlas. Accessed: 2025-05-21.\nSagnik Das, Ke Ma, Zhixin Shu, and Dimitris Samaras. 2022. Learning an Isometric\nSurface Parameterization for Texture Unwrapping.European Conference on Computer\nVision(2022).\nMatt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli Vander-\nBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. 2022.\nObjaverse: A Universe of Annotated 3D Objects.arXiv preprint arXiv:2212.08051\n(2022).\nMathieu Desbrun, Mark Meyer, and Pierre Alliez. 2002. Intrinsic parameterizations of\nsurface meshes. InComputer graphics forum, Vol. 21. Wiley Online Library, 209\u2013218.\nXianfeng Gu, Steven J Gortler, and Hugues Hoppe. 2002. Geometry images. InProceed-\nings of the 29th annual conference on Computer graphics and interactive techniques.\n355\u2013361.\nHaoyu Guo, He Zhu, Sida Peng, Yuang Wang, Yujun Shen, Ruizhen Hu, and Xiaowei\nZhou. 2024. Sam-guided graph cut for 3d instance segmentation. InEuropean\nConference on Computer Vision. Springer, 234\u2013251.\nQingdong He, Jinlong Peng, Zhengkai Jiang, Xiaobin Hu, Jiangning Zhang, Qiang Nie,\nYabiao Wang, and Chengjie Wang. 2024. PointSeg: A Training-Free Paradigm for 3D\nScene Segmentation via Foundation Models.arXiv preprint arXiv:2403.06403(2024).\nKai Hormann and G\u00fcnther Greiner. 2000. MIPS: An efficient global parametrization\nmethod.Curve and Surface Design: Saint-Malo 1999(2000), 153\u2013162.\nAlec Jacobson. 2013. common-3d-test-models. https://github.com/alecjacobson/\ncommon-3d-test-models. Accessed: 2025-05-21.\nAlec Jacobson and contributors. 2023. common-3d-test-models: Repository of common\n3D test meshes. https://github.com/alecjacobson/common-3d-test-models. Git\ncommit 8a4f864, accessed 2025-05-15.\nLi Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. 2020.\nPointgroup: Dual-set point grouping for 3d instance segmentation. InProceedings of\nthe IEEE/CVF conference on computer vision and Pattern recognition. 4867\u20134876.\nStephen C Johnson. 1967. Hierarchical clustering schemes.Psychometrika32, 3 (1967),\n241\u2013254.\nDan Julius, Vladislav Kraevoy, and Alla Sheffer. 2005. D-charts: Quasi-developable\nmesh segmentation. InComputer Graphics Forum, Vol. 24. Citeseer, 581\u2013590.\nAlan D Kalvin and Russell H Taylor. 1996. Superfaces: Polygonal mesh simplification\nwith bounded error.IEEE Computer Graphics and Applications16, 3 (1996), 64\u201377.\nSagi Katz and Ayellet Tal. 2003. Hierarchical mesh decomposition using fuzzy clustering\nand cuts.ACM transactions on graphics (TOG)22, 3 (2003), 954\u2013961.\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura\nGustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al .\n2023. Segment anything. InProceedings of the IEEE/CVF international conference on\ncomputer vision. 4015\u20134026.\nSebastian Koch, Albert Matveev, Zhongshi Jiang, Francis Williams, Alexey Artemov,\nEvgeny Burnaev, Marc Alexa, Denis Zorin, and Daniele Panozzo. 2019. Abc: A\nbig cad model dataset for geometric deep learning. InProceedings of the IEEE/CVF\nconference on computer vision and pattern recognition. 9601\u20139611.\nGuillaume Lavou\u00e9, Florent Dupont, and Atilla Baskurt. 2005. A new CAD mesh seg-\nmentation method, based on curvature tensor analysis.Computer-Aided Design37,\n10 (2005), 975\u2013987.\nBruno L\u00e9vy, Sylvain Petitjean, Nicolas Ray, and J\u00e9rome Maillot. 2002. Least squares\nconformal maps for automatic texture atlas generation.ACM Trans. Graph.21, 3\n(July 2002), 362\u2013371. https://doi.org/10.1145/566654.566590\nLiunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu\nZhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al .2022. Grounded\nlanguage-image pre-training. InProceedings of the IEEE/CVF conference on computer\nvision and pattern recognition. 10965\u201310975.Minchen Li, Danny M Kaufman, Vladimir G Kim, Justin Solomon, and Alla Sheffer. 2018.\nOptcuts: Joint optimization of surface cuts and parameterization.ACM transactions\non graphics (TOG)37, 6 (2018), 1\u201313.\nLigang Liu, Lei Zhang, Yin Xu, Craig Gotsman, and Steven J Gortler. 2008. A local/global\napproach to mesh parameterization. InComputer graphics forum, Vol. 27. Wiley\nOnline Library, 1495\u20131504.\nMinghua Liu, Mikaela Angelina Uy, Donglai Xiang, Hao Su, Sanja Fidler, Nicholas Sharp,\nand Jun Gao. 2025. PARTFIELD: Learning 3D Feature Fields for Part Segmentation\nand Beyond.arXiv preprint arXiv:2504.11451(2025).\nRichard Liu, Noam Aigerman, Vladimir G Kim, and Rana Hanocka. 2023. Da wand:\nDistortion-aware selection using neural mesh parameterization. InProceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition. 16739\u201316749.\nRong Liu and Hao Zhang. 2007a. Mesh segmentation via spectral embedding and\ncontour analysis. InComputer Graphics Forum, Vol. 26. Wiley Online Library, 385\u2013\n394.\nRong Liu and Hao Zhang. 2007b. Mesh Segmentation via Spectral Embedding and\nContour Analysis.Computer Graphics Forum (Proc. Eurographics 2007)26, 3 (2007),\n385\u2013394. https://doi.org/10.1111/j.1467-8659.2007.01061.x\nWilliam E Lorensen and Harvey E Cline. 1998. Marching cubes: A high resolution 3D\nsurface construction algorithm. InSeminal graphics: pioneering efforts that shaped\nthe field. 347\u2013353.\nKaichun Mo, Shilin Zhu, Angel X Chang, Li Yi, Subarna Tripathi, Leonidas J Guibas, and\nHao Su. 2019. Partnet: A large-scale benchmark for fine-grained and hierarchical\npart-level 3d object understanding. InProceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition. 909\u2013918.\nLuca Morreale, Noam Aigerman, Vladimir G. Kim, and Niloy J. Mitra. 2021. Neural\nSurface Maps. InConference on Computer Vision and Pattern Recognition.\nPatrick Mullen, Yiying Tong, Pierre Alliez, and Mathieu Desbrun. 2008. Spectral\nconformal parameterization. InComputer Graphics Forum, Vol. 27. Wiley Online\nLibrary, 1487\u20131494.\nSeonghun Oh, Xiaodi Yuan, Xinyue Wei, Ruoxi Shi, Fanbo Xiang, Minghua Liu, and\nHao Su. 2025. PaMO: Parallel Mesh Optimization for Intersection-Free Low-Poly\nModeling on the GPU.arXiv preprint arXiv:2509.05595(2025).\nNico Pietroni, Marco Tarini, and Paolo Cignoni. 2009. Almost isometric mesh pa-\nrameterization through abstract domains.IEEE Transactions on Visualization and\nComputer Graphics16, 4 (2009), 621\u2013635.\nRoi Poranne, Marco Tarini, Sandro Huber, Daniele Panozzo, and Olga Sorkine-Hornung.\n2017. AutoCuts: Simultaneous Distortion and Cut Optimization for UV Mapping.\nACM Transactions on Graphics(2017).\nAlex Pothen, Horst D Simon, and Kang-Pu Liou. 1990. Partitioning sparse matrices\nwith eigenvectors of graphs.SIAM journal on matrix analysis and applications11, 3\n(1990), 430\u2013452.\nSandeep Pulla, Anshuman Razdan, and Gerald Farin. 2001. Improved curvature estima-\ntion for watershed segmentation of 3-dimensional meshes.IEEE Transactions on\nVisualization and Computer Graphics5, 4 (2001), 308\u2013321.\nGuocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai, Hasan Hammoud, Mohamed\nElhoseiny, and Bernard Ghanem. 2022. Pointnext: Revisiting pointnet++ with\nimproved training and scaling strategies.Advances in neural information processing\nsystems35 (2022), 23192\u201323204.\nMichael Rabinovich, Roi Poranne, Daniele Panozzo, and Olga Sorkine-Hornung. 2017.\nScalable locally injective mappings.ACM Transactions on Graphics (TOG)36, 4\n(2017), 1.\nNicolas Ray and Bruno L\u00e9vy. 2003. Hierarchical least squares conformal map.11th\nPacific Conference onComputer Graphics and Applications, 2003. Proceedings.(2003),\n263\u2013270. https://api.semanticscholar.org/CorpusID:1121017\nBruno Roy. 2023. Neural ShDF: Reviving an Efficient and Consistent Mesh Segmentation\nMethod.arXiv preprint arXiv:2306.11737(2023).\nPedro V Sander, Steven Gortler, John Snyder, and Hugues Hoppe. 2002. Signal-\nspecialized parameterization. (2002).\nPedro V Sander, John Snyder, Steven J Gortler, and Hugues Hoppe. 2001. Texture map-\nping progressive meshes. InProceedings of the 28th annual conference on Computer\ngraphics and interactive techniques. 409\u2013416.\nPedro V. Sander, Zo\u00eb J. Wood, Steven J. Gortler, John M. Snyder, and Hugues Hoppe.\n2003. Multi-Chart Geometry Images. InEurographics Symposium on Geometry\nProcessing. https://api.semanticscholar.org/CorpusID:4893489\nRohan Sawhney and Keenan Crane. 2017. Boundary first flattening.ACM Transactions\non Graphics (ToG)37, 1 (2017), 1\u201314.\nChristian Sch\u00fcller, Ladislav Kavan, Daniele Panozzo, and Olga Sorkine-Hornung. 2013.\nLocally injective mappings. InComputer Graphics Forum, Vol. 32. Wiley Online\nLibrary, 125\u2013135.\nAlla Sheffer and Eric De Sturler. 2000. Surface Parameterization for Meshing by Trian-\ngulation Flattenin.\nAlla Sheffer and Eric de Sturler. 2001. Parameterization of faceted surfaces for meshing\nusing angle-based flattening.Engineering with computers17 (2001), 326\u2013337.\nAlla Sheffer, Bruno L\u00e9vy, Maxim Mogilnitsky, and Alexander Bogomyakov. 2005. ABF++:\nfast and robust angle based flattening.ACM Transactions on Graphics (TOG)24, 2\nSA Conference Papers \u201925, December 15\u201318, 2025, Hong Kong, Hong Kong.\n14\u2022Zhaoning Wang, Xinyue Wei, Ruoxi Shi, Xiaoshuai Zhang, Hao Su, and Minghua Liu\n(2005), 311\u2013330.\nOlga Sorkine and Marc Alexa. 2007. As-rigid-as-possible surface modeling. InSympo-\nsium on Geometry processing, Vol. 4. Citeseer, 109\u2013116.\nOlga Sorkine, Daniel Cohen-Or, Rony Goldenthal, and Dani Lischinski. 2002. Bounded-\ndistortion piecewise mesh parameterization. InIEEE Visualization, 2002. VIS 2002.\nIEEE, 355\u2013362.\nPratul P Srinivasan, Stephan J Garbin, Dor Verbin, Jonathan T Barron, and Ben Milden-\nhall. 2024. Nuvo: Neural uv mapping for unruly 3d representations. InEuropean\nConference on Computer Vision. Springer, 18\u201334.\nKehua Su, Li Cui, Kun Qian, Na Lei, Junwei Zhang, Min Zhang, and Xianfeng David Gu.\n2016. Area-preserving mesh parameterization for poly-annulus surfaces based on\noptimal mass transportation.Computer Aided Geometric Design46 (2016), 76\u201391.\nShigeo Takahashi, Hsiang-Yun Wu, Seow Hui Saw, Chun-Cheng Lin, and Hsu-Chun\nYen. 2011. Optimized topological surgery for unfolding 3d meshes. InComputer\ngraphics forum, Vol. 30. Wiley Online Library, 2077\u20132086.\nGabriel Taubin. 1995. A signal processing approach to fair surface design. InProceedings\nof the 22nd annual conference on Computer graphics and interactive techniques. 351\u2013\n358.\nWilliam Thomas Tutte. 1963. How to draw a graph.Proceedings of the London Mathe-\nmatical Society3, 1 (1963), 743\u2013767.\nThang Vu, Kookhoi Kim, Tung M Luu, Thanh Nguyen, and Chang D Yoo. 2022. Soft-\ngroup for 3d instance segmentation on point clouds. InProceedings of the IEEE/CVF\nconference on computer vision and pattern recognition. 2708\u20132717.\nFanbo Xiang, Zexiang Xu, Milos Hasan, Yannick Hold-Geoffroy, Kalyan Sunkavalli, and\nHao Su. 2021. Neutex: Neural texture mapping for volumetric neural rendering. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\n7119\u20137128.\nJianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong\nChen, Xin Tong, and Jiaolong Yang. 2024. Structured 3d latents for scalable and\nversatile 3d generation.arXiv preprint arXiv:2412.01506(2024).\nMutian Xu, Xingyilang Yin, Lingteng Qiu, Yang Liu, Xin Tong, and Xiaoguang Han.\n2023. Sampro3d: Locating sam prompts in 3d for zero-shot scene segmentation.\narXiv preprint arXiv:2311.17707(2023).\nXiuwei Xu, Huangxing Chen, Linqing Zhao, Ziwei Wang, Jie Zhou, and Jiwen Lu.\n2024. Embodiedsam: Online segment any 3d thing in real time.arXiv preprint\narXiv:2408.11811(2024).\nHitoshi Yamauchi, Stefan Gumhold, Rhaleb Zayer, and Hans-Peter Seidel. 2005. Mesh\nsegmentation driven by Gaussian curvature.The Visual Computer21 (2005), 659\u2013\n668.\nJingqi Yan, Xin Yang, Pengfei Shi, and David Zhang. 2005. Mesh parameterization\nby minimizing the synthesized distortion metric with the coefficient-optimizing\nalgorithm.IEEE transactions on visualization and computer graphics12, 1 (2005),\n83\u201392.\nYunhan Yang, Yukun Huang, Yuan-Chen Guo, Liangjun Lu, Xiaoyang Wu, Edmund Y\nLam, Yan-Pei Cao, and Xihui Liu. 2024. Sampart3d: Segment any part in 3d objects.\narXiv preprint arXiv:2411.07184(2024).\nYunhan Yang, Xiaoyang Wu, Tong He, Hengshuang Zhao, and Xihui Liu. 2023. Sam3d:\nSegment anything in 3d scenes.arXiv preprint arXiv:2306.03908(2023).\nLi Yi, Vladimir G Kim, Duygu Ceylan, I-Chao Shen, Mengyan Yan, Hao Su, Cewu Lu,\nQixing Huang, Alla Sheffer, and Leonidas Guibas. 2016. A scalable active framework\nfor region annotation in 3d shape collections.ACM Transactions on Graphics (ToG)\n35, 6 (2016), 1\u201312.\nYingda Yin, Yuzheng Liu, Yang Xiao, Daniel Cohen-Or, Jingwei Huang, and Baoquan\nChen. 2024. Sai3d: Segment any instance in 3d scenes. InProceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. 3292\u20133302.\nJonathan Young. 2019. xatlas. https://github.com/jpcy/xatlas. Accessed: 2025-09-26.\nMei-Heng Yueh, Wen-Wei Lin, Chin-Tien Wu, and Shing-Tung Yau. 2019. A novel\nstretch energy minimization algorithm for equiareal parameterizations.Journal of\nScientific Computing78 (2019), 1353\u20131386.\nRhaleb Zayer, Bruno L\u00e9vy, and Hans-Peter Seidel. 2007. Linear angle based param-\neterization. InFifth Eurographics Symposium on Geometry Processing-SGP 2007.\nEurographics Association, 135\u2013141.\nEugene Zhang, Konstantin Mischaikow, and Greg Turk. 2005. Feature-based surface\nparameterization and texture mapping.ACM Transactions on Graphics (TOG)24, 1\n(2005), 1\u201327.\nQijian Zhang, Junhui Hou, Wenping Wang, and Ying He. 2024. Flatten Anything:\nUnsupervised Neural Surface Parameterization. InProc. NeurIPS.\nYuming Zhao, Qijian Zhang, Junhui Hou, Jiazhi Xia, Wenping Wang, and Ying He. 2025.\nFlexPara: Flexible Neural Surface Parameterization.arXiv preprint arXiv:2504.19210\n(2025).\nKun Zhou, John Synder, Baining Guo, and Heung-Yeung Shum. 2004. Iso-charts: stretch-\ndriven mesh parameterization using spectral analysis. InProceedings of the 2004\nEurographics/ACM SIGGRAPH symposium on Geometry processing. 45\u201354.\nQian-Yi Zhou, Jaesik Park, and Vladlen Koltun. 2018a. Open3D: A modern library for\n3D data processing.arXiv preprint arXiv:1801.09847(2018).Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun. 2018b. Open3D: A Modern Library for\n3D Data Processing.arXiv:1801.09847(2018).\n3 Coords Computing \u0141ukasz Czy\u017c. 2025. UVPackmaster 3: GPU Accelerated UV Packing\nEngine. https://uvpackmaster.com/. Accessed: 2025-05-21.\nSA Conference Papers \u201925, December 15\u201318, 2025, Hong Kong, Hong Kong.",
      "summary": "PartUV is a novel part-based UV unwrapping pipeline designed to efficiently flatten complex 3D meshes, especially those generated by AI, into 2D charts with minimal distortion and fragmentation. Leveraging semantic part decomposition and geometric heuristics, PartUV produces fewer, part-aligned charts while maintaining low distortion and supporting challenging mesh types.",
      "key_contributions": [
        "Introduces a part-based UV unwrapping approach that generates fewer, semantically aligned charts with low distortion.",
        "Integrates high-level semantic part decomposition (via PartField) with geometric heuristics in a recursive, top-down framework.",
        "Extends parameterization and packing algorithms to handle non-manifold and degenerate meshes robustly.",
        "Demonstrates extensive parallelization for efficiency and scalability.",
        "Enables new applications such as part-specific multi-tiles packing."
      ],
      "methodology": "PartUV combines learning-based semantic part decomposition (PartField) with novel geometric heuristics to recursively partition and unwrap 3D meshes, ensuring each chart meets a user-defined distortion threshold while minimizing chart count. The pipeline incorporates advanced parameterization, packing, and dedicated handling of challenging mesh conditions.",
      "results": "PartUV outperforms existing UV unwrapping methods and neural approaches in reducing chart count and seam length, achieves comparable distortion levels, and demonstrates high success rates on diverse and challenging mesh datasets. It also supports new applications like part-specific multi-tiles packing.",
      "keywords": [
        "UV unwrapping",
        "3D mesh processing",
        "semantic part decomposition",
        "distortion minimization",
        "AI-generated meshes",
        "parameterization",
        "mesh segmentation"
      ],
      "status": "new",
      "notes": null,
      "created_at": "2025-11-23T20:38:14.100014",
      "needs_llm_processing": false
    }
  }
}